{"meta":{"title":"zsStrike","subtitle":null,"description":null,"author":"zsStrike","url":"http://blog.zsstrike.tech","root":"/"},"pages":[{"title":"","date":"2022-05-19T05:25:00.081Z","updated":"2022-05-19T05:25:00.081Z","comments":true,"path":"about/index.html","permalink":"http://blog.zsstrike.tech/about/index.html","excerpt":"","text":"我是一个有钻研精神的学生，在校期间专业课程成绩都在A以上。我喜欢探索和追问，对于不了解的东西能够坚持询问和探索。我具有良好的团队合作精神，善于分析和解决问题，对于创新技术有着强烈的求知欲。除此之外，我热爱生活，性格开朗，喜欢交朋友。"},{"title":"标签","date":"2022-05-19T04:29:44.000Z","updated":"2022-05-19T05:30:00.568Z","comments":true,"path":"tags/index.html","permalink":"http://blog.zsstrike.tech/tags/index.html","excerpt":"","text":""},{"title":"Projects","date":"2022-05-16T07:41:46.390Z","updated":"2022-05-16T07:41:46.390Z","comments":true,"path":"projects/index.html","permalink":"http://blog.zsstrike.tech/projects/index.html","excerpt":"","text":"Foresyn本项目是参与国际基因工程机器大赛（IGEM）创建的，目的是整合生物学相关数据库（如BiGG），优化用户交互界面，最终实现一个Web端应用，为使用者提供生物实验相关数据以及对实验进行预测。期间担任前端组组长，主要负责设计和实现Web界面，同时同后端人员沟通设计API为前端提供必要的数据。 项目地址：https://github.com/USTCSoftware2019/foresyn Activity-Planning本项目主要是构建一个Web站点用于展示中科大社团活动安排表，供学生们提前了解相关活动的信息，以此提前进行时间安排。该项目数据库使用MongoDB，后端用Nodejs搭配Express框架，使用模板语法渲染视图。 项目地址：https://github.com/zsStrike/activity-planing Rust-FreeRTOSFreeRTOS由C语言编写，鉴于C语言的不安全性，选择相对更安全的Rust语言来重构FreeRTOS系统，以此提供更加安全和稳定的嵌入式实时操作系统。在该项目中主要完成对链表的重构任务，以及协同小组其他成员测试链表结构的正确性和完成其他重构的其他任务。 项目地址：https://github.com/OSH-2019/x-rust-freertos"},{"title":"categories","date":"2022-05-19T04:31:27.000Z","updated":"2022-05-19T04:31:42.717Z","comments":true,"path":"categories/index.html","permalink":"http://blog.zsstrike.tech/categories/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2022-05-16T07:41:46.390Z","updated":"2022-05-16T07:41:46.390Z","comments":true,"path":"friends/index.html","permalink":"http://blog.zsstrike.tech/friends/index.html","excerpt":"这里写友链上方的内容。","text":"这里写友链上方的内容。 这里可以写友链页面下方的文字备注，例如自己的友链规范、示例等。"},{"title":"","date":"2022-05-16T07:41:46.389Z","updated":"2022-05-16T07:41:46.389Z","comments":true,"path":"assets/css/style.css","permalink":"http://blog.zsstrike.tech/assets/css/style.css","excerpt":"","text":"@charset \"utf-8\"; @font-face { font-family: 'Varela Round'; src: url(https://cdn.jsdelivr.net/gh/xaoxuu/cdn-fonts@19.1.7/VarelaRound/VarelaRound-Regular.ttf); font-weight: normal; font-style: normal; } @font-face { font-family: 'Source Sans Pro'; src: url(https://cdn.jsdelivr.net/gh/xaoxuu/cdn-fonts@master/SourceSansPro/SourceSansPro-Regular.ttf); font-weight: normal; font-style: normal; } /*! normalize.css v3.0.2 | MIT License | git.io/normalize */ /** * 1. Set default font family to sans-serif. * 2. Prevent iOS text size adjust after orientation change, without disabling * user zoom. */ html { font-family: sans-serif; /* 1 */ -ms-text-size-adjust: 100%; /* 2 */ -webkit-text-size-adjust: 100%; /* 2 */ } /** * Remove default margin. */ body { margin: 0; } /* HTML5 display definitions ========================================================================== */ /** * Correct `block` display not defined for any HTML5 element in IE 8/9. * Correct `block` display not defined for `details` or `summary` in IE 10/11 * and Firefox. * Correct `block` display not defined for `main` in IE 11. */ article, aside, details, figcaption, figure, footer, header, hgroup, main, menu, nav, section, summary { display: block; } /** * 1. Correct `inline-block` display not defined in IE 8/9. * 2. Normalize vertical alignment of `progress` in Chrome, Firefox, and Opera. */ audio, canvas, progress, video { display: inline-block; /* 1 */ vertical-align: baseline; /* 2 */ } /** * Prevent modern browsers from displaying `audio` without controls. * Remove excess height in iOS 5 devices. */ audio:not([controls]) { display: none; height: 0; } /** * Address `[hidden]` styling not present in IE 8/9/10. * Hide the `template` element in IE 8/9/11, Safari, and Firefox < 22. */ [hidden], template { display: none; } /* Links ========================================================================== */ /** * Remove the gray background color from active links in IE 10. */ a { background-color: transparent; } /** * Improve readability when focused and also mouse hovered in all browsers. */ a:active, a:hover { outline: 0; } /* Text-level semantics ========================================================================== */ /** * Address styling not present in IE 8/9/10/11, Safari, and Chrome. */ abbr[title] { border-bottom: 1px dotted; } /** * Address style set to `bolder` in Firefox 4+, Safari, and Chrome. */ b, strong { font-weight: bold; } /** * Address styling not present in Safari and Chrome. */ dfn { font-style: italic; } /** * Address variable `h1` font-size and margin within `section` and `article` * contexts in Firefox 4+, Safari, and Chrome. */ h1 { font-size: 2em; margin: 0.67em 0; } /** * Address styling not present in IE 8/9. */ mark { background: #ff0; color: #000; } /** * Address inconsistent and variable font size in all browsers. */ small { font-size: 80%; } /** * Prevent `sub` and `sup` affecting `line-height` in all browsers. */ sub, sup { font-size: 75%; line-height: 0; position: relative; vertical-align: baseline; } sup { top: -0.5em; } sub { bottom: -0.25em; } /* Embedded content ========================================================================== */ /** * Remove border when inside `a` element in IE 8/9/10. */ img { border: 0; } /** * Correct overflow not hidden in IE 9/10/11. */ svg:not(:root) { overflow: hidden; } /* Grouping content ========================================================================== */ /** * Address margin not present in IE 8/9 and Safari. */ figure { margin: 1em 40px; } /** * Address differences between Firefox and other browsers. */ hr { -moz-box-sizing: content-box; box-sizing: content-box; height: 0; border: 0; border-radius: 1px; border-bottom: 1px solid rgba(51, 51, 51, 0.1); } /** * Contain overflow in all browsers. */ pre { overflow: auto; } /** * Address odd `em`-unit font size rendering in all browsers. */ code, kbd, pre, samp { font-family: monospace, monospace; font-size: 1em; } /* Forms ========================================================================== */ /** * Known limitation: by default, Chrome and Safari on OS X allow very limited * styling of `select`, unless a `border` property is set. */ /** * 1. Correct color not being inherited. * Known issue: affects color of disabled elements. * 2. Correct font properties not being inherited. * 3. Address margins set differently in Firefox 4+, Safari, and Chrome. */ button, input, optgroup, select, textarea { color: inherit; /* 1 */ font: inherit; /* 2 */ margin: 0; /* 3 */ } /** * Address `overflow` set to `hidden` in IE 8/9/10/11. */ button { overflow: visible; } /** * Address inconsistent `text-transform` inheritance for `button` and `select`. * All other form control elements do not inherit `text-transform` values. * Correct `button` style inheritance in Firefox, IE 8/9/10/11, and Opera. * Correct `select` style inheritance in Firefox. */ button, select { text-transform: none; } /** * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio` * and `video` controls. * 2. Correct inability to style clickable `input` types in iOS. * 3. Improve usability and consistency of cursor style between image-type * `input` and others. */ button, html input[type=\"button\"], input[type=\"reset\"], input[type=\"submit\"] { -webkit-appearance: button; /* 2 */ cursor: pointer; /* 3 */ } /** * Re-set default cursor for disabled elements. */ button[disabled], html input[disabled] { cursor: default; } /** * Remove inner padding and border in Firefox 4+. */ button::-moz-focus-inner, input::-moz-focus-inner { border: 0; padding: 0; } /** * Address Firefox 4+ setting `line-height` on `input` using `!important` in * the UA stylesheet. */ input { line-height: normal; } /** * It's recommended that you don't attempt to style these elements. * Firefox's implementation doesn't respect box-sizing, padding, or width. * * 1. Address box sizing set to `content-box` in IE 8/9/10. * 2. Remove excess padding in IE 8/9/10. */ input[type=\"checkbox\"], input[type=\"radio\"] { box-sizing: border-box; /* 1 */ padding: 0; /* 2 */ } /** * Fix the cursor style for Chrome's increment/decrement buttons. For certain * `font-size` values of the `input`, it causes the cursor style of the * decrement button to change from `default` to `text`. */ input[type=\"number\"]::-webkit-inner-spin-button, input[type=\"number\"]::-webkit-outer-spin-button { height: auto; } /** * 1. Address `appearance` set to `searchfield` in Safari and Chrome. * 2. Address `box-sizing` set to `border-box` in Safari and Chrome * (include `-moz` to future-proof). */ input[type=\"search\"] { -webkit-appearance: textfield; /* 1 */ -moz-box-sizing: content-box; -webkit-box-sizing: content-box; /* 2 */ box-sizing: content-box; } /** * Remove inner padding and search cancel button in Safari and Chrome on OS X. * Safari (but not Chrome) clips the cancel button when the search input has * padding (and `textfield` appearance). */ input[type=\"search\"]::-webkit-search-cancel-button, input[type=\"search\"]::-webkit-search-decoration { -webkit-appearance: none; } /** * Define consistent border, margin, and padding. */ fieldset { border: 1px solid #c0c0c0; margin: 0 2px; padding: 0.35em 0.625em 0.75em; } /** * 1. Correct `color` not being inherited in IE 8/9/10/11. * 2. Remove padding so people aren't caught out if they zero out fieldsets. */ legend { border: 0; /* 1 */ padding: 0; /* 2 */ } /** * Remove default vertical scrollbar in IE 8/9/10/11. */ textarea { overflow: auto; } /** * Don't inherit the `font-weight` (applied by a rule above). * NOTE: the default cannot safely be changed in Chrome and Safari on OS X. */ optgroup { font-weight: bold; } /* Tables ========================================================================== */ /** * Remove most spacing between table cells. */ table { border-collapse: collapse; width: 100%; } table th { background-color: #f7f7f7; } table td, table th { text-align: justify; padding: 4px 8px; border: 1px solid #F4F4F4; } td, th { padding: 0; } /* Basic Settings */ * { box-sizing: border-box; outline: none; margin: 0; padding: 0; } /* My Base */ html { color: #333333; width: 100%; height: 100%; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; font-size: 16px; line-height: 1.5rem; -webkit-text-size-adjust: 100%; -ms-text-size-adjust: 100%; text-rendering: optimizelegibility; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); } body { background-color: #F4F4F4; } body.modal-active { overflow: hidden; } @media (max-width: 680px) { body.modal-active { position: fixed; top: 0; right: 0; bottom: 0; left: 0; } } body.z_menu-open .menu-phone { transform: translate3d(-16px, 0, 0); } fancybox { display: flex; justify-content: center; } .cover-wrapper { padding-bottom: 2px; } .cover-wrapper .cover { top: 0; left: 0; max-width: 100%; height: calc(100vh); display: flex; flex-wrap: nowrap; flex-direction: column; align-items: center; align-self: center; align-content: center; } .cover-wrapper .cover .title, .cover-wrapper .cover .logo { font-size: 48px; margin-top: calc(28vh - 2*16px); text-align: center; font-weight: bold; } .cover-wrapper .cover .title { line-height: calc(24px*2 + 2*16px); } .cover-wrapper .cover .logo { max-height: 100px; max-width: calc(100% - 4*16px); } @media (max-width: 580px) { .cover-wrapper .cover .title, .cover-wrapper .cover .logo { font-size: 48px; line-height: 52.8px; } } .cover-wrapper .cover .m_search { margin-top: calc(2vh + 2*16px); position: relative; max-width: calc(100% - 1*16px); width: 340px; line-height: 48px; vertical-align: middle; } .cover-wrapper .cover .m_search .form { position: relative; display: block; width: 100%; } .cover-wrapper .cover .m_search .icon, .cover-wrapper .cover .m_search .input { transition: all 0.3s ease; -moz-transition: all 0.3s ease; -webkit-transition: all 0.3s ease; -o-transition: all 0.3s ease; } .cover-wrapper .cover .m_search .icon { position: absolute; display: block; line-height: 44px; height: 44px; width: 32px; top: 0; left: 5px; font-size: 16px; color: rgba(51, 51, 51, 0.6); } .cover-wrapper .cover .m_search .input { display: block; font-size: 16px; line-height: 16px; height: 44px; width: 100%; color: #333333; box-shadow: none; box-sizing: border-box; -webkit-appearance: none; padding-left: 36px; border-radius: 64px; background: #ffffff; border: 1px dashed transparent; } @media (max-width: 580px) { .cover-wrapper .cover .m_search .input { padding-left: 36px; } } .cover-wrapper .cover .m_search .input::-webkit-input-placeholder { padding-top: 2px; color: rgba(51, 51, 51, 0.6); } .cover-wrapper .cover .m_search .input:-moz-placeholder { padding-top: 2px; color: rgba(51, 51, 51, 0.6); } .cover-wrapper .cover .m_search .input::-moz-placeholder { padding-top: 2px; color: rgba(51, 51, 51, 0.6); } .cover-wrapper .cover .m_search .input:-ms-input-placeholder { padding-top: 2px; color: rgba(51, 51, 51, 0.6); } .cover-wrapper .cover .m_search .input:hover ~ .icon { color: #1BC3FB; } .cover-wrapper .cover .m_search .input:focus { border: 1px solid #1BC3FB; } .cover-wrapper .cover .m_search .input:focus ~ .icon { color: #1BC3FB; } .cover-wrapper .cover.half { height: calc(60vh - 16px - 64px); } .cover-wrapper .cover.half .title, .cover-wrapper .cover.half .logo { margin-top: calc(22vh - 4*16px); } @media (max-width: 580px) { .cover-wrapper .cover.half { height: calc(45vh - 16px - 64px); } .cover-wrapper .cover.half .title, .cover-wrapper .cover.half .logo { margin-top: calc(24vh - 6*16px); } } .cover-wrapper .cover.half .m_search { margin-top: 16px; } .cover-wrapper .cover, .cover-wrapper .cover a { color: #1BC3FB; } .cover-wrapper .cover .menu { margin-top: 16px; } .cover-wrapper .cover .menu ul { display: flex; flex-wrap: wrap; align-items: baseline; justify-content: center; } .cover-wrapper .cover .menu ul li { display: flex; flex-wrap: wrap; align-items: center; padding: 0; height: auto; } .cover-wrapper .cover .menu ul > li > a { font-size: 14px; padding: 2px; margin: 0 4px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; color: rgba(51, 51, 51, 0.85); border-bottom: 1px solid transparent; } .cover-wrapper .cover .menu ul > li > a:hover, .cover-wrapper .cover .menu ul > li > a.active { color: #1BC3FB; border-bottom: 1px solid #1BC3FB; } .cover-wrapper .cover .switcher > li a:hover { background: rgba(27, 195, 251, 0.15); } .z-depth-nav, .l_header, #u-search .modal .modal-header { box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.24), 0 3px 6px 0px rgba(0, 0, 0, 0.1); } .z-depth-nav-raised, #u-search .modal .modal-header:hover { box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } .z-depth-main, .l_main .post, .widget { box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } .z-depth-main-raised { box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } .z-depth-0 { box-shadow: 0 1px 4px 0 rgba(0, 0, 0, 0.07); } .z-depth-1 { box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.08), 0 2px 4px 0 rgba(0, 0, 0, 0.1); } .z-depth-1-half { box-shadow: 0 2px 3px 0px rgba(0, 0, 0, 0.4), 0 0px 8px 0px rgba(0, 0, 0, 0.2); } .z-depth-2 { box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.1), 0 3px 10px 0 rgba(0, 0, 0, 0.1); } .z-depth-3 { box-shadow: 0 6px 10px 0 rgba(0, 0, 0, 0.12), 0 8px 25px 0 rgba(0, 0, 0, 0.1); } .z-depth-4 { box-shadow: 0 8px 14px 0 rgba(0, 0, 0, 0.11), 0 12px 22px 0 rgba(0, 0, 0, 0.11); } .z-depth-5 { box-shadow: 0 12px 12px 0 rgba(0, 0, 0, 0.1), 0 20px 33px 0 rgba(0, 0, 0, 0.11); } .z-depth-0 { box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.08), 0 2px 4px 0 rgba(0, 0, 0, 0.08); } .hoverable { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; box-shadow: 0; } .hoverable:hover { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; box-shadow: 0 8px 17px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19); } ::-moz-selection { background: rgba(33, 150, 243, 0.2); } ::selection { background: rgba(33, 150, 243, 0.2); } h1, h2, h3, h4, h5, h6 { -webkit-font-feature-settings: 'dlig' 1, 'liga' 1, 'lnum' 1, 'kern' 1; -moz-font-feature-settings: 'dlig' 1, 'liga' 1, 'lnum' 1, 'kern' 1; -o-font-feature-settings: 'dlig' 1, 'liga' 1, 'lnum' 1, 'kern' 1; text-rendering: geometricPrecision; margin: 0 0 0.4em 0; } h1 { font-size: 24px; } h2 { font-size: 24px; } h3 { font-size: 20.8px; } h4 { font-size: 18.4px; } h5 { font-size: 16px; } h6 { font-size: 14px; } a { color: #444444; cursor: pointer; text-decoration: none; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } a:hover { text-decoration: none; } pre { tab-size: 4; -moz-tab-size: 4; -o-tab-size: 4; -webkit-tab-size: 4; } img { max-width: 100%; } /** * Util */ .clearfix { zoom: 1; } .clearfix:before, .clearfix:after { content: \" \"; display: table; } .clearfix:after { clear: both; } .hidden { text-indent: -9999px; visibility: hidden; display: none; } .inner { position: relative; width: 80%; max-width: 710px; margin: 0 auto; } .vertical { display: table-cell; vertical-align: middle; } .right { float: right; } .left { float: left; } .disable-trans { -moz-transition: none !important; -webkit-transition: none !important; transition: none !important; } .txt-ellipsis, .widget .content ul.entry a .name, .widget .content ul.popular-posts a .name { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; } ul, ol { padding-left: 0; } li { list-style: none; } .mark { position: relative; } .mark a { color: #444444; display: inline-block; padding: 0 8px; border-left: 4px solid transparent; background: transparent; border-radius: 4px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .mark a:hover { background: rgba(27, 195, 251, 0.1); border-left: 4px solid #1BC3FB; padding: 8px; } .mark a:active { border-left: 8px solid #1BC3FB; } ul.h-list { display: flex; align-items: center; height: 100%; } ul.h-list > li { height: 100%; justify-content: center; } /** * Loading bar */ #loading-bar-wrapper { position: fixed; top: 62px; left: 0; width: 100%; z-index: 99999; } #loading-bar { position: fixed; width: 0; height: 2px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; background-color: rgba(255, 255, 255, 0.5); } #loading-bar.pure { background-color: rgba(27, 195, 251, 0.5); } .body-wrapper { position: relative; display: flex; width: 100%; max-width: 1080px; margin: 0 auto; flex-wrap: wrap; justify-content: space-between; align-items: stretch; } .container--flex { display: flex; flex-wrap: nowrap; justify-content: space-between; align-items: center; } .l_body { position: relative; margin: 16px; margin-top: 16px; } .l_body.nocover { margin-top: 80px; } @media (max-width: 580px) { .l_body { margin: 80px 0 16px; border-radius: 0; } } .l_body .s-top { transition: all 0.6s ease; -moz-transition: all 0.6s ease; -webkit-transition: all 0.6s ease; -o-transition: all 0.6s ease; z-index: 9; position: fixed; width: 48px; height: 48px; line-height: 48px; border-radius: 100%; bottom: 32px; right: 32px; transform: translateY(100px) scale(0); transform-origin: bottom; color: #333333; } @media (max-width: 768px) { .l_body .s-top { right: 16px; } } .l_body .s-top.show { transform: translateY(0) scale(1); } .l_body .s-top.show.hl { background: #1BC3FB; color: white; box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } @media (min-width: 768px) { .l_body .s-top:hover { transform: scale(1.2); border-radius: 25%; background: #1BC3FB; color: white; box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } .l_body .s-top:hover.hl { box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } } .l_header { position: fixed; z-index: 9999; top: 0; width: 100%; font-size: 16px; line-height: 64px; height: 64px; overflow: hidden; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; padding: 0 16px; margin-bottom: 16px; background: #1BC3FB; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_header .wrapper { padding: auto 16px; max-width: 1080px; margin: auto; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_header .wrapper a.logo { color: #262626; } .l_header.no_sidebar .wrapper { max-width: 768px; margin: auto; } .l_header .wrapper.sub { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; transform: translateY(-64px); } @media (max-width: 580px) { .l_header .wrapper.sub .logo { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; padding-left: 16px; padding-right: 0; font-size: 16px; } } .l_header .nav--main, .l_header .nav-sub { height: 64px; } .l_header.hide { transform: translateY(100px) scale(0); } .l_header.show { transform: translateY(0) scale(1); } .l_header, .l_header a { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; height: 64px; line-height: 64px; color: white; } .l_header .logo { padding: 0 24px; font-size: 19.2px; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; letter-spacing: 0; } @media (max-width: 580px) { .l_header .logo { padding: 0 16px; } } .l_header .logo.img { padding: 0 16px 0 0; } .l_header .logo img { height: 100%; } .l_header img.logo { padding: 4px 0; } .l_header .nav-sub .logo { padding: 0 24px; font-size: 16px; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; } @media (max-width: 580px) { .l_header .nav-sub .logo { letter-spacing: -0.5px; padding-top: 1px; } } .l_header .menu { position: relative; flex: 1 0 auto; height: 64px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; margin: 0 16px 0 0; } .l_header .menu ul > li > a { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; display: block; font-size: 16px; color: rgba(255, 255, 255, 0.7); padding: 0 8px; } .l_header .menu ul > li > a:hover { color: white; border-bottom: 2px solid white; background: rgba(255, 255, 255, 0.1); } .l_header .menu ul > li > a:active, .l_header .menu ul > li > a.active { color: white; border-bottom: 2px solid white; } @media (max-width: 580px) { .l_header .menu { display: none; } } .l_header .switcher { display: none; font-size: 16px; line-height: 64px; } .l_header .switcher .s-toc { display: none; } @media (max-width: 768px) { .l_header .switcher .s-toc { display: block; } } .l_header .switcher > li { height: 48px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; margin: 2px; } @media (max-width: 580px) { .l_header .switcher > li { margin: 0; height: 48px; } } .l_header .switcher > li a { display: flex; justify-content: center; align-items: center; width: 48px; height: 48px; border-radius: 100px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_header .switcher > li a:hover { background: rgba(255, 255, 255, 0.3); } @media (max-width: 580px) { .l_header .switcher > li a { width: 32px; height: 48px; } } @media (max-width: 580px) { .l_header .switcher { display: flex; padding-left: 8px; padding-right: 10px; } } .l_header .nav-sub .switcher { display: flex; } .l_header .m_search { position: relative; display: flex; width: 285px; height: 64px; } @media (max-width: 1350px) { .l_header .m_search { width: 240px; } } .l_header .m_search .form { position: relative; display: block; width: 100%; margin: auto; } .l_header .m_search .icon, .l_header .m_search .input { transition: all 0.3s ease; -moz-transition: all 0.3s ease; -webkit-transition: all 0.3s ease; -o-transition: all 0.3s ease; } .l_header .m_search .icon { position: absolute; display: block; line-height: 40px; height: 40px; width: 32px; top: 0; left: 5px; font-size: 16px; color: rgba(255, 255, 255, 0.6); } .l_header .m_search .input { display: block; font-size: 16px; line-height: 16px; height: 40px; width: 100%; color: rgba(255, 255, 255, 0.6); box-shadow: none; box-sizing: border-box; -webkit-appearance: none; padding-left: 36px; border-radius: 8px; background: rgba(255, 255, 255, 0.15); border: 1px dashed transparent; } @media (max-width: 580px) { .l_header .m_search .input { padding-left: 36px; } } .l_header .m_search .input::-webkit-input-placeholder { padding-top: 2px; color: rgba(255, 255, 255, 0.6); } .l_header .m_search .input:-moz-placeholder { padding-top: 2px; color: rgba(255, 255, 255, 0.6); } .l_header .m_search .input::-moz-placeholder { padding-top: 2px; color: rgba(255, 255, 255, 0.6); } .l_header .m_search .input:-ms-input-placeholder { padding-top: 2px; color: rgba(255, 255, 255, 0.6); } .l_header .m_search .input:hover { color: white; border: 1px solid rgba(255, 255, 255, 0.6); } .l_header .m_search .input:focus { color: white; border: 1px solid white; } .l_header .m_search .input:focus ~ .icon { color: white; } .l_header.pure { background: white; box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } .l_header.pure, .l_header.pure a { color: #1BC3FB; } .l_header.pure .menu ul > li > a { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; color: rgba(51, 51, 51, 0.85); } .l_header.pure .menu ul > li > a.current { border-bottom: 2px solid rgba(27, 195, 251, 0.8); } .l_header.pure .menu ul > li > a:hover { color: #1BC3FB; border-bottom: 2px solid #1BC3FB; background: rgba(27, 195, 251, 0.1); } .l_header.pure .menu ul > li > a:active, .l_header.pure .menu ul > li > a.active { color: #1BC3FB; border-bottom: 2px solid #1BC3FB; } .l_header.pure .switcher > li a:hover { background: rgba(27, 195, 251, 0.15); } .l_header.pure .m_search .icon { color: rgba(51, 51, 51, 0.6); } .l_header.pure .m_search .input { color: #333333; background: #F4F4F4; } .l_header.pure .m_search .input::-webkit-input-placeholder { color: rgba(51, 51, 51, 0.6); } .l_header.pure .m_search .input:-moz-placeholder { color: rgba(51, 51, 51, 0.6); } .l_header.pure .m_search .input::-moz-placeholder { color: rgba(51, 51, 51, 0.6); } .l_header.pure .m_search .input:-ms-input-placeholder { color: rgba(51, 51, 51, 0.6); } .l_header.pure .m_search .input:hover { border: 1px solid rgba(27, 195, 251, 0.6); } .l_header.pure .m_search .input:hover ~ .icon { color: rgba(27, 195, 251, 0.8); } .l_header.pure .m_search .input:focus { color: #333333; background: rgba(27, 195, 251, 0.15); border: 1px solid #1BC3FB; } .l_header.pure .m_search .input:focus ~ .icon { color: #1BC3FB; } @media (max-width: 580px) { .l_header { padding: 0; } .l_header .m_search { width: 0; overflow: hidden; position: absolute; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; margin: 0 8px; } .l_header.z_search-open .logo { opacity: 0; } .l_header.z_search-open .m_search { width: calc(100vw - 2*16px - 2*32px); } } .menu-phone { position: fixed; top: 80px; right: 0; z-index: 10000; line-height: 32px; background: white; border-right: 0; box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); border-radius: 12px; transform: translate3d(-40px, -40px, 0) scale(0, 0); transform-origin: right top; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .menu-phone .header { border-top-left-radius: 12px; border-top-right-radius: 12px; background-color: rgba(27, 195, 251, 0.9); color: white; font-size: 16px; line-height: 1.8em; padding: 8px 22px; } .menu-phone:hover { box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } .menu-phone:active { box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } .menu-phone nav { padding: 8px 0px; } .menu-phone nav .nav { height: 36px; line-height: 36px; position: relative; display: block; color: #444444; padding: 2px 20px; border-left: 4px solid transparent; border-right: 4px solid transparent; } .menu-phone nav .nav:hover, .menu-phone nav .nav.active { border-left: 4px solid #1BC3FB; background: rgba(27, 195, 251, 0.1); } .cover-wrapper .l_header { transition: all 0.5s ease; -moz-transition: all 0.5s ease; -webkit-transition: all 0.5s ease; -o-transition: all 0.5s ease; transform: translateY(-96px); } .cover-wrapper .l_header.show { transform: translateY(0); } .l_main { width: calc(100% - 1 * 285px); padding-right: 16px; float: left; } @media (max-width: 1350px) { .l_main { width: calc(100% - 1 * 240px); } } @media (max-width: 768px) { .l_main { width: 100%; } } .l_main.no_sidebar { width: 100%; padding-right: 0; max-width: 768px; margin: auto; } .l_main.no_sidebar ~ .l_side { display: none; } .l_main .post-list { position: relative; margin: 0px auto; column-gap: 0; } @media (max-width: 580px) { .l_main .post-list { margin: 0; } } .l_main ul.popular-posts h3 { padding: 0; margin: 0; font-size: 16px; } .l_main #comments { position: relative; } @media (max-width: 580px) { } .l_main #comments #valine_container p { line-height: 1.7; } .l_main #comments #valine_container p img { max-height: 28px; } .l_main #comments #valine_container img { display: inline; } .l_main #comments #valine_container .vwrap { border-radius: 12px; border-style: dashed; border: 1px dashed rgba(51, 51, 51, 0.3); transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_main #comments #valine_container .vwrap:hover { border: 1px dashed #1bc3fb; } .l_main #comments #valine_container .vwrap .vheader .vinput { border-radius: 0; border-bottom: 1px dashed rgba(51, 51, 51, 0.3); } .l_main #comments #valine_container .vwrap .vheader .vinput:hover { border-bottom: 1px dashed #1BC3FB; } .l_main #comments #valine_container .vwrap .vheader .vinput:focus { border-bottom: 1px solid #1BC3FB; } .l_main #comments #valine_container .vwrap .vedit .vctrl span { color: #1BC3FB; padding: 0; margin: 10px; } .l_main #comments #valine_container button { border: none; padding-left: 2.4em; padding-right: 2.4em; font-weight: bold; background-color: #1BC3FB; color: white; border-radius: 6px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_main #comments #valine_container button:hover { background: #04a8df; } .l_main #comments #valine_container blockquote { padding: 16px; border-left: 4px solid #1BC3FB; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_main #comments #valine_container blockquote p { text-align: left; word-wrap: normal; margin: 0; font-size: 14px; line-height: 21px; } .l_main #comments #valine_container pre code { border: none; } .l_main #comments #valine_container code { font-family: 'Ubuntu', Menlo, Monaco, courier, \"Lucida Console\", monospace, 'Source Code Pro', \"Microsoft YaHei\", Helvetica, Arial, sans-serif; font-size: 12.8px; color: rgba(51, 51, 51, 0.9); } .l_main #comments #valine_container a, .l_main #comments #valine_container .vemoji-btn, .l_main #comments #valine_container .vpreview-btn { color: #1BC3FB; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_main #comments #valine_container a:hover, .l_main #comments #valine_container .vemoji-btn:hover, .l_main #comments #valine_container .vpreview-btn:hover { color: #ff5722; text-decoration: underline; } .l_main #comments #valine_container a:active, .l_main #comments #valine_container .vemoji-btn:active, .l_main #comments #valine_container .vpreview-btn:active { color: #a22700; } .l_main #comments #valine_container .vhead span.vnick { color: rgba(51, 51, 51, 0.9); } .l_main #comments #valine_container .vhead a.vnick { color: #ff9800; font-weight: bold; } .l_main #comments #valine_container .vhead a.vnick:hover { color: #ff5722; text-decoration: underline; } .l_main #comments #valine_container .vhead .vsys { margin: 2px; padding: 1px 8px; background-color: rgba(51, 51, 51, 0.1); } .l_main #comments #valine_container .vcard .vquote { border-left: none; } .l_main #comments #valine_container .vcard .vh { border-bottom: 1px dashed rgba(51, 51, 51, 0.1); } .l_main #comments #valine_container .vmeta .vat { font-weight: bold; color: #1BC3FB; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_main #comments #valine_container .vmeta .vat:hover { color: #ff5722; text-decoration: underline; } .l_main #comments #valine_container .vmeta .vat:active { color: #a22700; } .l_main #comments #valine_container .vinput { color: #333333; } .l_main #comments #valine_container p { color: #333333; } .l_main #comments .vemojis { justify-content: space-between; } .l_main #comments .vemojis i { width: auto; height: 36px; padding: 0; margin: 8px 8px 0 8px; } .l_main #comments .vemojis i .emoji { height: 24px; margin-top: 6px; background: transparent; } .l_main #comments p .emoji { display: inline; height: 28px; background: transparent; } .l_main .post-wrapper { column-break-inside: avoid; break-inside: avoid-column; } .l_main .post-wrapper { margin-bottom: 16px; } .l_main .post-wrapper .post .meta { margin-bottom: 16px; } .l_main .post-wrapper .post .meta .title { font-size: 24px; } .l_main .post-wrapper .post .meta .title a { font-size: 24px; } .l_main .post-wrapper .post .full-width { margin-left: -24px; margin-right: -24px; width: calc(100% + 3 * 16px); } .l_main .post-wrapper .post .auto-padding { padding-left: 24px; padding-right: 24px; border-bottom-left-radius: 12px; border-bottom-right-radius: 12px; overflow: auto; } .l_main .post-wrapper .tags { margin-bottom: -32px; } @media (max-width: 580px) { .l_main .post-wrapper .tags { margin-bottom: -24px; } .l_main .post-wrapper .post .meta { margin-bottom: 16px; } .l_main .post-wrapper .post .meta .title { font-size: 24px; } .l_main .post-wrapper .post .full-width { margin-left: -16px; margin-right: -16px; padding-left: 16px; padding-right: 16px; width: calc(100% + 2 * 16px); } .l_main .post-wrapper .post .auto-padding { padding-left: 16px; padding-right: 16px; border-bottom-left-radius: 12px; border-bottom-right-radius: 12px; overflow: auto; } .l_main .post-wrapper .post .highlight { margin-left: 0px; margin-right: 0px; width: calc(100% - 0 * 16px); } } @media (max-width: 580px) and (max-width: 580px) { .l_main .post-wrapper .post { padding: 24px 16px; } .l_main .post-wrapper .post .highlight { margin-left: 0px; margin-right: 0px; width: calc(100% - 0 * 16px); } .l_main .post-wrapper .post .auto-padding { border-bottom-left-radius: 0; border-bottom-right-radius: 0; } } @media (max-width: 580px) { .l_main .widget { border-radius: 0; margin-left: 0; margin-right: 0; width: auto; } .l_main .widget:hover { box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } } .l_main .post { position: relative; margin: 16px auto; padding: 32px 24px; background: white; border-radius: 12px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_main .post h1 { font-weight: normal; font-size: 28.8px; line-height: 1.7; color: #333333; } .l_main .post:hover { box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } .l_main .post:active { box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } @media (max-width: 580px) { .l_main .post { border-radius: 0; } .l_main .post:hover { box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } } .l_main .post .tags a { color: rgba(51, 51, 51, 0.7); } .l_main .post .meta { color: rgba(51, 51, 51, 0.7); font-size: 13.3px; } .l_main .post .meta#header-meta { margin-top: 0; margin-bottom: 16px; } .l_main .post .meta#footer-meta { margin-top: 32px; margin-bottom: 8px; } .l_main .post .meta .aplayer, .l_main .post .meta .thumbnail { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; width: 65px; height: 65px; border-radius: 100%; float: right; margin: 4px; box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } .l_main .post .meta .aplayer:hover, .l_main .post .meta .thumbnail:hover { border-radius: 25%; transform: scale(1.1); box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } @media (max-width: 580px) { .l_main .post .meta .aplayer:hover, .l_main .post .meta .thumbnail:hover { border-radius: 100%; transform: scale(1); box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } } .l_main .post .meta .thumbnail { width: auto; border-radius: 4px; box-shadow: none; } .l_main .post .meta .thumbnail:hover { border-radius: 4px; transform: scale(1.1) rotate(4deg); box-shadow: none; } .l_main .post .meta .title { text-align: left; font-size: 28.8px; margin: 0; } @media (max-width: 580px) { .l_main .post .meta .title { font-size: 24px; } } .l_main .post .meta .title a { display: inline; line-height: 1.7; font-weight: normal; color: #333333; text-decoration: none; font-size: 28.8px; } @media (max-width: 580px) { .l_main .post .meta .title a { font-size: 24px; } } .l_main .post .meta .title a:hover { color: #ff5722; } .l_main .post .meta .new-meta-box { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; padding-top: 4px; padding-bottom: 8px; display: flex; align-items: center; flex-wrap: wrap; } .l_main .post .meta .new-meta-box .new-meta-item { color: rgba(51, 51, 51, 0.7); font-size: 14px; line-height: 24px; display: flex; align-items: center; justify-content: center; padding: 2px; margin: 0 8px 0 0; border-radius: 4px; } .l_main .post .meta .new-meta-box .new-meta-item .notlink { cursor: default; } .l_main .post .meta .new-meta-box .new-meta-item .notlink:hover { color: rgba(51, 51, 51, 0.7); } .l_main .post .meta .new-meta-box .new-meta-item .notlink:hover p { color: rgba(51, 51, 51, 0.7); } .l_main .post .meta .new-meta-box .new-meta-item:last-child { margin-right: 0; } .l_main .post .meta .new-meta-box .new-meta-item img, .l_main .post .meta .new-meta-box .new-meta-item i { border-radius: 100%; display: inline; } .l_main .post .meta .new-meta-box .new-meta-item i { margin-right: 4px; border-radius: 0; } .l_main .post .meta .new-meta-box .new-meta-item i.fa-hashtag { margin-right: 1px; } .l_main .post .meta .new-meta-box .new-meta-item p, .l_main .post .meta .new-meta-box .new-meta-item a { color: rgba(51, 51, 51, 0.7); padding-left: 0; padding-right: 4px; } .l_main .post .meta .new-meta-box .new-meta-item a { font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; display: flex; justify-content: center; align-items: center; } .l_main .post .meta .new-meta-box .new-meta-item a img { height: 17px; width: 17px; margin-right: 5px; transform: translateY(-1px); } .l_main .post .meta .new-meta-box .new-meta-item a p { margin: 0; padding-top: 2px; font-weight: normal; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_main .post .meta .new-meta-box .new-meta-item a:hover { color: #ff5722; text-decoration: none; } .l_main .post .meta .new-meta-box .new-meta-item a:hover p { color: #ff5722; } .l_main .post .meta .new-meta-box .share-body { height: 22px; display: flex; } .l_main .post .meta .new-meta-box .share-body a { padding: 0; margin-right: 4px; } .l_main .post .meta .new-meta-box .share-body a img { height: 22px; width: auto; background: transparent; } .l_main .post .full-width, .l_main .post .highlight { margin-left: 0px; margin-right: 0px; width: calc(100% - 0 * 16px); } .l_main .post img { display: flex; justify-content: center; align-items: center; max-width: 100%; border-radius: 4px; background: none; } .l_main .post span img { display: inline-block; } .l_main .post a img { display: inline; } @media (max-width: 768px) { .l_main { padding-right: 0; } .l_main .post .meta { margin-bottom: 16px; } .l_main .post .meta .title { font-size: 24px; } .l_main .post .full-width { margin-left: -16px; margin-right: -16px; padding-left: 16px; padding-right: 16px; width: calc(100% + 2 * 16px); } .l_main .post .auto-padding { padding-left: 16px; padding-right: 16px; border-bottom-left-radius: 12px; border-bottom-right-radius: 12px; overflow: auto; } .l_main .post .highlight { margin-left: 0px; margin-right: 0px; width: calc(100% - 0 * 16px); } } @media (max-width: 768px) and (max-width: 580px) { .l_main { width: 100%; } } @media (max-width: 768px) and (max-width: 580px) { .l_main .post { padding: 24px 16px; } .l_main .post .highlight { margin-left: 0px; margin-right: 0px; width: calc(100% - 0 * 16px); } .l_main .post .auto-padding { border-bottom-left-radius: 0; border-bottom-right-radius: 0; } } .l_main .prev-next { width: 100%; display: flex; justify-content: space-between; align-items: baseline; color: rgba(51, 51, 51, 0.5); margin: 0; } .l_main .prev-next .prev { text-align: left; border-top-right-radius: 32px; border-bottom-right-radius: 32px; } .l_main .prev-next .next { text-align: right; border-top-left-radius: 32px; border-bottom-left-radius: 32px; } .l_main .prev-next p { margin: 16px; } .l_main .prev-next a { color: rgba(27, 195, 251, 0.9); } .l_main .prev-next section { color: rgba(51, 51, 51, 0.8); padding: 16px; border-radius: 12px; } .l_main .prev-next section:hover { color: #ff5722; } @media (max-width: 580px) { .l_main .prev-next section { border-radius: 0; } } .alert { display: none; position: fixed; top: 50%; left: 50%; transform: translate(-50%, -50%); z-index: 99999; text-align: center; padding: 24px 36px; border-radius: 4px; box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; font-weight: bold; font-size: 16px; } .alert.alert-success { color: #3c763d; background-color: #dff0d8; border-color: #d6e9c6; } .alert.alert-info { color: #31708f; background-color: #d9edf7; border-color: #bce8f1; } .alert.alert-warning { color: #8a6d3b; background-color: #fcf8e3; border-color: #faebcc; } .alert.alert-danger { color: #a94442; background-color: #f2dede; border-color: #ebccd1; } .l_side { width: 285px; float: right; position: relative; display: flex; flex-direction: column; } @media (max-width: 1350px) { .l_side { width: 240px; } } @media (max-width: 768px) { .l_side { width: 100%; } } .widget { z-index: 0; background: white; margin-top: 16px; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; font-size: 16px; border-radius: 12px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; width: 100%; max-height: calc(100% - 64px - 4 * 16px); } .widget:hover { box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } .widget:active { box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); } @media (max-width: 580px) { .widget { width: calc(100% - 2 * 16px); margin: 16px 16px 0 16px; } } .widget header { display: flex; justify-content: space-between; border-top-left-radius: 12px; border-top-right-radius: 12px; background-color: #1BC3FB; color: white; font-weight: bold; line-height: 1.5em; padding: 8px 16px; } .widget header .rightBtn { color: white; } .widget header .rightBtn:hover { color: #037094; } .widget header .rightBtn:hover.rotate90 { transform: rotate(90deg); } .widget header.pure { background-color: white; color: #1BC3FB; padding-top: 14px; padding-bottom: 14px; } .widget header.pure .rightBtn { color: #1BC3FB; } .widget header.pure .rightBtn:hover { color: #037094; } .widget .content { text-align: justify; padding: 8px; max-height: calc(100% - 64px - 12.5 * 16px); } .widget .content ul > li a { color: rgba(51, 51, 51, 0.8); padding: 0 16px; line-height: 36px; display: flex; justify-content: space-between; align-content: center; border-left: 2px solid transparent; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .widget .content ul.entry a .name, .widget .content ul.popular-posts a .name { flex: auto; color: rgba(51, 51, 51, 0.8); } .widget .content ul.entry a .badge, .widget .content ul.popular-posts a .badge { flex: none; font-weight: normal; font-size: 14px; color: rgba(51, 51, 51, 0.7); } .widget .content ul.entry a:hover, .widget .content ul.popular-posts a:hover { border-left: 4px solid #1BC3FB; background: rgba(27, 195, 251, 0.1); } .widget .content ul.entry a:active, .widget .content ul.popular-posts a:active { border-left: 8px solid #1BC3FB; } .widget .content ul.entry a.child, .widget .content ul.popular-posts a.child { padding-left: 32px; } .widget.author { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .widget.author .content { padding: 0; } .widget.author .content div.avatar { display: flex; justify-content: center; } .widget.author .content img { padding: 0; margin: 0; display: flex; justify-content: center; width: 285px; height: 285px; border-top-left-radius: 12px; border-top-right-radius: 12px; } @media (max-width: 1350px) { .widget.author .content img { width: 240px; height: 240px; } } @media (max-width: 768px) { .widget.author .content img { width: 96px; height: 96px; border-radius: 100%; margin-top: 8px; padding: 8px; } } .widget.author .content h2 { text-align: center; font-weight: bold; margin: 8px; } @media (max-width: 768px) { .widget.author .content h2 { margin: 8px; } } .widget.author .content p { font-size: 16px; font-weight: bold; text-align: center; margin: 8px 8px 0 8px; empty-cells: hide; } .widget.author .content .social-wrapper { display: flex; justify-content: space-between; flex-wrap: wrap; margin: 4px 8px; } .widget.author .content .social-wrapper a { color: rgba(51, 51, 51, 0.7); padding: 0; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .widget.author .content .social-wrapper a:hover { color: #ff5722; } .widget.author .content .social-wrapper a.social { display: flex; justify-content: center; align-items: center; width: 32px; height: 32px; margin: 4px; border-radius: 100px; } .widget.author .content .social-wrapper a.social:hover { background: rgba(27, 195, 251, 0.1); color: #1BC3FB; } @media (max-width: 768px) { .widget.author .content .social-wrapper { justify-content: center; display: none; } } @media (max-width: 768px) { .widget.author { box-shadow: none; background: #F4F4F4; margin-top: 32px; } } .widget.plain .content { font-size: 14px; font-weight: bold; word-break: break-all; padding: 8px 16px; line-height: 22px; } .widget.plain .content.pure { padding: 0 16px 16px 16px; } .widget.plain .content a { color: #1BC3FB; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .widget.plain .content a:hover { color: #ff5722; text-decoration: underline; } .widget.plain .content a:active { color: #a22700; } .widget.list .content { padding: 8px 0; } .widget.list .content.pure { padding-top: 0; } .widget.list .content a { font-size: 14px; font-weight: bold; } .widget.list .content a:hover { text-decoration: none; } .widget.list .content a i { color: rgba(51, 51, 51, 0.7); line-height: 36px; margin-right: 3px; } .widget.list .content a img { display: inline; vertical-align: middle; height: 18px; width: 18px; margin-bottom: 4px; } .widget.list .content a img#round { border-radius: 100%; } .widget.grid .content { padding: 8px 0; } .widget.grid .content.pure { padding-top: 0; } .widget.grid .content ul.grid { border: none; display: flex; flex-wrap: wrap; justify-content: space-around; padding: 0 16px; } .widget.grid .content ul.grid a { text-align: center; border-radius: 12px; margin: 4px 0; padding: 4px 8px; display: flex; flex-direction: column; align-items: center; font-size: 12.6px; font-weight: bold; line-height: 18.2px; color: rgba(51, 51, 51, 0.7); border: 1px solid transparent; } .widget.grid .content ul.grid a i { margin-top: 0.3em; margin-bottom: 0.3em; font-size: 1.8em; } .widget.grid .content ul.grid a img { display: inline; vertical-align: middle; margin-bottom: 4px; } .widget.grid .content ul.grid a img#round { border-radius: 100%; } .widget.grid .content ul.grid a:hover { color: #1BC3FB; background: rgba(27, 195, 251, 0.1); border-radius: 4px; } .widget.grid .content ul.grid a:active { color: #1BC3FB; } .widget.grid .content ul.grid a.active { color: #1BC3FB; border: 1px solid #1BC3FB; } .widget.category .content { padding: 8px 0; font-size: 14px; font-weight: bold; } .widget.category .content.pure { padding-top: 0; } .widget.tagcloud .content { text-align: justify; padding: 8px 16px; } .widget.tagcloud .content.pure { padding: 0 16px 16px 16px; } .widget.tagcloud .content a { display: inline-block; transition: all 0.1s ease; -moz-transition: all 0.1s ease; -webkit-transition: all 0.1s ease; -o-transition: all 0.1s ease; line-height: 1.6em; } .widget.tagcloud .content a:hover { color: #ff5722 !important; text-decoration: underline; } .widget.music header.pure { padding-bottom: 4px; } .widget.music .content { padding: 12px; padding-top: 8px; } .widget.music .content.pure { padding-top: 4px; } .widget.music .content .aplayer { border-radius: 4px; color: #666; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; } .widget.related_posts .content { padding: 8px 0; font-size: 14px; font-weight: bold; } .widget.related_posts .content.pure { padding-top: 0; } .widget.related_posts .content h3 { font-size: 14px; font-weight: bold; margin: 0; } .widget.related_posts .content h3 a { line-height: inherit; padding-top: 4px; padding-bottom: 4px; } .l_side .toc-wrapper { z-index: 1; overflow: hidden; border-radius: 12px; position: sticky; top: 80px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .l_side .toc-wrapper header { position: sticky; width: 100%; top: 0; } .l_side .toc-wrapper .content { padding: 8px 0 16px 0; max-height: 500px; overflow: auto; } .l_side .toc-wrapper .content.pure { padding-top: 0; } .l_side .toc-wrapper .content a { border-left: 4px solid transparent; } .l_side .toc-wrapper .content a:hover { color: #333333; border-left: 4px solid #1BC3FB; } .l_side .toc-wrapper .content a:active { border-left: 8px solid #1BC3FB; } .l_side .toc-wrapper .content a.active { color: #333333; border-left: 4px solid #1BC3FB; background: rgba(27, 195, 251, 0.1); } .l_side .toc-wrapper.active { position: fixed; box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); top: 64px; width: 285px; } .l_side .toc-wrapper.active header .s-toc { transform: rotate(30deg); } @media (max-width: 1350px) { .l_side .toc-wrapper.active { width: 240px; } } @media (max-width: 768px) { .l_side .toc-wrapper.active { width: calc(100% - 2 * 16px); } } @media (max-width: 768px) { .l_side .toc-wrapper { position: fixed; max-height: 1000px; width: calc(100% - 2 * 16px); top: 64px; box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); visibility: hidden; transform: scale(0, 0); transform-origin: right top; } .l_side .toc-wrapper .rightBtn { display: none; } .l_side .toc-wrapper.active { visibility: visible; transform: scale(1, 1); } } .l_side .toc-wrapper a { padding-left: 8px; color: rgba(51, 51, 51, 0.6); font-size: 14px; display: inline-block; } .l_side .toc-wrapper ol .toc-item.toc-level-1 .toc-child a { padding-left: 12.8px; font-weight: normal; } .l_side .toc-wrapper ol .toc-item.toc-level-2 .toc-child a { padding-left: 25.6px; font-weight: normal; } .l_side .toc-wrapper ol .toc-item.toc-level-3 .toc-child a { padding-left: 38.4px; font-weight: normal; } .l_side .toc-wrapper ol .toc-item.toc-level-4 .toc-child a { padding-left: 51.2px; font-weight: normal; } .l_side .toc-wrapper ol li { width: auto; text-align: left; } .l_side .toc-wrapper ol li a { padding: 0 8px 0 11px; font-weight: bold; width: 100%; } .l_side .toc-wrapper:empty { display: none; } #archive-page { margin-bottom: 32px; } #archive-page .archive { position: relative; } #archive-page .archive .archive-year { font-size: 16px; margin-top: 4em; margin-bottom: 1em; } #archive-page .archive .archive-year:first-child { margin-top: 0em; padding-top: 0; } #archive-page .archive .archive-year h2 { margin-top: 1em; } #archive-page .archive .archive-year a { color: #333333; text-decoration: none; } #archive-page .archive .archive-post a { width: 100%; display: inline-flex; flex-flow: row nowrap; justify-content: flex-start; align-items: flex-start; text-decoration: none; } #archive-page .archive .archive-post a.child { padding-left: 32px; } #archive-page .archive .archive-post time { color: #333333; flex: none; font-size: 14px; padding: 0.5em 0.5em 0.5em 3em; } @media (max-width: 580px) { #archive-page .archive .archive-post time { padding: 0.5em 0.5em 0.5em 0; } } #archive-page .archive .archive-post .title { flex: auto; padding: 0.5em; font-size: 14px; color: #333333; } #archive-page .archive .archive-post .title i { color: #1BC3FB; } #archive-page .archive .archive-post .title i.music { color: #ff5722; } #archive-page .archive .archive-post .title i.red { color: #FE5F58; } #archive-page .archive .archive-post .title i.green { color: #3DC550; } #archive-page .archive .archive-post .title i.yellow { color: #FFBD2B; } #archive-page .archive .archive-post .title i.blue { color: #1BCDFC; } #archive-page .archive .archive-post .title i.theme { color: #1BC3FB; } #archive-page .archive .archive-post .title i.accent { color: #ff5722; } #archive-page .archive .archive-post .title i.orange { color: #ff5722; } .article { color: #333333; font-size: 16px; line-height: 1.7; word-break: break-all; word-wrap: break-word; } .article img { position: relative; margin: 0 auto; background: white; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } @media (max-width: 580px) { .article img { box-shadow: none; } } .article span img { display: inline; margin: auto; } .article .aplayer { margin: 0; display: inline-block; width: 400px; max-width: 100%; border-radius: 4px; color: #666; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; } .article p.small-img img, .article div.small-img img { width: auto; max-width: 100%; margin: 0; box-shadow: none; } .article p { margin-top: 0.5em; margin-bottom: 1em; max-width: 100%; overflow: auto; } .article p strong { color: #333333; padding-left: 2px; padding-right: 2px; } .article p .mjx-math { font-family: 'Ubuntu', Menlo, Monaco, courier, \"Lucida Console\", monospace, 'Source Code Pro', \"Microsoft YaHei\", Helvetica, Arial, sans-serif; background: rgba(244, 244, 244, 0.5); padding: 8px; border-radius: 4px; } .article ul, .article ol { font-size: 15.2px; list-style: initial; padding-left: 10px; margin-left: 10px; margin-bottom: 1em; } .article ul.center, .article ol.center { justify-content: center; } .article ul.pure, .article ol.pure { margin: 0; padding: 0; display: flex; flex-wrap: wrap; align-items: stretch; } .article ul.pure br, .article ol.pure br { display: none; } @media screen and (max-width: 900px) { .article ul.pure, .article ol.pure { justify-content: space-between; } } .article ul.pure li, .article ol.pure li { margin: 8px; display: flex; width: 75px; flex-direction: column; align-items: stretch; vertical-align: middle; text-align: center; font-size: 0.8em; line-height: 1.2em; overflow: hidden; } .article ul.pure li a, .article ol.pure li a { display: flex; flex-direction: column; align-items: center; text-align: center; } .article ul.pure li img, .article ol.pure li img { margin-bottom: 8px; } .article ul.pure.rounded img, .article ol.pure.rounded img { border-radius: 25%; } .article ul.pure.circle img, .article ol.pure.circle img { border-radius: 50%; } @media screen and (max-width: 900px) { .article ul.pure.about, .article ol.pure.about { justify-content: center; } } .article ul > li { list-style: initial; } .article ol > li { margin-left: 10px; list-style: decimal; } .article a { color: #1BC3FB; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .article a:before { display: none; } .article a:hover { color: #ff5722; text-decoration: underline; } .article a:active { color: #a22700; } .article h1, .article h2, .article h3, .article h4, .article h5, .article h6 { position: relative; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; font-weight: normal; margin-top: 1.5em; margin-bottom: 1em; } .article h1.title, .article h2.title, .article h3.title, .article h4.title, .article h5.title, .article h6.title { left: 0; } .article h1.title:before, .article h2.title:before, .article h3.title:before, .article h4.title:before, .article h5.title:before, .article h6.title:before { content: none; } .article h1, .article h2 { color: #262626; margin-top: 3em; border-bottom: 1px solid rgba(51, 51, 51, 0.1); padding-bottom: 0.2em; } .article h3:first-child, .article h4:first-child, .article h5:first-child, .article h6:first-child { margin-top: 0; padding-top: 0; } .article h1 { font-size: 24px; } .article h2 { font-size: 24px; text-align: left; } .article h3 { font-size: 20.8px; color: #262626; text-align: left; } .article h4 { font-weight: bold; font-size: 18.4px; } .article h5 { font-weight: bold; color: #333333; font-size: 16px; } .article h6 { color: rgba(51, 51, 51, 0.75); font-size: 14px; } .article .subtitle h6 { color: rgba(51, 51, 51, 0.9); } .article figure figcaption span { display: inline-block; margin-right: 5px; } .article blockquote { position: relative; width: 100%; font-size: 14px; background: rgba(27, 195, 251, 0.1); margin: 1em 0; padding: 16px; border-left: 4px solid #1BC3FB; border-radius: 4px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .article blockquote p { text-align: left; word-wrap: normal; margin: 0; font-size: 14px; line-height: 21px; } .article blockquote footer strong { margin-right: 7px; } .article blockquote.pullquote.right { border-left: none; border-right: 4px solid #1BC3FB; } .article blockquote.pullquote.right p { text-align: right; } .article pre { display: block; -moz-box-sizing: border-box; box-sizing: border-box; font-family: 'Ubuntu', Menlo, Monaco, courier, \"Lucida Console\", monospace, 'Source Code Pro', \"Microsoft YaHei\", Helvetica, Arial, sans-serif; color: #333333; } .article code { font-family: 'Ubuntu', Menlo, Monaco, courier, \"Lucida Console\", monospace, 'Source Code Pro', \"Microsoft YaHei\", Helvetica, Arial, sans-serif; padding: 3px 3px 0px 3px; margin: 0px 2px; vertical-align: center; border-radius: 2px; border: 1px solid rgba(27, 195, 251, 0.5); font-size: 12.8px; background: rgba(27, 195, 251, 0.1); } @media (max-width: 580px) { .article code { font-size: 12.16px; } } .article .readmore { font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; font-size: 0.8em; letter-spacing: 0.1em; margin-top: 16px; } .article .readmore a { text-decoration: none; display: inline-block; vertical-align: middle; line-height: 2rem; font-weight: bold; background-color: #1BC3FB; padding: 0.2em 2.4em; color: white; border-radius: 6px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .article .readmore a:hover { background: #04a8df; } .article .tags { position: relative; padding-top: 8px; padding-bottom: 8px; font-size: 14px; line-height: 1.7; margin-top: 16px; background: rgba(231, 231, 231, 0.5); word-spacing: 8px; } .article .tags a { color: #333333; position: relative; display: inline-block; word-spacing: 0; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .article .tags a:hover { color: #ff5722; background: transparent; text-decoration: none; } .article table:not('.highlight table') { width: 100%; } .article table:not('.highlight table') td, .article table:not('.highlight table') th { padding: 12px 24px; } @media (max-width: 580px) { .article ul, .article ol { font-size: 15.2px; } .article figure { font-size: 13px; line-height: 1.6em; } } .article .prev-next { width: 100%; display: flex; justify-content: space-between; align-content: flex-start; } .article .prev-next section { width: 100%; padding: 8px; color: rgba(51, 51, 51, 0.7); background-color: rgba(244, 244, 244, 0.5); border-radius: 12px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .article .prev-next section p { font-size: 16px; line-height: 1.7; margin: 0; } .article .prev-next section h4 { margin-top: 8px; margin-bottom: 8px; position: relative; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; font-weight: bold; font-size: 16px; } @media (max-width: 580px) { .article .prev-next section h4 { letter-spacing: -1px; } } .article .prev-next section h6 { margin: 0; word-spacing: normal; } .article .prev-next section .tags { background: transparent; padding: 0; margin-top: 8px; margin-bottom: 0; font-size: 12.6px; word-spacing: 4px; } .article .prev-next section:first-child { margin-left: 0; margin-right: 0; } .article .prev-next .prev { text-align: left; margin-left: 0; margin-right: 8px; border-top-right-radius: 12px; border-bottom-right-radius: 12px; } .article .prev-next .next { text-align: right; margin-left: 8px; margin-right: 0; border-top-left-radius: 12px; border-bottom-left-radius: 12px; } .highlight { position: relative; width: 100%; margin-top: 1em; margin-bottom: 1.2em; overflow: auto; display: block; background: #F4FAFE; font-size: 13.3px; font-family: 'Ubuntu', Menlo, Monaco, courier, \"Lucida Console\", monospace, 'Source Code Pro', \"Microsoft YaHei\", Helvetica, Arial, sans-serif; line-height: 1.7; border-radius: 4px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; /* Handle */ } .highlight figcaption { padding: 4px 8px; background-color: #e6f4fd; } .highlight table td, .highlight table th { padding: 0; } .highlight .gutter { width: 24px; padding: 0 12px; text-align: right; border-width: 0; margin-left: 0; background-color: #e6f4fd; } .highlight .gutter pre { color: rgba(51, 51, 51, 0.8); } .highlight .code { padding: 16px; vertical-align: top; border: 0px solid #efefef; } .highlight .code:before { content: \"\"; position: absolute; top: 0; right: 0; color: rgba(51, 51, 51, 0.8); font-size: 11.2px; padding: 4px 8px 0; line-height: 1.7; } .highlight.html .code:before { content: \"HTML\"; } .highlight.js .code:before { content: \"JS\"; } .highlight.bash .code:before { content: \"BASH\"; } .highlight.shell .code:before { content: \"SHELL\"; } .highlight.css .code:before { content: \"CSS\"; } .highlight.less .code:before { content: \"LESS\"; } .highlight.swift .code:before { content: \"SWIFT\"; } .highlight.objc .code:before { content: \"OBJECTIVE-C\"; } .highlight.c .code:before { content: \"C\"; } .highlight.java .code:before { content: \"JAVA\"; } .highlight.python .code:before { content: \"PYTHON\"; } .highlight.plain .code:before { content: \"\"; } .highlight::-webkit-scrollbar { height: 4px; width: 4px; } .highlight::-webkit-scrollbar-track-piece { background: transparent; } .highlight::-webkit-scrollbar-thumb { background: #ddeffc; cursor: pointer; border-radius: 4px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .highlight::-webkit-scrollbar-thumb:hover { background: #bce0f9; } @media (max-width: 580px) { .article .highlight { font-size: 12.635px; } } .art-item-footer { height: 40px; line-height: 1.7; font-size: 14px; } .art-item-footer .art-item-left, .art-item-footer .art-item-right { width: 50%; height: 40px; line-height: 40px; text-overflow: ellipsis; white-space: nowrap; overflow: hidden; } .art-item-footer .art-item-left { float: left; text-align: left; } .art-item-footer .art-item-right { float: right; text-align: right; } @media (max-width: 580px) { .art-item-footer { font-size: 12.635px; } } pre .line { color: rgba(51, 51, 51, 0.9); } pre .marked { background-color: rgba(255, 189, 43, 0.2); border-radius: 2px; border: 1px solid rgba(255, 189, 43, 0.4); } pre .title { color: #3f51b5; } pre .comment { color: rgba(61, 139, 64, 0.7); } pre .keyword, pre .javascript .function, pre .attr { color: #9c27b0; } pre .type, pre .built_in, pre .tag .name { color: #4BA7EE; } pre .variable, pre .attribute, pre .regexp, pre .ruby .constant, pre .xml .tag .title, pre .xml .pi, pre .xml .doctype, pre .html .doctype, pre .css .id, pre .css .class, pre .css .pseudo { color: #FD8607; } pre .number, pre .preprocessor, pre .literal, pre .params, pre .constant { color: #FD8607; } pre .class, pre .ruby .class .title, pre .css .rules .attribute { color: #ff9800; } pre .string { color: #3d8b40; } pre .value, pre .inheritance, pre .header, pre .ruby .symbol, pre .xml .cdata { color: #4caf50; } pre .css .hexcolor { color: #66cccc; } pre .function, pre .python .decorator, pre .python .title, pre .ruby .function .title, pre .ruby .title .keyword, pre .perl .sub, pre .javascript .title, pre .coffeescript .title { color: #6699cc; } .html .tag .name { color: #EE2B29; } .highlight { position: relative; } .btn-copy { display: inline-block; cursor: pointer; background-color: #FCFCFC; background-image: linear-gradient(#fcfcfc, #eee); border: 1px solid #d5d5d5; border-radius: 2px; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; -webkit-appearance: none; font-size: 13px; font-weight: 700; line-height: 20px; color: #666; padding: 2px 6px; position: absolute; right: 5px; top: 5px; opacity: 0; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } .btn-copy:hover { color: #444; } .btn-copy span { margin-left: 5px; } .highlight:hover .btn-copy { opacity: 1; } /* Pagination */ #page-nav { position: relative; width: 100%; padding: 20px 0px; } #page-nav .page-number, #page-nav .space { display: none; } #page-nav .next, #page-nav .prev { font-size: 0.8125em; font-weight: normal; color: #aaaaaa; border-radius: 2px; } #page-nav .next:hover, #page-nav .prev:hover { color: #444444; } #page-nav .next span, #page-nav .prev span { line-height: 20px; vertical-align: middle; } #page-nav .next span.icon, #page-nav .prev span.icon { position: relative; top: 1px; } #page-nav .next { float: right; padding: 0 7px 2px 10px; } #page-nav .prev { float: left; padding: 0 10px 2px 7px; } #u-search { display: none; position: fixed; top: 0; left: 0; width: 100%; height: 100%; padding: 60px 20px; z-index: 999999; } @media (max-width: 680px) { #u-search { padding: 0px; } } #u-search .modal { position: fixed; height: 80%; width: 100%; max-width: 640px; left: 50%; top: 0; margin: 64px 0px 0px -320px; background: #fff; box-shadow: 0 7px 8px -4px rgba(0, 0, 0, 0.2), 0 13px 19px 2px rgba(0, 0, 0, 0.14), 0 5px 24px 4px rgba(0, 0, 0, 0.12); z-index: 3; border-radius: 12px; overflow: hidden; } @media (max-width: 680px) { #u-search .modal { box-shadow: none; max-width: none; top: 0; left: 0; margin: 0; height: 100%; border-radius: 0; } } #u-search .modal .modal-ajax-content { opacity: 0; visibility: hidden; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } #u-search .modal .modal-ajax-content.loaded { opacity: 1; visibility: visible; } #u-search .modal .modal-header { position: relative; width: 100%; height: 64px; background-color: #1BC3FB; z-index: 3; border-top-left-radius: 12px; border-top-right-radius: 12px; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } @media (max-width: 680px) { #u-search .modal .modal-header { padding: 0px; border-radius: 0; } } #u-search .modal .modal-header .btn-close { display: block; position: absolute; width: 55px; height: 64px; top: 0; right: 0; color: white; cursor: pointer; text-align: center; line-height: 64px; vertical-align: middle; font-size: 1.3em; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; z-index: 2; } #u-search .modal .modal-header .btn-close:hover { transform: rotate(90deg); } #u-search .modal .modal-header .modal-loading { position: absolute; bottom: 0; left: 0; width: 100%; height: 2px; background: transparent; z-index: 1; } #u-search .modal .modal-header .modal-loading .modal-loading-bar { display: block; position: relative; width: 0%; height: 100%; background: #ffffff; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } #u-search .modal .modal-header #u-search-modal-form { position: relative; width: 100%; height: 100%; z-index: 2; } #u-search .modal .modal-header #u-search-modal-form #u-search-modal-input { width: 100%; padding: 0px 50px; height: 64px; font-size: 16px; line-height: 1.7; vertical-align: middle; color: white; border: none; background: transparent; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; font-weight: thin; appearance: none; box-shadow: none; } #u-search .modal .modal-header #u-search-modal-form #u-search-modal-input:focus { border-top-left-radius: 12px; border-top-right-radius: 12px; } #u-search .modal .modal-header #u-search-modal-btn-submit { position: absolute; top: 0; left: 0; padding-left: 5px; padding-top: 2px; background: transparent; border: none; width: 50px; height: 64px; vertical-align: middle; font-size: 1.3em; color: white; z-index: 2; } #u-search .modal .modal-footer { position: absolute; bottom: 0; left: 0; width: 100%; height: 50px; padding: 0px 15px; background: #fff; border-top: 1px solid #dddddd; } #u-search .modal .modal-footer .logo { position: absolute; top: 0; left: 0; width: 100%; height: 100%; text-align: center; z-index: 0; } #u-search .modal .modal-footer .logo a { display: inline-block; } #u-search .modal .modal-footer .logo.google img { height: 24px; margin-top: 13px; } #u-search .modal .modal-footer .logo.baidu img { height: 22px; margin-top: 14px; } #u-search .modal .modal-footer .logo img { position: relative; display: inline-block; width: auto; height: 18px; margin-top: 16px; } #u-search .modal .modal-footer .modal-error { position: relative; float: left; vertical-align: middle; line-height: 50px; font-size: 13px; z-index: 1; } #u-search .modal .modal-footer .modal-metadata { position: relative; float: left; vertical-align: middle; line-height: 50px; font-size: 13px; z-index: 1; } #u-search .modal .modal-footer .nav { position: relative; display: block; float: right; vertical-align: middle; font-size: 13px; font-weight: 500; line-height: 50px; color: #828282; cursor: pointer; z-index: 1; } #u-search .modal .modal-footer .nav:hover { color: #444444; } #u-search .modal .modal-footer .nav.btn-next { margin-left: 10px; } #u-search .modal .modal-footer .nav .icon { font-size: 12px; } #u-search .modal .modal-body { position: absolute; padding: 64px 50px 80px 50px; width: 100%; height: 100%; top: 0; left: 0; overflow-y: scroll; -webkit-overflow-scrolling: touch; background-color: white; border-radius: 12px; } @media (max-width: 680px) { #u-search .modal .modal-body { padding: 60px 20px 80px 20px; } } #u-search .modal .modal-body .modal-results { list-style: none; } #u-search .modal .modal-body .modal-results li { border-bottom: 1px solid #e6e8ea; } #u-search .modal .modal-body .modal-results li:last-child { border-bottom: none; } #u-search .modal .modal-body .modal-results .result { position: relative; display: block; padding: 15px 30px 15px 0px; text-decoration: none; } #u-search .modal .modal-body .modal-results .result:hover .title { color: #ff5722; } #u-search .modal .modal-body .modal-results .result .title { display: inline-block; max-width: 100%; color: #4d4d4d; font-size: 16px; font-weight: bold; padding: 1px; margin-bottom: 2px; line-height: 1.7; white-space: normal; overflow: hidden; text-overflow: ellipsis; } #u-search .modal .modal-body .modal-results .result .digest { display: block; white-space: pre-wrap; overflow: scroll; text-overflow: ellipsis; font-size: 14px; line-height: 1.7; color: #808080; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } #u-search .modal .modal-body .modal-results .result .icon { position: absolute; top: 50%; right: 0; margin-top: -4px; font-size: 11px; color: #828282; } #u-search .modal-overlay { position: absolute; top: 0; left: 0; width: 100%; height: 100%; background: rgba(0, 0, 0, 0.7); z-index: 1; } #footer { position: relative; padding: 40px 10px 120px 10px; width: 100%; color: rgba(51, 51, 51, 0.5); margin: 0px auto; font-size: 14px; overflow: hidden; text-align: center; font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; } #footer .licenses { color: rgba(51, 51, 51, 0.5); text-decoration: underline; } #footer .codename { text-decoration: underline; } #footer .social-wrapper { display: flex; justify-content: center; flex-wrap: wrap; margin: 4px 8px; } #footer a { color: rgba(51, 51, 51, 0.7); padding: 0; transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; } #footer a:hover { color: #ff5722; } #footer a.social { position: relative; display: inline-block; text-align: center; display: flex; justify-content: center; align-items: center; width: 32px; height: 32px; margin: 4px; border-radius: 100px; } #footer a.social:hover { background: rgba(27, 195, 251, 0.1); color: #1BC3FB; } @media (max-width: 768px) { #footer { justify-content: center; } } .article.typo.l_friends .friends-group h2 { font-size: 20.8px; } .article.typo.l_friends .friends-group .friend-content { display: flex; flex-wrap: wrap; margin: -8px; border-radius: 12px; } .article.typo.l_friends .friends-group .friend-content .friend-card { display: flex; border-radius: 12px; box-shadow: 0 1px 2px 0px rgba(0, 0, 0, 0.1), 0 2px 4px 0px rgba(0, 0, 0, 0.1); background: #eee; margin: 8px; color: rgba(51, 51, 51, 0.8); justify-content: flex-start; align-content: flex-start; flex-direction: column; border: 1px solid transparent; width: calc(100%/3 - 16px); } @media (max-width: 1024px) { .article.typo.l_friends .friends-group .friend-content .friend-card { width: calc(100%/3 - 16px); } } @media (max-width: 768px) { .article.typo.l_friends .friends-group .friend-content .friend-card { width: calc(100%/2 - 16px); } } @media (max-width: 580px) { .article.typo.l_friends .friends-group .friend-content .friend-card { width: 100%; margin: 0 4px; border-radius: 0; flex-direction: row; } .article.typo.l_friends .friends-group .friend-content .friend-card:first-child { border-top-left-radius: 12px; border-top-right-radius: 12px; } .article.typo.l_friends .friends-group .friend-content .friend-card:last-child { border-bottom-left-radius: 12px; border-bottom-right-radius: 12px; } } .article.typo.l_friends .friends-group .friend-content .friend-card:hover { text-decoration: none; box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1), 0 32px 64px 0px rgba(0, 0, 0, 0.1); transform: scale(1.05); border-radius: 12px; } @media (max-width: 580px) { .article.typo.l_friends .friends-group .friend-content .friend-card:hover { transform: scale(1.02); margin: 8px 0; } } .article.typo.l_friends .friends-group .friend-content .friend-card:hover .friend-left .avatar { transform: scale(1.2) rotate(12deg); box-shadow: 0 2px 4px 0px rgba(0, 0, 0, 0.1), 0 4px 8px 0px rgba(0, 0, 0, 0.1), 0 8px 16px 0px rgba(0, 0, 0, 0.1), 0 16px 32px 0px rgba(0, 0, 0, 0.1); } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-left { display: flex; align-self: center; } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-left .avatar { width: 64px; height: 64px; min-width: 64px; min-height: 64px; margin: 16px 8px 4px 8px; border-radius: 100%; border: 0px solid transparent; } @media (max-width: 580px) { .article.typo.l_friends .friends-group .friend-content .friend-card .friend-left .avatar { margin: 16px; } } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right { margin: 4px 8px; display: flex; flex-direction: column; text-align: center; } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right p { text-align: center; } @media (max-width: 580px) { .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right { margin: 16px 16px 16px 0; text-align: left; } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right p { text-align: left; } } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right .friend-tags-wrapper { transition: all 0.25s ease; -moz-transition: all 0.25s ease; -webkit-transition: all 0.25s ease; -o-transition: all 0.25s ease; margin-left: -2px; word-break: break-all; margin-bottom: 8px; } @media (max-width: 580px) { .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right .friend-tags-wrapper { margin-bottom: 0; } } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right p { margin: 0; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.15); } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right p.friend-name { font-family: Menlo, Monaco, 'Varela Round', \"Microsoft YaHei\", \"Source Sans Pro\", \"Helvetica Neue\", monospace, \"Lucida Console\", sans-serif, Helvetica, \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", Source Han Sans CN Regular, WenQuanYi Micro Hei, Arial, sans-serif; font-size: 16px; font-weight: bold; padding-top: 4px; } .article.typo.l_friends .friends-group .friend-content .friend-card .friend-right p.tags { font-size: 11.9px; display: inline; background: none; word-wrap: break-word; padding-right: 4px; }"}],"posts":[{"title":"Redis 面试题目汇总","slug":"Redis-面试题目汇总","date":"2022-12-05T08:09:03.000Z","updated":"2022-12-06T05:41:11.713Z","comments":true,"path":"2022/12/05/Redis-面试题目汇总/","link":"","permalink":"http://blog.zsstrike.tech/2022/12/05/Redis-%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E6%B1%87%E6%80%BB/","excerpt":"本文整理了 Redis 的相关问题，以便更深入掌握 Redis。","text":"本文整理了 Redis 的相关问题，以便更深入掌握 Redis。 Redis 介绍？ Redis 是一个内存数据库，不过和传统的 RDBM 不同，Redis 属于 NoSQL，其存储类型为 KV。Redis 被广泛用于缓存方向，同时也可用于分布式锁，高速消息队列和事件发布与订阅等方面。Redis 提供了多种数据类型来支持不同的业务场景，还支持持久化、Lua 脚本、多种集群（主从复制模式，分布式切片集群）方案。 Redis 使用场景？ 热点数据的缓存 限时业务应用，通过 expire 设置 key 的过期时间 计数器相关问题 分布式锁，通过 setnx 实现，通过 lua 脚本实现原子性 延时操作，下单 10 分钟后无操作自动取消订单，通过 Zset 实现 排行榜相关问题，使用 Zset 实现 点赞，好友等相互关系存储，使用 Set 的集合命令实现 队列，通过 list 可以实现简单队列，可以使用 Stream 提供消息队列支持，存在局限性 那 NoSQL 的 BASE 理论是什么？ BASE 理论是 CAP 理论中对一致性的妥协，和传统事务的 ACID 截然不同，BASE 不追求强一致性，而是允许数据在一段时间内是不一致的，但最终达到一致状态，从而获得更高的可用性和性能。 分布式缓存常见的技术选型方案有哪些？ 使用比较多的是 Memcached 和 Redis，Memcached 是分布式缓存最开始兴起的那会，比较常用的，现在基本使用 Redis。 Redis 和 Memcached 的区别和共同点？ 共同点：内存数据库，过期策略，性能高。 区别： Redis 支持更丰富的数据类型，Memcached 只支持最简单的 k&#x2F;v 数据类型 Redis 支持数据的持久化，灾难恢复机制，原生集群模式，Lua 脚本 Redis 采用的是 IO 多路复用模型，Memcached 使用多线程，非阻塞 IO 复用的网络模型 Redis 支持惰性删除和定期删除，但是 Memcached 只支持惰性删除 为什么使用 Redis 作为 MySQL 缓存？ 高性能：将热点数据放在缓存中，就不用访问数据库了，提升用户体验 高并发：数据在缓存中，支持更高级别的并发 Redis 对象机制解析？ 通过 void* ptr 指向实际的数据结构，使用 type 可以检查命令是否能够执行，检查 encoding 来选择合适的函数执行命令，实现了命令的多态；通过 lru 统计上一次访问时间，或者是 lfu 的相关信息，用于内存淘汰策略实现；通过 refcount 实现对象计数，便于进行对象共享（默认共享 10000 以内的整数）和内存回收。 Redis 是单线程吗？ Redis 单线程指的是其处理用户指令，解析请求，进行数据读写，发送数据给客户端任务都是一个线程内执行的，但是 Redis 程序并不是单线程的： Redis 2.6 版本之前，启动 2 个后台线程，用于关闭文件和 AOF 刷盘 Redis 4.0 版本之后，新增了一个后台线程（lazyfree），用来异步释放 Redis 内存。使用 del 会同步释放内存，处理大 key 的时候，可能造成卡顿，可以使用 unlink 命令 Redis 单线程模型详解？ Redis 基于 Reactor 单线程实现，通过 IO 多路复用程序来监听来自客户端的大量连接，减少了资源的消耗，Redis 服务器本身是一个事件驱动程序，主要分为时间事件和文件事件。当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时，与操作相对应的文件事件就会产生，对应的文件事件处理器就会被调用。 Redis 没有使用多线程？为什么不使用多线程？ Redis 其实在 4.0 之后就加入了对多线程的支持，在 6.0 后才算是使用了多线程，之前未使用的原因有：单线程编程容易维护，Redis 的性能瓶颈在内存和网络，多线程会带来上下文切换的开销等。 Redis 6.0 之后为何引入了多线程？ 主要是为了提高网络 IO 读写性能，因为这是其一个性能瓶颈，但是执行命令仍然是单线程顺序执行。因此，该版本之后，在 Redis 启动的时候，会创建以下线程： Redis-server：主线程，主要负责执行命令 bio_close_file，bio_aof_fsync，bio_lazy_free：后台线程，处理耗时任务 io_thd_1，io_thd_2，io_thd_3：三个 IO 多线程，分担 Redis 的网络 IO 的压力 Redis 大 key 如何处理？ 大 key：指 key 对应的 value 很大，如 String 类型值大于 10KB，或者元素个数大于 5000 个 大 key 影响： 客户端超时阻塞 引发网络阻塞 阻塞工作线程 内存分布不均（Slot 平均分配） AOF 写回策略为 Always 时，每次写大 key 都会阻塞较长时间 AOF 重写和 RDB 快照时，都会进行 fork，fork 中需要复制父进程的页表，此过程耗时并且阻塞主进程，另外，如果开启了内存大页会产生写放大的问题 通过以下方法找到大 key： redis-cli –bigkeys：最好在从节点上执行，只能返回每种类型的最大一个 bigkey 使用 SCAN 命令查找：使用 SCAN 扫描，使用 TYPE 查看类型，最后统计值大小 使用 RdbTools 工具查找大 key 删除大 key 的方式： 分批次删除：hscan，pop，sranmember，zremrangebyrank 异步删除：使用 unlink 代替 del 进行删除，不会造成阻塞 Redis 管道技术作用？ 是客户端提供的批处理技术，用于一次处理多个 Redis 命令，从而提升交互性能，可以解决多个命令执行时的网络等待。 Redis 给缓存数据设置过期时间有啥用？ 内存是有限的，节省内存资源；像 token 这类的数据存在时效性，如果利用传统数据库处理的话，这样更麻烦并且性能更差；在实现分布式锁的时候，可以防止获取了锁资源的进程意外宕机而造成锁资源一直没有释放的问题。 Redis 是如何判断数据是否过期的呢？ Redis 通过过期字典来保存数据过期的事件，对应键值保存着其过期时间戳。 过期的数据的删除策略了解么？ 惰性删除：只会在取出 key 的时候检查，对 CPU 友好，但是可能有太多过期 key 存在于缓存中 定期删除：每隔一段时间抽取一批 key 执行删除过期 key 操作 定时删除：设置过期时间的同时，创建一个定时器，定时器超时时执行删除操作 Redis 采用的是定期删除 + 惰性删除，但是仍然存在定期删除和惰性删除漏掉了很多过期 key 的情况，可能导致 OOM，为了解决该问题，使用 Redis 内存淘汰机制。 Redis 如何做内存优化？ 缩减键和值的长度，共享对象池，字符串优化，编码优化，控制 key 的数量。 Redis 内存淘汰机制了解么？ 一共有 8 种：volatile-lru，volatile-lfu，volatile-ttl，volatile-random，allkeys-lru，allkeys-lfu，allkeys-random，no-eviction。通过 maxmemory_policy 配置。 Redis 持久化机制(怎么保证 Redis 挂掉之后再重启数据可以进行恢复)？ 支持 RDB 和 AOF 两种持久化机制，最好使用混合持久化（aof-use-rdb-preamble）。RDB 优点是文件紧凑，占用空间小，恢复速度快，AOF 优点是发生故障时，丢失的数据比 RDB 更少。 AOF 文件会越来越大，最后磁盘都装不下？ 当文件体积过大时，会自动创建子进程，专门对 AOF 进行重写。重写过程中，还需要将命令记录在 AOF 重写缓冲区，重写完成后，AOF 重写缓冲区的内容会被追加进去。 AOF 重写是通过读取数据库中的键值对来实现的 Redis 持久化时，对过期键如何处理？ RDB 格式： 生成阶段：会提前检查 key，过期的将将不会保存在新的 RDB 文件中 加载阶段： 如果是主服务器，会对其中的键检查，过期键不会被载入 如果是从服务器，不会进行检查 AOF 格式： 写入阶段：会保存对应的过期键，在删除后会追加 DEL 命令 重写阶段：会检查过期时间，已过期的键不会保存到重写后的 AOF 文件中 Redis 事务？ 并不支持原子性和持久性，实际上可以理解为将多个命令的请求打包，然后再顺序执行其中所有命令，该过程不会被打断。即使命令产生错误，也不会进行回滚，其原因： Redis 事务执行时，错误通常是编程错误造成的，这种错误基本不会出现在生产环境中 不支持事务回滚是因为这种复杂功能和 Redis 追求的简单高效设计主旨不符 那Redis字符串有什么特点？ Redis 的字符串如果保存的对象是整数类型，那么就用 int 存储。如果不能用整数表示，就用 SDS 来表示，SDS 通过记录长度，和预分配空间，可以高效计算长度，进行 append 操作。 Hash 扩容过程是怎样的？ 当装载因子超过阈值时，就会进行 rehash 过程，将 0 号表上的每个桶慢慢移动到 1 号表，所以叫渐进式 rehash。 能详细说下Rehash过程吗？ 首先，生成新哈希表 ht[1]，为 ht[1] 分配空间。 然后，迁移 ht[0] 数据到 ht[1]。在 Rehash进行期间，每次对字典执行增删查改操作，程序会顺带迁移一个 ht[0] 上的数据，并更新偏移索引。 最后，ht[1] 和 ht[0] 指针对象交换。 如果字典正在 Rehash，此时有请求过来，Redis 会怎么处理？ 针对新增 Key，是往 ht[1] 里面插入。针对读请求，先从 ht[0] 读，没找到再去 ht[1] 找。删除和更新操作和读操作类似。 跳表的实现？ 本质上是对链表的一种优化，通过逐层跳步采样的方式构建索引，以加快查找速度。 跳表的每个节点有多少层？ 使用概率均衡的思路，确定新插入节点的层数。Redis 使用随机函数决定层数。直观上来说，默认1层，和丢硬币一样，如果是正面就继续往上，这样持续迭代，最大层数 32 层。 Redis 的 Zset 为什么同时需要字典和跳表来实现？ Zset 是一个有序列表，字典和跳表分别对应两种查询场景，字典用来支持按成员查询数据，跳表则用以实现高效的范围查询，这样两个场景，性能都做到了极致。 为什么使用跳表而不是平衡树？ 内存占用上，平衡树每个节点 2 个指针，跳表每个节点 1&#x2F;(1 - p) 个指针，Redis 中 p 为 0.25 做范围查找的时候，跳表比平衡树操作更加简单 算法实现上，跳表比平衡树简单 Redis 如何实现延迟队列？ 延迟队列指的是把当前要执行的任务，往后推迟一段时间再执行，如淘宝下单后超过一定时间未付款则自动取消订单任务。可以通过 Zset 实现，使用 score 当作执行时间戳，消费者通过 zrangebyscore 进行轮询处理。 Redis 机器挂掉怎么办？ 可以用主从模式部署，即有一个或多个备用机器，备用机会作为 Slave 同步 Master 的数据，在 Redis 出现问题的时候，把 Slave 升级为 Master。 主从可以自动切换吗？ 本身是不能，但是 Redis 已经有了解决方案，即哨兵模式。哨兵来监测Redis服务是否正常，异常情况下，由哨兵代理切换。为避免哨兵成为单点，哨兵也需要多机部署。 如果 Master 挂掉，会选择哪个 Slave 呢？ 当哨兵集群选举出哨兵 Leader 后，由哨兵 Leader 从 Redis 从节点中依次选择一个作为主节点 优先级最高的节点 复制偏移量最大的节点 runid 最小的节点 前面你提到了哨兵 Leader，那它是怎么来的呢？ 当一个哨兵节点确认Redis集群的主节点主观下线后，会请求其他哨兵节点要求将自己选举为 Leader。如果一个哨兵节点获得的选举票数超过节点数的一半，且大于 quorum 配置的值，则该哨兵节点选举为Leader；否则重新进行选举。 为什么 Redis 集群的哈希槽是 16384（2 的 14 次方）？ Redis 在发送心跳包的时候需要将哈希槽的指派信息一起进行发送，如果使用 CRC16 原始范围，会造成心跳包过大，另一方面，Redis 集群一般不超过 1000 个节点，所以 16k 的槽位是个不错的选择。 为什么 Redis 集群中不建议使用发布订阅？ 在集群模式下，所有的 publish 命令都会向所有节点（包括从节点）进行广播，造成网络带宽的严重消耗。 Redis 集群会有写操作丢失吗？ 会，Redis 并不保证数据的强一致性，在 failover 过程中可能存在数据丢失。 Redis 性能这么高，那它是协程模型，还是多线程模型？ Redis 是单线程 Reactor 模型，通过高效的 IO 复用以及内存处理实现高性能。6.0 之后虽然加入了多线程来进行 IO 解包，但是处理逻辑依旧是单线程。 另外，如果考虑到 RDB 的 Fork，一些定时任务的处理，那么 Redis 也可以说多进程，这没有问题。但是 Redis 对数据的处理，至始至终，都是单线程。 可以详细说下6.0版本发布的多线程功能吗？ 多线程功能，主要用于提高解包的效率。和传统的 Multi Reactor 多线程模型不同，Redis 的多线程只负责处理网络 IO 的解包和协议转换，一方面是因为 Redis 的单线程处理足够快，另一方面也是为了兼容性做考虑。 如果数据太大，Redis 存不下了怎么办？ 使用集群模式，也就是将数据分片，不同的 Key 根据 Hash 路由到不同的节点。集群索引是通过一致性Hash 算法来完成，这种算法可以解决服务器数量发生改变时，所有的服务器缓存在同一时间失效的问题。 一致性Hash能详细讲一下吗？ 传统的 Hash 分片，在节点扩容或者缩容的时候，需要重新哈希。一致性Hash是说将数据和服务器，以相同的 Hash 函数，映射到同一个 Hash 环上，针对一个对象，在哈希环上顺时针查找距其最近的机器，这个机器就负责处理该对象的相关请求。这样在节点数量增加时，只有少量的数据需要重新哈希。 如何设计一个缓存策略，可以动态缓存热点数据呢？ 通过数据最新访问时间来做排名，并过滤掉不常访问的数据，只留下经常访问的数据 通过缓存系统做一个排序队列（存放 1000 个商品），系统根据访问时间进行排序 系统定期删除排名最后的 200 个商品，并且从数据库中随机读取 200 个商品加入队列中 Redis 经常用作缓存，那数据一致性怎么保证？ 从设计思路来说，有 Cache Aside 和 Read&#x2F;Write Through 两种模式，前者是把缓存责任交给应用层，后者是将缓存的责任，放置到服务提供方。 如果数据发生变化，你会怎样去更新缓存？ 一般有四种方式： 数据存到数据库中，成功后，再让缓存失效，等到读缓存不命中的时候，再加载进去； 通过消息队列更新缓存； 先更新缓存，再更新服务，这种情况相当于把 Cache 也做 Buffer 用； 启动一个同步服务，作为 MySQL 一个从节点，通过解析 binlog 同步重要缓存 说一下布隆过滤器的实现吧？ 底层是一个 64 位的整型，将字符串用多个 Hash 函数映射不同的二进制位置，将整型中对应位置设置为1。布隆过滤器优缺点都很明显，优点是空间、时间消耗都很小，缺点是结果不是完全准确，其能提供的信息是某样东西一定不存在或者可能存在。 那 Redis 可以做消息队列吗？ 可以，但是并不合适，Redis 本身没有支持 AMQP 规范，消息队列该有的能力缺胳膊少腿，消息可靠性不强。甚至 Redis 作者都看不下去了，开源了 Disque 来专事专做。 那你能谈谈 Redis 在秒杀场景的应用吗？ Redis 主要是起到选拔流量的作用，记录商品总数，还有就是已下单数，等达到总数之后拦截所有请求。可以多放些请求进来，然后塞入消息队列。 你能继续说说 Redis 在分布式锁中的应用吗？ 分布式锁也依赖存储组件，针对请求量的不同，可以选择 Etcd、MySQL、Redis 等。前两者可靠性更强，Redis 性能更高。 基于 Redis 实现分布式锁有什么优缺点？ 优点：性能高效，实现方便，避免单点故障 缺点： 超时时间不好设置 主从复制模式中的数据是异步复制的，这样导致分布式锁的不可靠性，可以使用 RedLock，其基本思路是让客户端和多个独立的 Redis 节点依次请求申请加锁，如果客户端能够和半数以上的节点成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败 那我们再聊聊 Redis 在限流场景的应用吧？ 在微服务架构下，限频器也需要分布式化。在分布式令牌桶中，Redis 负责管理令牌，微服务需要进行函数操作，就向 Redis 申请令牌，如果 Redis 当前还有令牌，就发放给它。拿到令牌，才能进行下一步操作。另一方面，令牌不光要消耗，还需要补充，出于性能考虑，可以使用懒生成的方式：使用令牌时，顺便生成令牌。这样子还有个好处：令牌的获取，和令牌的生成，都可以在一个 Lua 脚本中，保证了原子性。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.zsstrike.tech/tags/Redis/"}]},{"title":"《操作系统原理与设计》笔记","slug":"《操作系统原理与设计》笔记","date":"2022-10-10T13:57:12.000Z","updated":"2022-10-13T11:07:47.332Z","comments":true,"path":"2022/10/10/《操作系统原理与设计》笔记/","link":"","permalink":"http://blog.zsstrike.tech/2022/10/10/%E3%80%8A%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86%E4%B8%8E%E8%AE%BE%E8%AE%A1%E3%80%8B%E7%AC%94%E8%AE%B0/","excerpt":"本文用于整理《操作系统原理与设计》课程上所使用 Slides 的笔记，以备查阅。","text":"本文用于整理《操作系统原理与设计》课程上所使用 Slides 的笔记，以备查阅。 第一章 操作系统概览计算机系统概览： 系统组织：由 CPU 连接总线，通过设备控制器，和其他 IO 设备进行交互 存储结构：采用三角形式的存储结构，通过缓存来提高性能，也通过 DMA 技术提高外存性能 系统架构：多处理器系统，多核系统，集群系统 操作系统概览： 多道程序（multiprogramming）：用于提高 CPU 利用率 多任务处理（multitasking）：用于提高任务的可交互性 中断驱动理念：硬件中断通过设备产生，软件中断（exception&#x2F;trap）则由程序错误，系统调用等产生 双模式运作（dual-mode）：通过 mode bit 来区分是 user mode 还是 kernel mode，后者具有更高权限 系统调用：由内核提供给上层的 api，用于屏蔽与底层硬件交互的过程 操作系统标准：POSIX 和 BSD，分别实现的操作系统有 Linux 和 MacOS 操作系统组件： 进程：是程序的运行时实例，在 Linux 中通过树结构维护父子关系，需要提供同步，交互，死锁处理等功能 内存：是 CPU 能直接访问的最大的存储介质，需要提供内存分配，映射等功能 存储：文件系统的设计，独立于操作系统，用于高效管理持久化介质 杂项： 保护和安全：通过用户 ID 来决定该用户的权限 计算环境：传统计算机和现代手机，Client-Server 和 P2P，虚拟化，云计算 开源系统：Linux，Unix，Android 第二章 操作系统结构操作系统提供的服务： 对用户：程序执行，IO 操作，文件系统管理，进程通信，错误检测，用户界面 对系统：资源分配，审计系统，保护与安全 系统调用：对操作系统所提供服务的 API，方便上层用户使用，主要分为： 进程控制：load，execute，end，abort 文件管理：open，close，read，write 设备管理：request devices，release devices，read，write 信息维护：get time of day，dump memory 信息交互：message-passing 和 shared-memory 保护：get_permission，allow_user，set_permission，deny_user 操作系统结构： 简单结构：MS-DOS 宏内核（Monolithic Structure）：将所有的功能在内核处实现 微内核：将尽可能多的功能移动到用户态，内核部分只保留核心功能 模块化：提供可加载的内核模块，更加灵活高效 混合：结合多种不同的架构 第三章 进程程序：指的是一系列代码，将 C 语言编译为可执行文件通常需要： 预处理器：用于处理像 #define, #include, #ifdef 等指令 编译器：对处理后的代码进行词法语法分析，生成中间码（汇编码） 优化器：对中间代码进行优化，如循环展开等 汇编器：将中间代码生成为机器码，以 .o 结尾 链接器：用于将所有 .o 文件进行链接，分为静态链接（.a）和动态链接（.so） 静态链接时，将对应的库文件和 .o 文件合并，生成的可执行文件较大 动态链接时，只需要检查 .o 文件使用的函数是否在库文件中声明即可，生成的可执行文件较小 进程：指的是运行中的程序，此时程序已经被加载到内存中，处于活动状态 内存：代码段，数据段，堆，栈，程序计数器，寄存器 状态：新建，就绪，运行，等待，退出 PCB：进程控制块，处于内核空间中，包含有进行运行所需的一切信息 进程操作： 进程识别：getpid 进程创建：进程间存在父子关系，Linux 中为孤儿进程提供 re-parent 操作，使得进程按照树形维护，Windows 则允许孤儿进程成为树根，按照森林维护。进程创建使用 fork： 克隆的内容：程序代码，内存，打开文件表，程序计数器 不克隆的内容：fork 返回值（子进程 pid 或者 0），pid，运行时间 进程执行：fork 只能使得子进程运行相同的程序，可以通过 exec* 函数来执行新的程序 通过 exec* 函数执行新程序，执行完并不会返回到原来的程序中执行 exec* 函数将会丢弃掉原来进程的内存，寄存器值，但是会保存 pid，进程关系，以及运行时间等 进程等待： wait：若存在运行中的子进程，则父进程被挂起，直到有一个子进程进入退出态，否则不会挂起 waitpid：可以等待某个子进程状态的变化 内存被分为： 内核空间：存储内核数据结构，内核代码，设备驱动等，只允许内核代码访问 用户空间：存储进程的所需内存，进程执行程序的代码，内核代码和用户代码都可以访问 进程操作系统调用的内部实现： getpid：用户态程序调用 getpid 时，会进行上下文切换，此时处于内核态，可以直接访问到进程对应的 PCB，从而得到 pid，之后再次上下文切换到用户态继续执行原来的代码 fork：在内核态，进程（PCB）以双向链表连接起来，当调用 fork 时： 首先在内核空间复制一份和父进程相同的 PCB，但是需要修改对应的 PID，运行时间，父亲孩子指针 之后，将新创建的 PCB 添加到双向链表中 在用户态，进程的内存空间同样会被复制一份（写时复制） 子进程和父进程共享相同的打开文件表，因此都会输出到 shell 中 exec*： 首先根据给定的参数在系统中寻找对应的程序 然会根据 PCB 对进程用户态空间（局部变量，动态分配空间，全局变量，代码段）进行设置或清除 最后重置寄存器值，如将程序计数器 wait + exit： 当子进程调用 exit 时，内核态根据 PCB 找到用户态空间，将其释放，但是此时内核态进程链表中仍然存在该进程项，此时进程成为僵尸进程，同时内核会向其父进程通过 signal 来通知 当父进程调用 wait 时，内核将会为其设置一个 signal handling routine，用于在接收到对应的信号时执行，同时，内核此时将会挂起父进程，当对应的信号到来时，对应的 routine 被执行，默认情况下将会移除子进程项，释放其内核态资源，最后，这样的 signal handleing routine 被移除，同时返回到原来父进程的用户态。若在子进程结束之后再调用 wait，同样也会触发上述一系列动作 孤儿进程回收：init 进程将成为其父进程，并且 init 会周期性调用 wait 第四章 线程并发和并行：前者表示在某段时间内多个程序在执行，后者则表示在某个时刻多个程序同时在执行 线程：轻量级进程，是 CPU 运行的基本单位，相对于进程，线程的创建代价更小，而且相同进程下创建的线程还能够共享部分资源，对应的资源消耗也少一些 相同进程内线程内存共享情况： 共享：代码段，全局变量，动态内存，文件打开表等 非共享：栈，程序计数器，寄存器值 线程优点：更高的响应性，便于数据共享，经济，可扩展性强 线程模型：用于表示内核态线程和用户态线程之间的关系 多对一：容易实现，但是一旦调用一个阻塞式系统调用，其他线程将不会得到执行 一对一：更高的并发度，但是此时并不能创建过多的线程，因为内核态内存有限，Linux 和 Windows 使用 基于进程的调度器：只对进程进行调度，但是进程内哪个线程被执行需要库文件提供支持 基于线程的调度器：只对线程进行调度，linux 内核 2.6 以后支持 多对多：结合上述优点 多线程编程： 线程创建：pthread_create，需要定义对应的线程函数 线程等待：pthread_join，可以等待特定的子线程，和 wait 实现原理相同 线程标识：pthread_self，返回 pthread_t，结构体 线程退出：pthread_exit，线程主动退出 线程中止：pthread_kill，中止正在执行的线程 隐式多线程：ThreadPool，OpenMP 线程带来的问题： 系统调用实现： fork：进程将会复制所有的线程或者只会复制对应的调用者线程 exec：直接修改进程的代码段，因此所有线程都会被影响 signal：当一个 signal 到来时，应该将其分发给哪个线程，pthread_kill(pthread_t tid, int signal) Thread-Local Storage：每个线程独有的存储空间，用于存储特定线程的数据，和 static data 类似 第五章 进程通信与同步进程间通信模型： 共享内存 消息传递 进程间通信方式： POSIX 共享内存：通过将内存区域和文件关联起来，主要涉及 shm_open，ftruncate，mmap，shm_unlink 套接字：通过 IP 和 port 来唯一定义，可以通过套接字实现消息传递 管道：是一个共享对象，可以用来让进程间间通信 普通管道：只用于存在亲缘关系的进程间，单向，在 Unix 中通过 fork，父进程关闭读端，子进程关闭写端，就可以构成该普通管道 命名管道：可以在不具有亲缘关系的进程间通信，可能有多个写端，半双工，在 Unix 中称为 FIFOs，相关 api 如 mkfifo，open，read，write，close Race Condition：指的是程序的输出依赖于共享对象的被访问顺序，如 A 进程执行 X += 10，B 进程执行 X -= 10，最终结果可能是 -10, 0, 10，其结果依赖于程序间的执行顺序 进程互斥： 通过定义临界区方案可以实现进程互斥，其主要定义四个部分： 进入区：定义了进入临界区前需要的操作，如检查资源是否可用 临界区：表示此时正在操作共享对象 退出区：定义了退出临界区后需要的操作，如重新设置访问状态等 剩余区：程序的其他代码 临界区方案需要满足的条件： 互斥：两个进程不能同时在临界区内 前进：只有在临界区内的进程才能阻塞其他进程 有限等待：进程不应该一直等待进入临界区 临界区方案的实现方案： 屏蔽中断：和非抢占式内核原理相同，在进入区屏蔽中断，退出区允许中断，这样在临界区内的进程不会发生上下文交换，从而保证了互斥，该方法存在性能损失 互斥锁：使用锁来表示是否允许进程进入临界区，通常使用硬件指令，以自旋锁的方式实现，但是存在 CPU 消耗 严格轮换：定义一个共享变量，用来表示现在哪个进程被允许进入临界区，但是违背了前进条件，并且不高效 Peterson’s solution：先检查其他进程，若其他进程想要进入临界区，则让它们先进入，但是可能存在优先级反转问题，即高优先级的进程一直在让其他进程进入临界区 信号量：既可以用于线程互斥，也可以用于线程同步。上面的互斥锁，严格轮换和 Peterson’s solution 在进入区若条件不满足的时候，采用的方案都是 CPU 忙等，会消耗大量 CPU 资源，而是用信号量则不存在该问题，因为其会挂起和唤醒进程 12345678910111213141516171819202122232425typedef struct &#123; int value; struct process * list;&#125; semaphore;void down(semaphore *s) &#123; disable_interrupt(); while (*s == 0) &#123; enable_interrupt(); special_sleep(); disable_interrupt(); &#125; *s = *s - 1; enable_interrupt();&#125;void up(semaphore *s) &#123; disable_interrupt(); if (*s == 0) &#123; special_wakeup(); &#125; *s = *s + 1; enable_interrupt();&#125; 使用 disable_interrupt 来保证操作的原子性，在 special_sleep 之前需要 enable_interrupt，因为此时可以让出 CPU 资源给其他应用使用。 死锁：两个或多个线程由于相互等待而永远被阻塞的情况 条件：互斥，保持并等待，非抢占，循环等待 处理： 死锁检测和恢复：通过资源分配图，检测其中是否存在环路，然后让其中一个进程释放掉所占用资源 死锁预防：通过合理的资源分配，防止死锁产生，如银行家算法 鸵鸟算法：直接忽略死锁问题 死锁与饥饿：饥饿是指一个线程长时间得不到需要的资源而不能执行的现象，并不表示存在死锁 进程同步问题： 生产者-消费者问题：通过使用 mutex 信号量实现资源互斥访问，通过 empty 和 full 信号量实现同步 哲学就就餐问题： 最多允许 4 个哲学家同时拿筷子，此时不会存在循环等待 前四个哲学家先拿左边的筷子，最后一个哲学家先拿右边筷子（或者按照奇偶区分） 定义拿筷子这个操作是不可分的，定义 mutex &#x3D; 1 读者-写者问题：定义 write 信号量表示是否可写，定义 read_mutex 表示是否可读，同时定义 read_count，在其从 0 变为 1 时，down(&amp;write)，在其从 1 变为 0 时，up(&amp;write)，该方案可能存在写操作进程饥饿的情况 第六章 进程调度进程状态：转移过程图如下，其中在 waiting 状态时，分为可中断和不可中断，可中断则表示程序能够响应像 Ctrl + C 这样的命令，不可中断则表示该程序非常希望得到对应的资源 进程调度： 发生时刻： 一个新进程被创建 一个在正执行的进程被终结或者进入了等待状态 一个进程从 waiting 状态转移到 ready 状态 上下文切换：进程调度决定接下来选择哪个进程执行，而上下文切换则是实际的切换过程，切换过程将会保存当前进程的程序计数器和寄存器值，同时加载新的进程对应的程序计数器和寄存器值到对应寄存器中 类型： 抢占式调度：进程除了主动放弃 CPU 资源外，还可能存在其他事件让其主动放弃 CPU，如分时系统 非抢占式调度：当一个进程被调度执行，其只有进入终结态或者等待态才会允许其他进程执行 性能指标： 周转时间：从进程提交到结束所花费的时间 等待时间：进程在 ready_queue 中的等待时间 响应时间：从进程提交到首次执行的时间 调度算法： FIFO：非抢占，平均等待时间长 SJF：既可以是抢占式，也可以是非抢占式，长事务可能被饿死 RR：抢占式，很难合理控制时间片大小 Priority Scheduling：抢占式 多级优先队列：Linux 实际使用的方案，不同优先级上时间片大小不同，调度算法也不同 实时调度算法： 单调速率算法（RMS）：根据周期来定静态优先级，然后进行调度，但是不保证一系列进程能够被正确调度 最早截至日期调度（EDF）：根据截至日期指定优先级 Linux 调度器： 第七章 内存管理用户空间的内存管理： 内存空间：用户态空间按照段式内存管理，将进程空间分为不同的段，在 32 位系统中，每个进程的最大内存被限制为 4GB，注意，每个进程的内存段是相互独立和分离的 代码段（.text）：用于存储程序代码和常量的地方，该部分内存只读 数据段（.data &amp; .bss）： .data 段用于存储已经初始化了全局变量和静态变量 .bss 段用于存储尚未初始化的全局变量和静态变量，其会在其中记录变量对应的大小，在程序执行时分配，这样可以减少程序分发时的大小 栈：包含局部变量，函数参数，程序参数以及环境变量，函数调用以帧的形式实现，可以读写不在当前帧的其他局部变量，方便数据共享 堆：动态分配内存空间，当外部用户通过 malloc 进行内存获取时，如果剩余可用空间不够，其会通过 brk 系统调用来改变堆大小，保证其能够正常进行分配，空闲内存管理方式有 隐式空闲链表：size &amp; is_allocated，但是很难和前面的空闲块合并，可以增加 footer 信息 显式空闲链表：在空闲块增加指针（pre &amp; next） 分离空闲链表：按照不同大小来组织空闲链表，实际使用如 Linux 中的伙伴系统 空间分配可能带来问题：外部碎片和内部碎片 段错误：当访问（读或者写）了不该被访问的内存，操作系统将会返回段错误 内核空间的内存管理： 虚拟内存地址和物理内存地址：CPU 所处理到的内存地址都是某个程序的虚拟内存地址，需要经过翻译后才能转变为物理地址，访问实际的物理存储介质 不同进程使用相同的虚拟地址，最终可能被翻译为不同的物理地址 虚拟地址可以实现内存共享，只需要同时让其被翻译为被共享物理地址 进程内存可以增长：解决了外部碎片，不需要物理上连续的内存空间来保存进程 MMU：用于将虚拟地址转换为物理地址的硬件，存在于 CPU 上，实际上是一个查找表，每个进程都对应一个查找表，只有在运行态才会被加载到 MMU 中，其他时候则在内核空间中 页式内存管理：为了防止查找表过大，按照页进行转换，因此，内存分配的最小单位变为页大小，此时存在内部碎片的问题 多级页表：用于缓解页表过大的问题，但是会造成地址转换速度下降的问题 TLB：用于缓存最近转换过的地址，加速下次地址转换速度 demand paging：在我们调用 malloc 的时候，程序执行的速度很快，因为此时只是分配虚拟内存地址，在真正访问的时候此时才会加载物理内存，该情况称为 demand paging swap area：永久存储介质中用于存放物理页的存储空间，在物理页被全部占用，但此时仍然有新的 page fault 产生时，将会选择逐出某些物理页 copy on write：在 fork 后，父子进程共享相同的物理页，但是如果子进程需要写的物理页是，此时才会发生复制 页替换： 算法 Optimal FIFO LRU，Second-chance algorithm，CLOCK 相关问题： Belady 异常：物理帧数增加可能会导致缺页中断个数增加，在 FIFO 中出现，在 LRU 和 Optimal 中不会出现 Thrashing（抖动）：如果被更换页面是一个很快会被再次访问的页面，则将再次缺页中断，音响系统的执行效率 Linux 内存管理：使用伙伴系统分配空间，粒度为页，对于小对象可能还是太大，可以引入 Slab 分配器 内存文件映射：将文件加载到物理页中，使得操作文件像操作内存一样，提升了 IO 性能 第八章 存储管理硬盘： 结构：盘片，磁道，扇区，柱面，通过 逻辑块号 -&gt;（柱面，磁道，扇区） 进行定位 访问：寻道延迟 + 旋转延迟 + 传输延迟 密度： Constant linear velocity：CD&#x2F;DVD，需要不同的旋转速度来保证数据传输速率不变 Constant angular velocity：HDD，保持恒定的转速即可 使用方式： 文件系统：通过文件系统来进行使用，文件系统将会维护整个磁盘上面的数据 Raw Disk：不通过文件系统，直接操作磁盘 调度方式：FCFS，SSTF，SCAN，C-SCAN，LOOK，C-LOOK RAID： RAID0：不存在冗余，一个整体数据被划分到几个硬盘上存储，提高了数据的访问速度 RAID1：数据仍然被划分到几个硬盘上存储，每个硬盘复制一份，存在较大的存储开销 RAID01 + RAID10： RAID4：增加一个 DISK，专门用于进行校验和纠错，更新代价大 RAID5：和 RAID4 类似，但是校验数据块被分散存储在不同的硬盘中，更新代价大 RAID6：和 RAID5 类似，但是增加了两个校验块，容错性更大，更新代价大 第九章 文件系统文件系统两种数据类型： 文件：用于存储数据的最小单元，包含有文件属性和文件内容，文件名可以重复，但是路径名不可重复 目录：一种特殊的文件，其文件内容用于记录当前目录下的子文件和目录信息 文件系统操作流程： 文件创建：实际上是更新某个目录文件的内容 文件打开：进程提供路径名给操作系统，操作系统根据路径名在磁盘上找到文件属性对应的数据块，然后将文件属性复制到对应的文件打开表中，最后返回给进程该文件的文件描述符 文件读取：进程提供文件描述符给操作系统，操作系统根据文件描述符，找到对应的文件信息，硬盘返回数据，首先在内核态缓存，之后再复制到用户进程空间中 文件系统实现： Contiguous allocation：外部碎片 + 文件增长问题 Linked-list allocation：blocking + linked list，随机访问问题 Linked-list allocation: centralized next-block # (FAT)，需要缓存整个 FAT index-node allocation：将 FAT 分为不同的 index node，按照 index node 组织文件 硬链接：指的是创建不同的目录项，指向了相同的 inode 软连接：创建了新的 inode，里面存储了是原来文件的 pathname 文件系统信息存储：在 FAT 上存储 Boot Sector，在 Ext 上存储 Superblock，里面包含了文件系统的信息 分区：将磁盘划分为不同的分区，每个分区上的文件系统可以不同，硬盘需要存储 Boot Code 和分区表 第十章 输入输出系统和设备控制交互的方式： Direct IO instruction Memory Mapped IO：设备的数据和寄存器被映射到地址空间中 IO 交互方式： 轮询：CPU 一直轮询 busy 寄存器，直到其为 0，此时才能继续进行 IO 操作 中断：IO 发起中断，CPU 接收到中断后根据中断向量得到处理程序的地址，分为可屏蔽中断和非屏蔽中断，可屏蔽中断指 CPU 可以不响应该中断，非屏蔽中断则表示 CPU 必须执行中断处理函数 DMA：CPU 只需要发送数据地址给 DMA 控制器，DMA 在搬运完整个块时再来通知 CPU 已完成数据搬运 IO 设备：块设备，字符设备，网络设备 IO 方法：同步（阻塞和非阻塞）和异步，异步能提供更高的 CPU 利用率 IO 调度：每个设备维护一个等待队列，可以提供第等待时间和公平性 Buffer：存储数据的中间站，用于缓解不同处理速度和传输速度的问题 Cache：用于提高系统的性能，减少访问延迟","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://blog.zsstrike.tech/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Operating System","slug":"Operating-System","permalink":"http://blog.zsstrike.tech/tags/Operating-System/"}]},{"title":"《深入理解计算机系统》笔记","slug":"《深入理解计算机系统》笔记","date":"2022-09-27T03:01:21.000Z","updated":"2022-10-08T11:07:57.481Z","comments":true,"path":"2022/09/27/《深入理解计算机系统》笔记/","link":"","permalink":"http://blog.zsstrike.tech/2022/09/27/%E3%80%8A%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%AC%94%E8%AE%B0/","excerpt":"本文用于记录《深入学习计算机系统》中的知识点，以备查阅。","text":"本文用于记录《深入学习计算机系统》中的知识点，以备查阅。 第二章 信息的表示和处理字节顺序：大尾端（sun，sparc）和小尾端（x86，arm），看低地址放的是 MSB 字节还是 LSB 字节 位操作：&amp; | ～ ^ &lt;&lt; &gt;&gt; &gt;&gt;&gt; 整数表示： 无符号：$\\sum_{i &#x3D; 0}^{w - 1}x_i * 2^i$​​​​，有符号：$-x_{w - 1}*2^{w - 1} + \\sum_{i &#x3D; 0}^{w - 2}x_i * 2^i$​​​ 无符号与有符号之间的变换：保持每个位不变，但是按照转换后的整数类型解释 如果表达式中既有无符号整数，也有有符号整数，那么有符号整数会被解释成无符号整数（ux &gt; -1） 在判断表达式真假的时候，可以使用 0，-1，Tmin 等特殊值检验 扩展：符号扩展，0 扩展 截断：去掉最高的那些位 除法：整数除法中由于截断，正整数结果趋近 0，负整数结果远离 0 浮点数表示： IEEE 标准：采用 $(-1)^s M * 2^E$​ 表示，注意阶码偏移（单精度 127），隐含 1（E 全为 0 除外） 单精度编码规则如下图： 规格化时，此时 E &#x3D; exp - bias，M 前缀 1，非规格化时，此时 E &#x3D; -bias + 1，而不是 -bias，M 前缀 0 第三章 程序的机器级表示C 语言程序通过编译器产生汇编文件，汇编文件通过汇编器产生二进制文件，二进制文件通过链接器产生可执行文件。 x86-64 寄存器：%rax，%rbx，%rcx，%rdx，%rdi，%rsi，%rbp，**%rsp**，%r8-%15 数据传输：movq src, dst，可以是立即数，寄存器，内存数据，但是 src 和 dst 不能同时是内存数据 内存寻址模式：(R)，D(R)，D(Rb, Ri, S) 地址计算指令：leaq，计算出新的地址，不改变条件码 算术指令：op src, dst，表示 dst &#x3D; dst op src 条件码： 种类：CF，ZF，SF，OF 隐式设置：算术运算 显式设置：cmpq 和 testq 显式读取：setX dest，最低字节根据条件被设置为 0 或者 1，通常和 movzbl 一起使用 条件跳转：jmp，je(zero&#x2F;equal)，jne，ja，jb，jg，jl，jge，jle 条件 mov：如 cmovle，可能对于三目运算符有副作用 循环：在汇编中，都是通过 jmp 和 label 构成 switch 语句：通过 jumtable 实现，但是如果是稀疏格式，则使用决策树格式（if-elseif-else） x86-64 栈：向低地址区域增长，%rsp 指向栈顶 pushq src：%rsp -&#x3D; 8，stack[%rsp] &#x3D; src popq dest：dest &#x3D; stack[%rsp]，%rsp +&#x3D; 8 控制转移： call label：返回地址压栈，jmp label ret：返回地址出栈，jmp retaddr 数据转移： 函数参数：前 6 个参数分别存储在 %rdi，%rsi，%rdx，%rcx，%r8，%r9；后面参数压栈 返回值：%rax 局部参数：存储在栈中，注意对齐 对于每个函数，使用 Frame（帧）来进行数据管理，从而保证代码可重入，%rbp 用于指向上一个栈帧的位置，对于寄存器保护，一般由操作系统约定： 调用者保存：%rax，参数寄存器，%r10，%r11，浮点数寄存器 被调用者保存：%rbx, %r12, %r13, %r14, %r15，%rbp，%rsp 数组：一系列相同类型的数据，在内存中连续存储，多维数组和多级数组的区别 int A1[3][5] ：表示 3 * 5 的数据 int *A2[3][5] ：表示 3 * 5 的 int* 指针 int (*A3)[3][5] ：表示一个指向 3 * 5 数组的指针 int *(A4[3][5]) ：表示 3 * 5 的 int* 指针 int (*A5[3])[5] ：表示指针数组，指针指向大小为 5 的数组 结构体：不同类型数据的组合，由编译器决定域的大小和位置，注意每个元素的对齐和整个结构体的对齐 浮点数：浮点数参数被存储在 %xmm0, %xmm1, …%xmm7，返回值存储在 %xmm0 联合体：按照最大域分配，不同的域共用内存 缓冲区溢出： 原因：主要源于对有界字符数组不加长度限制的写操作造成的 类型：stack smashing attack，code injection attack 措施：使用更加安全的库函数，随机化堆栈偏移量，不可执行代码段保护，Stack Canaries 第五章 代码优化编译器在确保安全性的同时，尽最大可能进行优化，当面对不确定情况时，采用保守策略 通用优化方案： 代码移动：在循环内不变的运算移动到外部，减少运算次数 Reduction in Strength：用简单高效计算代替复杂且计算量大的计算，如移位代替乘法 公共(部分)子表达式重用：减少运算次数 优化障碍： 过程调用：如将 strlen 放在循环条件中，将会重复计算，此时编译器不会优化，因为过程调用可能存在副作用，并且不是每次都返回相同的值 别名问题：如矩阵相乘中内循环中每次都会涉及 C[i, j] 的存储和读取，此时编译器也不能优化，因为可能存在变量别名的问题，产生副作用 指令级并行： CPE：操作向量元素性能的表达式，和实际时钟数 $T &#x3D; CPE * n + overhead$ 流水线技术：不包括启动时间，加法流水线将会在每个周期产生一个结果 寄存器重命名技术：在硬件资源充足的时候，可以找出对应的关键路径 循环展开：在单个循环中执行多次循环中的语句，发掘并行基础 数据级并行：使用 SIMD 指令 分支预测： taken：跳转到分支目标地址 not taken：顺序执行下一条指令 第六章 存储器层次结构主存操作： 读操作：通过总线发送读取地址，传送数据到总线上，CPU 读取数据 写操作：通过总线发送写入地址，主存接受数据，写入数据到主存中 RAM： 类型：SRAM 和 DRAM，前者速度更快，用于 Cache 中，后者更加便宜，用于主存中 DRAM 读取：RAS 选择一行数据转存到 row buffer 中，CAS 选择一列数据送到总线上，row buffer 回写用于刷新 局部性原理：用于平衡主存和 CPU 之间的延迟，分为时间局部性，空间局部性 存储层次结构：金字塔型结构，顶部速度快，但是价格贵，底部速度慢，但是便宜 缓存： 组织结构：（S，E，B），直接映射 E &#x3D; 1，多路组相联 E &gt; 1，[tag index, set index, offset] 读块：通过 set index 找到对应的 set，通过 tag 对比和 valid，如果找到，进行偏移读取，否则失效 失效类型：Cold miss，Conflict miss，Capacity miss 替换策略：随机，LRU，LFU 写块：Write-through + No-write-allocate 或者 Write-back + Write-allocate 写命中时：write-through，write-back 写失效时：write-allocate，no-write-allocate 平均访问时间：hit rate * hit time + miss rate * miss time 矩阵乘法分析： 循环置换：A 点乘 B 行向量，得到 C 行向量，此时 miss 次数最少 分块：每个小块的 miss 次数变小，每个小矩阵的 miss 次数是 $B^2&#x2F;8$ 其他存储介质： NVM：非易失内存，掉电仍然保存数据，写次数较为有限 磁盘：访问延迟（寻道 + 旋转 + 传输），逻辑块到磁盘扇区的转换 SSD：相较于磁盘，没有机械移动装置，更快，但是写次数存在上限 第七章 链接动机：应用程序的模块化和高效化构建 链接器功能：符号解析和重定位 符号解析： 类别：强符号指代函数名和初始化过的全局变量，弱符号指代未初始化的全局变量 规则： 链接器不进行类型检查 不允许多个同名的强符号定义 如果有多个弱符号和一个强符号定义，则选择强符号定义 如果仅有多个同名的弱符号，则从中任选一个弱符号定义 避免类型错误匹配 尽可能避免使用全局变量 在头文件中声明所有非静态变量或函数 将 extern 全局符号声明放在头文件的声明中 重定位：将符号的相对引用方式定位到可执行文件中的绝对内存位置 库文件打包： 静态链接库：.a 文件，需要注意 gcc 命令的顺序，可以将所有库文件放在末尾 动态链接库：.so 文件，既可以加载时链接，也可以在运行时链接，库文件可被多个进程共享 PIC：库文件可以在任何地址加载和执行，而不需要被链接器重定位 库插桩技术：允许程序员拦截对任意函数的调用，可以增强安全性，增强代码调试能力，可以在编译时，链接时和运行时进行 第八章 异常控制流异常控制流： 底层机制：异常 高层机制：进程上下文切换，Signals，Nonlocal jumps 异常：为了响应某些事件而将控制权转移到操作系统，通过中断向量获取到处理代码的起始地址 Interrupts：由处理器外部事件引起的，如 timer，IO 中断 Traps： system calls，breakpoint traps Faults：page faults，protection faults，可能会重新执行产生错误的指令 Aborts：illegal instruction，parity error，machine check 进程：正在运行的程序 抽象：CPU 独占（context switching），内存独占（虚拟内存） 并发执行指的是多个进程的控制流有重叠，否则便是顺序执行 进程控制： 获取进程 ID：getpid，getppid 终止进程：exit，exit 被调用一次，不会返回 创建进程：fork，fork 被调用一次，但是返回两次，写操作采用 COW，共享打开的文件 回收子进程：wait，waitpid，如果父进程没有回收子进程，子进程最终会被 init 进程回收 加载并运行进程：execve，调用一次，不会返回（没有错误） Signals：内核通过更新目标进程上下文中的一些状态向目标进程(传递)一个 Signal，目标进程根据 Signal 类型采取某些反应，如 Ignore，Terminate，Catch，内核通过设置 signal mask 中相应的进行传递，可以通过 signal 函数定义 signal handler，handler 可以被其他的 handler 打断 Nonlocal jumps：强大(但危险)的用户级机制，用于将控制转移到任意位置，通过 setjmp&#x2F;longjmp 实现，setjmp 被调用一次，可能返回多次 第九章 虚拟内存使用虚拟内存的动机： 用作缓存：此时 VM 在磁盘上，而磁盘上的数据在 DRAM 中缓存，page table 用于快速查找数据是否在 DRAM 中，未命中则 page fault，会将 VM page 缓存到 DRAM 中，并重新执行相应指令 内存管理：每个进程有自己的虚拟地址空间，每个 VA 会被映射到 PA，一些物理页可以在进程间共享 内存保护：通过在 PTE 中扩展 permission 位实现权限控制 地址转换： 虚拟地址：[VPN, VPO] &#x3D; [TLBT, TLBI, VPO], VPN &#x3D; TLBT + TLBI 物理地址：[PPN, PPO] &#x3D; [CT, CI, CO], PPO &#x3D; CI + CO TLB：用于快速虚拟地址转换为物理地址，如果命中，则不用访存，否则访问响应的 PTE 多级 Page Table：节省内存 Memory Mapping：读取大文件，共享数据结构，基于文件的数据结构（如数据库） 动态内存分配：用于管理堆空间 假设：每个 word 8 字节，每个 payload 指针是 2 word 对齐的 碎片化：外部碎片和内部碎片 跟踪空闲块方案： 使用 block 大小隐式链接所有块：找空闲块的策略有 First Fit，Next Fit，Best Fit 和 Perfect Fit；相关操作有对空闲块 split，对相邻空闲块 merge；通过 header 字段实现遍历，通过 footer 实现反向遍历，但需要更多的内存开销；也可使用 header 中的 tag 字段来存储前一块是否被分配 使用专门的指针链接空闲块，但对于已经分配了的块和隐式方式相同 根据空闲块的大小分类并构造多个空闲块链表：通常按照 2 的次方划分 使用红黑树管理内存 垃圾回收：隐式内存管理，自动回收堆内存，常见的方案： Mark and Sweep Reference counting Copying collection 第十章 系统IOUnix 文件类型：普通文件，目录文件，块文件，PIPE，Socket Unix IO： 文件操作：open，close，总是返回最小的尚未分配的 fd，两次 open 相同的文件会有各自的文件表项 文件位置操作：lseek 读写文件：read，write，返回值是实际读写的字节的个数，小于 0 则代表出错 RIO：在网络程序等应用中提供高效和健壮的 IO 非缓冲 IO：rio_readn，rio_writen 缓冲 IO：rio_readlineb，rio_readnb，一次读很大空间的内容，存储在缓冲区内，为后续读节省时间 Unix 文件共享： Descriptor table，Open file table，node table fork：子进程和父进程指向相同的文件表项，共享相同的 position 重定向：dup2，共享相同的文件表项 标准 IO： 文件操作：fopen，fclose，fread，fwrite，fgets，fputs，fscanf，fprintf 流模型","categories":[],"tags":[{"name":"计算机系统","slug":"计算机系统","permalink":"http://blog.zsstrike.tech/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"},{"name":"Computer Systems","slug":"Computer-Systems","permalink":"http://blog.zsstrike.tech/tags/Computer-Systems/"}]},{"title":"《高级数据库系统》笔记","slug":"《高级数据库系统》笔记","date":"2022-09-25T05:39:45.000Z","updated":"2022-10-08T11:08:19.475Z","comments":true,"path":"2022/09/25/《高级数据库系统》笔记/","link":"","permalink":"http://blog.zsstrike.tech/2022/09/25/%E3%80%8A%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%AC%94%E8%AE%B0/","excerpt":"本文用于整理《高级数据库系统中》的知识点，以备查阅。","text":"本文用于整理《高级数据库系统中》的知识点，以备查阅。 第一章 数据库系统概述基本概念：数据，数据库，数据库模式，数据库管理系统，数据库系统。 Megatron 2000 数据库的问题： 元组平铺在磁盘上，删除更新操作代价大 低级的查询处理 无缓冲区管理，IO 代价大 无并发控制，不能保证数据库的一致性 无索引，查询效率低 无故障恢复 数据库模式设计不规范的话会带来很多问题，如数据冗余，更新异常，插入异常，删除异常，可以进行模式分解来解决该问题。 数据库语言类型：DDL，DML，DCL 第二章 关系数据库回顾ANSI&#x2F;SPARC 体系结构：三级模式结构和两级映像，确保了数据的独立性。 关系模型中的概念：超码，候选码，主码，替换码，外码 关系模型的三类完整性： 实体完整性 参照完整性，可为空 用户自定义完整性 关系代数：并，交，差，笛卡尔积，选择，投影，联接（自然联接和等值连接区别），除 第三章 存储介质存储器访问延迟： 磁盘结构：盘片，盘面，磁头，磁道，扇区，柱面，扇区之间存在间隙 磁盘块的存取时间：块是 DBMS 中的数据存取的最小单元，扇区是磁盘中数据存储的最小单元 读块延迟：寻道时间 S（平均寻道数） + 旋转延迟 R + 传输时间 T，传输时间是标称传输速率和实际旋转传输时间的较小值 写块延迟：同读块延迟，如果需要校验，则需要再次加上旋转时间和传输时间 块修改：读块 + 写块 块地址包含如下信息：物理设备号，柱面号，盘面号，扇区号 磁盘存取优化： 磁盘调度算法，如电梯调度 Random IO to Sequential IO 预取缓冲：单缓冲区，双缓冲区 第四章 数据表示数据项的表示： 记录的表示： 固定格式变长记录：定长字段在前，变长字段在后 可变格式变长记录： 记录在块中的组织： 定长记录：使用 &lt;块号,槽号&gt; 表示记录地址 变长记录：通过槽号获取到记录偏移量和对应的长度 记录的地址： 逻辑地址：文件号 + 逻辑块地址 + 块内偏移 物理地址：物理块地址 + 块内偏移 记录在块内的操作： 插入：若记录无序，插入到任意块的空闲空间中；若记录有序，找到记录应该放置的块，如果没有空间，可以找邻近块中的空闲空间，或者使用溢出块 删除：立即回收空间，使用删除标记 块在文件中的组织：链表式堆文件组织和目录式堆文件组织 第五章 缓冲区管理缓冲区结构： frame：一些连续的 page，通常等于 page 的大小，包含有 Dirty，PinCount 和 Latch 等信息 page：指代在内存中的数据块，操作系统操作文件的单元 Block, on the other hand, is a group of sectors that the operating system can address; Page is similar to a block, but is the RAM equivalent. Blocks are usually the smallest unit of “cold” storage, while Pages are usually the smallest unit of in-memory storage. In some storage systems, chuck could be an abstraction layer above a block. 请求块操作和释放块操作：替换策略，pin 和 unpin，dirty 替换策略： Belady’s （OPT）：理论上最佳的页面置换算法 LRU：替换最近最少被访问的 frame LRU-k：考虑 frame 的访问频率，如果某个 frame 的访问次数达到了 K 次以上，则应当尽量不置换 2Q：与 LRU-2 类似，不同之处在于访问 1 次的队列采用 FIFO，而不是 LRU Second-Chance LRU：连续两次置换才能将 frame 替换出去 CLOCK：把 Second-Chance FIFO 组织成环形 CF-LRU：SSD 上置换算法，减少缓存对闪存的写次数 缓冲管理器：接受上层的 Frame 请求，对 buffer 进行操作和维护，同时向存储管理器发送 Page 相关请求 第六章 索引结构顺序文件上的索引： 密集索引：空间占用大 稀疏索引：点查询性能不如密集索引 多级索引：稀疏二级索引 + 一级索引，一级索引过大时可以再次索引，将二级索引放入内存 主索引：记录在文件中按照主键存储，根据主键建立的主索引 密集辅助索引：非主键上建立的索引，重复键值可以使用间接桶 倒排索引：文档检索 B+ 树：通常 3 层，节点格式 n 个值，n + 1 个指针 叶节点：至少 (n + 1) / 2 个指针指向记录，过半记录 中间节点：至少 (n + 2) / 2 个指针指向子树，过半指针 根节点：至少两个指针 相关操作：查找，插入（递归分裂），删除（合并，删除叶节点最小值） 散列表： 散列表：查找，插入，删除，注意溢出块的处理 可扩展散列表：前 i 位用于区分桶，索引成倍增加，但对于某个桶来说，只有必要的时候才会分裂 线性散列表：后 i 位用于区分桶，线性增加，只会放在 m 桶或者 $m - 2^{i-1}$ （去掉最高位）桶 填充率：实际记录总数除以桶数（不包括溢出块数量） 多维索引： R-Tree：使用矩形覆盖，建立索引 网格文件：两个维度上建立索引 分段哈希函数 第七章 查询优化查询语句的执行流程：语法分析，逻辑计划生成，查询重写，物理计划生成，物理计划评价，执行物理计划 语法分析：构造语法分析树 逻辑计划生成：将语法分析数转换为代数表达式（逻辑计划） 查询重写：将初始逻辑计划按照转换规则转换，如选择连接顺序，优先选择，先投影后选择等，减少 IO 查询计划代价估计：T(R)，S(R)，V(R, A)，B(R) $W &#x3D; R1 × R2$ $W &#x3D; \\sigma_{A\\ op\\ a}(R)$ $ W &#x3D; R1\\bowtie R2 $ 物理操作符间参数传递：物化方式，流水线 第八章 连接算法物理操作符：逻辑操作符的特定实现，如 TableScan，SortScan，IndexScan 等 连接操作的实现算法：嵌套循环连接，归并连接，索引连接，散列连接 连接算法代价分析：假设记录连续存储，未排序，B(R2) &lt; B(R1)，R1 索引在内存中，buffer 大小 M Algorithm Cost M Nested Loop Join (B(R2) &#x2F; M-1) * (M-1 + B(R1)) $\\ge 2$ Merge Join 5 (B(R1) + B(R2)) $\\sqrt{B(R1)}$ Merge Join(Improved) 3 (B(R1) + B(R2)) $\\sqrt{B(R1) + B(R2)}$ Index Join B(R2) + T(R2) × T(R1)&#x2F;V(R1, C) Hash Join 3 (B(R1) + B(R2)) $\\sqrt{B(R2)} + 1$ 连接顺序的选择： 左右变元：较小的关系作为左变元（驱动表）（除了 index join） 连接树：给定 n 个关系的连接树数目为 $T(n) * n!$ ，通常选择更加高效的左深树，T(n) 卡特兰数 动态规划法选择连接顺序：代价函数 $f(n)&#x3D;f(n-1) + T(n - 1)， n \\ge 3$ 第九章 日志和恢复事务性质：ACID，通过事务可以确保数据库的一致性 事务原语操作：Input(X)，Output(X)，Read(X, t)，Write(X, t) 数据库故障：事务故障，介质故障，系统故障 数据库恢复策略：备份 + 日志（冗余） Undo日志：立即更新，内存代价小 在 x 被写到磁盘之前，对应该修改的日志记录必须已被写到磁盘上 当事务的所有修改结果都已写入磁盘后，才将 &lt;Commit,T&gt; 日志记录写到磁盘上 从日志尾部开始恢复那些未提交（没有执行 Commit，Abort）的事务 Redo日志：延迟更新，恢复代价小 在 x 被写到磁盘之前，对应该修改的日志记录必须已被写到磁盘上 在数据写回磁盘前先写 &lt;Commit,T&gt; 日志记录 从日志首部开始恢复那些已经提交（执行了 Commit）的事务 Undo&#x2F;Redo 日志： 在 x 被写到磁盘之前，对应该修改的日志记录必须已被写到磁盘上 在数据写回磁盘前先写 &lt;Commit,T&gt; 日志记录 恢复时先 Undo ，再 Redo 检查点：在检查点时刻，所有的事务都已经执行完毕，所有的数据（日志，数据库数据）都已经被持久化完毕，且在检查点创建时，不允许处理新的事务（非静止检查点除外） 日志轮转技术：防止日志文件会占用大量的磁盘空间 第十章 并发控制可串化调度（Serializable Schedule）：如果一个调度的结果与某一串行调度（Serial schedule）执行的结果等价，则称该调度是可串化调度，否则是不可串调度 冲突可串性：如果一个调度满足冲突可串性，则该调度是可串化调度，通常使用优先图来判断冲突可串性 视图可串性：弱于冲突可串性，但仍可保证调度的可串性，只需要确保读到的值相同，最后数据库状态一致即可，可以使用多重图判断，即对 Wi(A) =&gt; Rj(A)，若存在 Wk(A)，那么 Tk 要么在 Ti 前，要么在 Tj 后 2PL： 对任何数据读写前，需要获得该数据上的锁，在释放一个锁之后，事务不再获得任何锁 相关锁：X Lock，S Lock，U Lock，注意&lt;S, U&gt;是相容的，&lt;U, S&gt;是不相容的 多粒度锁：在给节点 P 加锁的时候需要判断 P 的上层节点，下层节点和本节点，代价大 意向锁：对任一结点 P 加 S(X) 锁，必须先对从根结点到 P 的路径上的所有结点加 IS(IX) 锁 死锁： 死锁检测：超时，等待图 死锁预防：按照锁对象的某种顺序加锁，使用时间戳（Wait-Die 和 Wound-Wait） OCC：通常分为读阶段，有效性确认阶段，写阶段 第十一章 NoSQL 数据库数据库变迁：SQL，NoSQL，NewSQL NoSQL 特点：非关系型的，高扩展性，没有预先定义的模式，CAP NoSQL 兴起的原因： RDBMS 无法满足 Web 2.0 的需求（海量数据，高并发） One size fits all 模式很难适用于截然不同的业务场景（OLTP，OLAP） Web 2.0 网站系统通常不要求严格的数据库事务，不要求严格的读写一致性 混合架构： 亚马逊对于购物篮这种数据，使用 KV 存储 对于订单采用关系数据库 对于历史记录，采用文档数据库 NoSQL 主要类型：键值数据库、列存储数据库、文档数据库和图数据库 NoSQL 分布式系统基础：CAP 和 BASE LSM-Tree： 将随机写操作转化为顺序写，支持高吞吐的写，适合写多读少的应用 需要 Compaction，会带来写放大的问题和系统性能抖动 Compaction 策略：size-tiered 策略，leveled 策略","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://blog.zsstrike.tech/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Database","slug":"Database","permalink":"http://blog.zsstrike.tech/tags/Database/"}]},{"title":"《算法》备忘录","slug":"《算法》备忘录","date":"2022-05-12T14:37:18.000Z","updated":"2022-10-08T11:08:26.464Z","comments":true,"path":"2022/05/12/《算法》备忘录/","link":"","permalink":"http://blog.zsstrike.tech/2022/05/12/%E3%80%8A%E7%AE%97%E6%B3%95%E3%80%8B%E5%A4%87%E5%BF%98%E5%BD%95/","excerpt":"本文用于在算法学习过程中，总结相关算法套路以及算法编写经验，以备查阅。","text":"本文用于在算法学习过程中，总结相关算法套路以及算法编写经验，以备查阅。 01 算法复杂度及其分析算法复杂度表示方法： $O$：表示算法的渐进上界 $\\Omega$：表示算法的渐进下界 $\\Theta$：表示算法的渐进紧确界 递归算法复杂度分析： 递归树方法 代入检测法 主方法：令 $a \\ge 1, b &gt; 1$ 是常数，$f(n)$​ 是一个函数，T(n) 是非负整数上的递推式： $$ T(n) &#x3D; aT(\\frac{n}{b}) + f(n) $$ T(n) 有以下方法紧确界： 如果存在 $\\epsilon &gt; 0$，使得 $f(n) &#x3D; O(n^{log_ba - \\epsilon})$，则 $T(n) &#x3D; \\Theta(n^{log_ba})$ 如果 $f(n) &#x3D; O(n^{log_ba})$，则 $T(n) &#x3D; \\Theta(n^{log_ba}lgn)$ 如果存在 $\\epsilon &gt; 0$，使得 $f(n) &#x3D; O(n^{log_ba + \\epsilon})$，并且存在 $c &lt; 1$ 和足够大的 n，满足 $af(n&#x2F;b) \\le cf(n)$，则 $T(n) &#x3D; \\Theta(f(n))$ 02 排序算法常见排序算法： 名称 平均复杂度 最坏复杂度 原地算法 是否稳定 冒泡排序 $\\Theta(n^2)$ $\\Theta(n^2)$ 原地 稳定 插入排序 $\\Theta(n^2)$ $\\Theta(n^2)$ 原地 稳定 选择排序 $\\Theta(n^2)$ $\\Theta(n^2)$ 原地 不稳定 归并排序 $\\Theta(nlgn)$ $\\Theta(nlgn)$ O(n) 稳定 快速排序 $\\Theta(nlgn)$ $\\Theta(n^2)$ 原地 不稳定 堆排序 $\\Theta(nlgn)$ $\\Theta(nlgn)$ 原地 不稳定 希尔排序 $\\Theta(nlgn)$ $O(n^s), 1&lt;s&lt;2$ 原地 不稳定 计数排序 $\\Theta(k+n)$ 非原地 稳定 桶排序 $\\Theta(k+n)$ 非原地 稳定 基数排序 $\\Theta(d(k+n))$ 非原地 稳定 03 数据结构基本数据结构：数组，链表 扩展数据结构：队列，栈，树（二叉树，多叉树），散列表，二叉搜索树，红黑树 散列表： 普通数组概念的推广，使用哈希函数进行位置索引，快速查找 冲突处理：链接法，开放寻址法 二叉搜索树： 定义：左子树中的值小于父节点值，右子树中的值大于父节点值 遍历方式：前序遍历，中序遍历，后序遍历 操作：search，successor，predecessor，insert，delete 红黑树： 一种近似平衡的二叉树，可以看作是 2-3 树的实现 性质： 每个节点颜色或者红色，或者黑色 父节点黑色 叶子节点（Nil 节点）黑色 红色节点的两个子节点必须是黑色的 每个节点到其为根的树的叶子节点的路径中的黑色节点数目相同 新插入的节点颜色为红色 平衡方法：左旋和右旋 操作： 插入：如果插入节点和其父节点都是红色节点，则 case 1：叔节点是红色，则将祖父节点，父节点，叔节点反色，进入 case 2 或者 3 case 2：叔节点是黑色，且自己是右孩子，将父节点左旋，转到 case 3 case 3：叔节点是黑色，且自己是左孩子，将祖父节点右旋，祖父节点和叔节点颜色互换 删除： case 1：兄弟节点红色，父节点左旋，父节点和兄弟节点变色 case 2：兄弟节点黑色，并且其左右节点都是黑色，则将兄弟节点变为红色 case 3：兄弟节点黑色，其左节点红色，右节点黑色，则兄弟节点右旋，兄弟节点和其左节点变色 case 4：兄弟节点黑色，右节点红色，父节点左旋，兄弟节点右节点变色 04 编程方法分治：将问题划分为互不相交的小问题，递归地解决小问题，再进行组合来解决原问题 动态规划：和分治类似，不过其应用于具有重叠子问题的情况，关键在于找到递归解定义，然后使用备忘录或者自底向上方法进行解决 贪心算法：和动态规划类似，不过其在解决问题的过程中，每一步都选择最优解，即小问题的最优解构成了大问题的最优解","categories":[],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://blog.zsstrike.tech/tags/Algorithm/"}]},{"title":"Deepin 使用手册","slug":"Deepin-使用手册","date":"2022-04-03T08:41:22.000Z","updated":"2022-05-16T07:41:45.992Z","comments":true,"path":"2022/04/03/Deepin-使用手册/","link":"","permalink":"http://blog.zsstrike.tech/2022/04/03/Deepin-%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/","excerpt":"本文用于记录在使用 Deepin 20.x 过程中遇到的问题以及解决方案。","text":"本文用于记录在使用 Deepin 20.x 过程中遇到的问题以及解决方案。 Deepin 装机初始化脚本每次装机都需要手动安装一系列的安装，很麻烦，可以使用脚本安装必备软件： 12345678910111213#!/bin/bashsudo apt install spark-storesudo apt install typorasudo apt install google-chrome-stablesudo apt install codesudo apt install netease-cloud-musicsudo apt install timeshiftsudo apt install qv2raysudo apt install goldendictsudo apt install vlcsudo apt install okularsudo apt install wps-office 阿里云盘挂载通过 aliyundrive-webdav 下载插件为阿里云盘提供 webdav 服务，获取 refresh_token 后在命令行执行命令即可开启 webdav 服务器。 安装 rclone ，该插件可以为 webdav 服务提供中间层，在其上创建一个 vfs，同时能够提供缓存等功能，这样的话，使用 Deepin 文管能够流畅访问云数据。 在 ～/.config/autostart 中添加对应的 desktop 文件，用于自启动 webdav 服务器，同时通过 rclone mount 用来挂载对应的云盘数据。 启动脚本如下： 123456789101112131415161718192021222324#!/bin/bashnohup /home/strike/.local/bin/aliyundrive-webdav \\-r &lt;refreshToken&gt; \\--port 7963 \\-U admin \\-W admin \\&gt; /home/strike/.config/autostart/aliyun.log 2&gt;&amp;1 &amp;sleep 3# not support poll-intervalnohup /usr/bin/rclone mount aliyun:/ /mnt/aliyun \\--cache-dir /media/strike/HHD/AliyunCache \\--dir-cache-time 100h \\--vfs-cache-mode full \\--vfs-cache-max-age 500h \\--vfs-read-chunk-size 10M \\--vfs-read-ahead 10M \\--buffer-size 10M \\--vfs-read-chunk-size-limit 100M \\--log-file /home/strike/.config/autostart/aliyun.log \\--log-level INFO &amp;","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.zsstrike.tech/tags/Linux/"}]},{"title":"《Java》备忘录","slug":"《Java》备忘录","date":"2022-03-21T11:19:56.000Z","updated":"2022-05-16T07:41:46.126Z","comments":true,"path":"2022/03/21/《Java》备忘录/","link":"","permalink":"http://blog.zsstrike.tech/2022/03/21/%E3%80%8AJava%E3%80%8B%E5%A4%87%E5%BF%98%E5%BD%95/","excerpt":"本文用于记录 Java 相关知识，以备查阅。","text":"本文用于记录 Java 相关知识，以备查阅。 01 面向对象面向对象的三大特征： 封装：通过访问修饰符实现，优点是减少耦合，提高软件的可重用性 继承：通过 extends 实现，表示 is-a 的关系 多态：编译时多态指的是方法的重载，运行时多态指的是对象引用所指向的具体类型在运行期间才确定，运行时多态通过继承，重写，向上转型实现 02 基础知识数据类型：8 种基本类型，对应 8 种包装类型，通过 xxxValue 和 valueOf 自动拆箱和装箱 缓存池： 基本类型：除了 long，float 和 double，其它类型都有一字节的缓存池 String：字面量创建会先从常量池获取，没有的话再创建，new 创建不会加入常量池 String：被声明为 final，内部存储的 value 数组也被声明为 final，同时内部没有改变 value 指向的方法，因此保证了 String 类不可变，不可变的好处： 可以缓存 hash 值 String Pool 的需要，如果可变将没有意义 安全性和线程安全 String，StringBuilder，StringBuffer： String 不可变，StringBuilder 和 StringBuffer 可变 String 和 StringBuffer 是线程安全的，StringBuilder 不是线程安全的 String.intern：可以保证相同内容的字符串变量引用同一的内存对象，其首先将字符串对象放到字符串常量池中，然后返回这个对象引用 参数传递：Java 中的参数是以值传递的形式传入方法中的，而不是引用传递 float 和 double：浮点数字面量默认是 double 类型，需要加后缀 f 将其表示为 float 类型 隐式类型转换：整型数字面量是 int 类型，使用复合运算符如 += ，会自动类型转换 switch：从 Java 7 开始，支持 String 对象（对应的 hashcode ），但是其仍旧不支持 long 访问权限： 类可见：其它类可以用这个类创建实例对象 成员可见：其它类可以用这个类的实例对象访问到该成员 访问权限 本类 本包的类 子类 非子类的外包类 public 是 是 是 是 protected 是 是 是 否 default 是 是 否 否 private 是 否 否 否 抽象类和接口： 抽象类：一般会包含抽象方法，抽象方法一定位于抽象类中，不能被实例化 接口：是抽象类的延伸，在 Java 8 开始，引入了默认方法 比较： 抽象类提供了 is-a 的关系，而接口提供了 like-a 关系 一个类只能继承一个抽象类，但是可以实现多个接口 接口的字段只能是 static 和 final ，成员（字段和方法）只能是 public super： 访问父类的构造函数，需要放到子类构造函数第一行 访问父类的成员 重写和重载： 重写：存在于继承体系中，为了满足里式替换原则，需要满足 参数列表和父类相同 子类方法的访问权限必须大于等于父类方法 子类方法的返回类型必须是父类方法返回类型或为其子类型 重载：存在于同一个类中，指方法名相同，但是参数类型，个数，顺序至少有一个不同，应该注意的是，返回值不同，其它都相同不算是重载 Object 通用方法：getClass，hashCode，equals，clone，toString，notify，notifyAll，wait，finalize equals 方法实现： 检查是否为同一个对象的引用，如果是直接返回 true 检查是否是同一个类型，如果不是，直接返回 false 将 Object 对象进行转型 判断每个关键域是否相等 hashCode：用于返回对象的哈希值，通常将每个字段看作是 R 进制中的某一位，R 一般取 31 clone：是 Object 类的 protected 方法，并不是 public，如果想要重写该方法，还需要实现 Cloneable 空接口。分为浅拷贝和深拷贝两种方式，可以使用更加安全的拷贝构造函数来拷贝一个对象 final： 修饰数据时，表示数据是常量，对于引用类型，只是使其引用不变 修饰方法时，表示方法不能被重写，private 方法隐式地指定为 final 修饰类时，表示类不可被继承 static： 静态变量：类变量，所有实例共享静态变量 静态方法：在类加载的时候就存在了，不能是抽象方法，不能包含 this 和 super 静态语句块：类初始化时运行一次 静态内部类：非静态内部类依赖于外部类的实例，而静态内部类不需要 静态导包：不用指定类名就可以使用方法，但是可读性降低 初始化顺序：静态变量和静态语句块优先于实例变量和普通语句块，静态变量和静态语句块的初始化顺序取决于它们在代码中的顺序。在存在继承的情况下： 父类(静态变量、静态语句块) 子类(静态变量、静态语句块) 父类(实例变量、普通语句块) 父类(构造函数) 子类(实例变量、普通语句块) 子类(构造函数) Java 7 版本新特性： Strings in Switch Statement Type Inference for Generic Instance Creation Multiple Exception Handling Support for Dynamic Languages Try with Resources Java nio Package Binary Literals, Underscore in literals Diamond Syntax Java 8 版本新特性： Lambda Expressions Pipelines and Streams Date and Time API Default Methods Type Annotations Nashhorn JavaScript Engine Concurrent Accumulators Parallel operations PermGen Error Removed Java 和 C++ 区别： Java 是纯粹的面向对象语言，C++ 既支持面向对象也支持面向过程 Java 通过虚拟机从而实现跨平台特性，但是 C++ 依赖于特定的平台 Java 没有指针，它的引用可以理解为安全指针，而 C++ 具有和 C 一样的指针 Java 支持自动垃圾回收，而 C++ 需要手动回收 Java 不支持多重继承，只能通过实现多个接口来达到相同目的，而 C++ 支持多重继承 Java 不支持操作符重载，而 C++ 可以 Java 的 goto 是保留字，但是不可用，C++ 可以使用 goto Java 不支持条件编译，C++ 通过 #ifdef #ifndef 等预处理命令从而实现条件编译 03 泛型机制引入泛型的意义： 适用于多种数据类型执行相同的代码 使用泛型可以提供编译前的检查，不需要强制类型转换，更加安全 泛型的使用： 泛型类 泛型接口 泛型方法：相比泛型类，其更加灵活，在使用不同的参数的时候，不需要再次实例化一个对象 泛型的上下限：为类型参数增加限制 上限：&lt;T extends Number&gt;，注意，extends 后面可以是接口，表示实现了接口的类型 下限：&lt;T super String&gt;，表示 T 是 String 或者 String 的父类（Object） 无限制通配符：&lt;T&gt; 或者 &lt;?&gt; 多个限制：&lt;T extends A &amp; B &gt; 或者 &lt;T extends A , B &gt;，通常一个类和多个接口 类型擦除：Java 中实现泛型的方式是在编译阶段进行类型擦除，即 将所有泛型表示都替换为具体的类型 为了保证类型安全，必要时插入强制类型转换 自动产生桥接方法保证擦除后的代码具有泛型的多态性 证明类型擦除： 原始类型相同：如ArrayList&lt;String&gt; 和 ArrayList&lt;Integer&gt; 的 getClass 返回值相同 通过反射可以添加其他类型的元素： strList.getClass().getMethod(&quot;add&quot;, Object.class).invoke(list, &quot;asd&quot;) 原始类型：指擦除了泛型信息，最后在字节码中的类型变量的真正类型 泛型的编译期检查：Java 编译期会先检查代码中泛型的类型，然后再进行类型擦除，之后进行编译 类型检查就是针对引用的，谁是一个引用，用这个引用调用泛型方法，就会对这个引用调用的方法进行类型检测，而无关它真正引用的对象 参数化类型不考虑继承关系，下面的引用传递不被允许： 1234// 编译错误，ClassCastException ArrayList&lt;String&gt; list1 = new ArrayList&lt;Object&gt;(); //编译错误，违背使用泛型的初衷ArrayList&lt;Object&gt; list2 = new ArrayList&lt;String&gt;(); 泛型的多态实现：类型擦除会造成多态的冲突，JVM 采用桥接方法解决该问题 1234567891011121314151617181920class DateInter extends Pair&lt;Date&gt; &#123; @Override public void setValue(Date value) &#123; super.setValue(value); &#125; @Override public Date getValue() &#123; return super.getValue(); &#125; // 编译器添加的桥接方法 public void setValue(Object value) &#123; setValue((Date)value); &#125; // 编译期添加的桥接方法 public Object getValue() &#123; return getValue(); &#125;&#125; 另外，子类中桥接方法Object getValue()和Date getValue()是同时存在的，虚拟机可以通过返回值和参数类型来区别，但是在编写程序的时候，Java 编译期不允许我们这样做。 基本类型不能作为泛型类型：无限制泛型擦除后将变为 Obejct，而 Obejct 不能存储 int 等基本类型 泛型类型不能实例化：本质上由于类型擦除造成的，如果确实需要实例化泛型，可以使用反射 泛型数组：采用通配符的方式初始化泛型数组（存在警告），因为对于通配符的方式最后取出数据是要做显式类型转换的，符合预期逻辑。更加优雅的方式是使用反射：Array.newInstance 泛型类中的静态方法和静态变量：不可以使用泛型类所声明的泛型类型参数，因为静态变量和静态方法不需要使用对象来调用，从而类型参数不确定，但是可以使用泛型静态方法 获取泛型的参数类型：通过反射 java.lang.reflect.Type 获取 04 注解机制注解分类： Java 自带的标准注解：@Override，@Deprecated，@SupressWarning 元注解：用于定义注解的注解 自定义注解：根据自己需求定义注解 自带的标准注解： @Override：表示当前的方法定义将覆盖父类中的方法 @Deprecated：表示代码被弃用，如果使用了被 @Deprecated 注解的代码则编译器将发出警告 @SuppressWarnings：表示关闭编译器警告信息 元注解：描述注解的注解 @Target：描述注解的使用范围，取值范围在 ElementType 枚举类中 @Retention：描述注解保留的时间范围，取值范围在 RetentionPolicy 中，共三种 @Documented：描述在使用 javadoc 工具为类生成帮助文档时是否要保留其注解信息 @Inherited：被它修饰的注解将具有继承性，被该注解修饰的父类，其子类自动具有该注解 @Repeatable：允许在同一申明类型(类，属性，或方法)的多次使用同一个注解 @Native：修饰成员变量，则表示这个变量可以被本地代码引用，常常被代码生成工具使用 注解和反射：通过反射下的 AnnotatedElement 接口可以获取注解，要求该注解范围是 RUNTIME 的 注解不支持继承：尽管在内部实现中，注解被翻译成 interface，但是并不能使用 extends 来继承某个 @interface，另外，在注解编译后，编译器会自动将其继承到 Annotation 接口 注解使用场景： 配置化到注解化：框架的演进 继承实现到注解实现：junit3 到 Junit4 自定义注解和 AOP：通过切面实现解耦 05 异常机制异常结构层次： 可查异常：编译器要求必须处理的异常，包括非运行时异常 不可查异常：编译器不强制要求处理的异常，包括运行时异常和错误 异常关键字：try，catch，fainally，throw，throws 异常的声明（throws）：在方法末尾添加该语句即可，添加异常的规则 如果是不可查异常，那么可以不使用 throws 来声明，因为在运行时会被系统抛出 必须声明任何方法可抛出的的可查异常 重写方法声明的异常必须是被重写方法声明的异常或者其子类 异常的抛出（throw）：有时在 catch 中抛出一个异常，目的是为了改变异常的类型 异常自定义：通过继承异常类实现，可以实现带有详细描述信息的构造函数用于调试 异常的捕获： try-catch：同一个 catch 可以捕获多种不同的异常，使用 | 分割 try-catch-finally：不管有没有出现异常，finally 中的语句块始终会被执行，通常 finnaly 里面不要包含 return 语句 try-finally：保证资源使用后被关闭 try-with-resource：自动释放资源，需要资源实现了 AutoClaseable 接口的类 异常实践： 只针对不正常的情况才使用异常，如可以通过判断规避掉 NullPointorException 使用 finally 或者 try-with-resources 关闭资源 尽量使用标准的异常，基于语义的基础 对异常进行文档说明 优先捕获最具体的异常 不要捕获 Throwable 类，因为其会捕获 Error（如 OOM） 不要忽略异常，至少记录异常的信息 不要记录并抛出异常，可能导致多个地方输出同一个异常信息 包装异常时不要抛弃原始的异常，否则失去了原始的堆栈信息 不要使用异常控制程序的流程 不要在 finally 块中使用 return，会覆盖 try 块中的返回点 JVM 处理异常的机制：编译器通过编译后，会生成对应的异常表，异常表每项表示为（from，to，target，type），表示在 [from, to] 代码段中可能发生异常，如果发生 type 类异常，就跳转到 target 对应的代码段。在发生异常的时候，JVM 查找异常表，跳转到对应的 target 地方 异常耗时：建立一个异常对象，大概是一个普通 Object 对象的 20 倍，而抛出，接收一个异常，所花费时间大约是建立异常对象的 4 倍 06 反射机制反射机制：在运行时，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性。 Class 类：Class 类也是一个类，其实例用于表示运行时的类（class &amp; enum）或接口（interface &amp; annotation），数组和基本类型同样也被映射为 Class 对象的一个类 手动编写的类编译后会产生 Class 对象，其被保存在同名 .class 文件中 每个类在内存中只有一个对应的 Class 对象来描述其信息，采用单例模式 Class 类只存在似有构造函数，因此对应的 Class 对象只能通过 JVM 加载和创建 Class 类对象的获取： 根据类名：类名.class 根据对象：对象.getClass() 根据全限定类名：Class.forName(全限定类名) Constructor 类：表示 Class 对象所表示类的构造方法，相关方法如下： getConstructor(Class&lt;?&gt;… parameterTypes)：返回具有 public 访问权限的构造函数对象 getDeclaredConstructor(Class&lt;?&gt;… parameterTypes)：返回所有（包括 private）构造函数对象 newInstance()：调用无参构造器创建新的实例 newInstance(Object… initargs) Field 类：提供有关类或接口单个字段的信息，以及对它的动态访问权限 getField：获取指定的名称，且具有 public 修饰的字段，包括继承字段 getDeclaredField：获取指定的字段（包括 private），不包括继承的字段 set(Object obj, Object value) &amp; get(Object obj)：不可设置 final 字段 setAccessible(boolean flag)：设置其可访问性，用于访问 private 属性 Method 类：提供关于类或接口上单独某个方法的信息 getMethod(String name, Class&lt;?&gt;… parameterTypes) getDeclaredMethod(String name, Class&lt;?&gt;… parameterTypes) invoke(Object obj, Object… args) 反射调用流程： 反射类和反射方法的获取，都是通过从列表中顺序搜寻查找匹配的方法 当找到需要的方法时，都会 copy 一份出来，保证数据隔离 每个类都可以获取 method 反射方法，并作用到其他实例身上 反射也是线程安全的 反射使用 reflectionData 缓存 Class 信息，避免开销 07 SPI 机制SPI 机制：JDK 内置的服务提供发现机制，可以用来启用框架扩展和替换组件。服务提供者定义接口后，需要在 classpath 下的 META-INF/services/ 目录下创建一个服务接口命名的文件，这个文件里的内容就是这个接口具体的实现类。 SPI 机制：JDBC DriverManager，在以前开发时，需要先 Class.forName 加载数据库相关的驱动，而在 JDBC4.0 之后直接获取连接就可以了 JDBC 接口定义：java 中定义了接口 java.sql.Driver，但是没有具体实现 实现： mysql：在 mysql 的 jar 包中，可以找到 META-INF/services 目录，该目录下面有一个 java.sql.Driver 的文件，文件内容是 com.mysql.cj.jdbc.Driver postgresql：在对应的目录下面，也可以找到对应的配置文件 使用方法：直接通过 DriverManager.getConnection 来获取连接，实际执行代码的步骤 从系统变量中获取有关驱动的定义 使用 SPI 来获取驱动的实现：ServiceLoader.load 遍历使用 SPI 获取到的具体实现，实例化各个实现类 SPI 的缺点： 不能按需加载，需要遍历所有的实现并实例化（懒加载），然后在循环中才能找到我们需要的实现 获取某个实现类的方式不灵活，只能通过 Iterator 形式获取 不是并发安全的 08 Collection 类集合类：集合类用于容纳其他的 Java 对象，其只能存放对象，基本类型通常需要装包和解包 Collection：存储对象的集合 Map：存储键值对的映射表 ArrayList：实现了 List 接口，允许存放 null 元素，底层通过 Object 数组实现，以便容纳任何类型的对象，为了追求效率，并没有实现线程同步 add，addAll，get，set，remove，indexOf，lastIndexOf 自动扩容：如果添加数据时，超过 capacity 值，就会自动扩容，每次扩容变为之前容量的 1.5 倍，在实际添加大量元素前，可以通过 ensureCapacity 来提前分配 Fail-Fast：采用了快速失败机制，记录 modCount 参数来实现，在并发修改时，迭代器很快就会失败 LinkedList：底层是带有头尾节点的双向链表，同时实现了 List 接口和 Deque 接口，即可以看作是顺序容器，又可以看作是一个队列，同时可看作一个栈。不过关于栈或者队列，现在首选的是 ArrayDeque，其有着更好的性能。如果需要多个线程并发访问，可以采用 Collections.synchronizedList() 进行包装 getFirst，getLast，removeFirst，removeLast，remove，add，addAll，clear，set，get Queue 方法：offer，poll，peek，remove，element Qeque 方法：offerFirst，offerLast，peekFirst，peekLast，pollFirst，pollLast，removeFirstOccurrence，removeLastOccurrence Stack：当需要使用栈时，推荐使用更高效的 ArrayDeque Queue：支持两组格式的 api 抛出异常：add，remove，element 返回值（null）实现：offer，poll，peek Deque：同样支持两组格式的 api，无非是如 offerFirst&#x2F;offerLast 格式，ArrayDeque 实现了 Deque 接口，其底层采用循环数组实现 PriorityQueue：优先队列保证每次取出的元素都是队列中权值最小的（默认），元素大小即可以通过元素本身自然顺序，也可以通过构造时传入的比较器，不允许放入 null 元素，通过完全二叉树实现的小顶堆，意味着可以通过数组作为底层数据结构 抛出异常：add，element，remove 返回值：offer，peek，poll HashMap：实现了 Map 接口，既允许 key 为 null，也允许 value 为 null，该类未实现线程同步 插入：采用头插法进行插入，为了解决冲突，采用冲突链表方式 hashcode 决定了对象会被放到哪个 bucket 中，当多个对象的哈希值冲突时，equals 方法决定了这些对象是否是同一个对象 数组扩容：有两个参数会影响 HashMap 的性能，初始容量和负载系数，负载系数用来指定自动扩容的临界值，当 entry 的数量超过 capacity * load_factor 时，容器将自动扩容并且重新哈希，每次扩容后容量为原来的 2 倍 Java 7 采用链表和数组实现 HashMap： Java 8 采用链表，数组和红黑树实现，主要不同在于当链表中的元素超过 8 个时，会将链表转换为红黑树，使得在进行查找的时候降低时间复杂度 HashSet：对 HashMap 的一个简单封装，对 HashSet 的函数调用都会转换成合适的 HashMap 方法 LinkedHashMap：实现了 Map 接口，可以看作是 linkedlist 增强的 hashmap，其采用双向链表的形式将所有的 entry 连接起来，这样是为了保证元素的迭代顺序和插入顺序相同，另外，遍历的时候只需要从 header 开始遍历即可，遍历的时间复杂度和元素个数相同 TreeMap：实现了 SortedMap 接口，会按照 key 的大小顺序对 Map 中的元素排序，其底层采用红黑树 ceilingKey，floorKey，higherKey，lowerKey headMap，tailMap descendingKeySet，pollFirstEntry，pollLastEntry，subMap WeakHashMap：里面的 entry 可能会被 GC 自动删除，即使程序员没有调用 remove 或者 clear 方法 用于需要缓存的场景，这是在于缓存 miss 并不会造成错误 弱引用：虽然弱引用可以用来访问对象，但是在进行垃圾回收时并不会被考虑在内，仅有弱引用指向的对象依旧会被 GC 回收 并没有 WeakHashSet：可以通过 Collections.newSetFromMap 09 IO 知识体系 IO 分类： 传输方式： 字节流：读取单个字节，用来处理二进制文件 字符流：读取单个字符，用来处理文本文件 操作对象：文件，数组，管道，基本数据类型，打印，对象，缓冲，转换 字节转字符：通过 &#123;Input,Output&#125;Stream&#123;Reader,Writer&#125; 实现，char 类型使用 UTF-16be 编码 IO 设计模式：装饰者模式，FilterInputStream 属于抽象装饰者，为组件提供额外功能，如缓冲 IO 常见类的使用： 磁盘操作：File，表示文件和目录的信息 字节操作：InputStream 和 OutputStream 字符擦做：Reader 和 Writer 对象操作：Serializable，只是一个标准接口，transient 关键字可以让某些属性不被序列化 网络操作：Socket Unix 下的五种 IO 模型： 阻塞式 IO： 非阻塞式 IO： 多路复用 IO： 信号驱动 IO： 异步 IO：应用进程在调用 recvfrom 操作时不会阻塞 五种 IO 模型的对比： IO 多路复用： 工作模式： LT 模式：当 epoll_wait() 检测到描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用 epoll_wait() 会再次通知进程，默认模式，同时支持 Blocking 和 No-Blocking ET 模式：通知之后进程必须立即处理事件，下次不会对该事件通知，减少了 epoll 事件被重复触发的次数，效率高些，只支持 No-Blocking，防止出现饿死情况 应用场景： select：timeout 参数精度 1ns，而 poll 和 epoll 为 1ms，更加适用于实时性要求高的场景，同时兼容性也好一些 poll：没有最大描述符数量的限制，如果需要监控的描述符状态变化多，而且都是非常短暂的，没有必要使用 epoll。因为 epoll 中的所有描述符都存储在内核中，造成每次需要对描述符的状态改变都需要通过 epoll_ctl() 进行系统调用，频繁系统调用降低效率 epoll：只需要运行在 linux 上，并且有非常大量的描述符需要同时轮询 IO 概念区分： 阻塞 IO 和 非阻塞 IO：程序级别，当程序请求 OS IO 操作后，如果 IO 资源没有准备好，应该如何处理 同步 IO 和 异步 IO：操作系统级别，当程序请求 OS IO 操作后，如果 IO 资源没有准备好，该如何响应 BS 架构发展： 单线程：服务器同时只能处理单个请求，会造成多个客户端等待问题 多线程：accept 使用单线程方式，处理请求时使用多线程或者线程池，但是 accept 和 read 还是阻塞 Java IO 和 NIO 区别： 是否阻塞：IO 是阻塞的，NIO 则是非阻塞的 操作粒度： IO 中对流进行操作，读写操作按照字节为单位，简单，但是效率低 NIO 中对通道进行操作，读写操作按照块为单位，高效，但是缺少简单性 NIO 相关概念： 通道：是对流的模拟，通过通道可以读取并写入数据，即双向的 缓冲区：发送和接受通道中的数据都需要首先放到缓冲区中，包括 capacity，position 和 limit 成员，通过 flip 可以切换缓冲区的读写状态 选择器：通过轮询的方式去监听多个通道上的事件，让一个线程可以处理多个事件 创建选择器：Selector.open 将通道注册到选择器上：ssChannel.register，注册事件可以是 accept，也可以是 read &amp; write 监听事件：selector.select，会阻塞直到至少一个事件到达 获取到达的事件：selector.selectedKeys 事件循环 内存映射文件：是一种读写文件数据的方法，比常规的基于流和基于通道的 IO 快得多，通过 fc.map 创建该映射缓冲 MappedByteBuffer，就可以像使用 ByteBuffer 一样使用它 典型的多路复用 IO 实现： IO模型 性能 关键思路 操作系统 JAVA 支持情况 select 较高 Reactor windows&#x2F;Linux Reactor 模式 poll 较高 Reactor Linux Linux 下的 JAVA NIO 框架 epoll 高 Reactor&#x2F;Proactor Linux 使用 epoll 进行支持 kqueue 高 Proactor Linux 不支持 Reactor 模型：基于事件驱动，主要包括三个组件： Reactor：等待客户端的连接，并将其派发给 Acceptor Acceptor：进行客户端连接的获取，之后交给线程池进行网络读写 Handler：用于处理连接的网络读写操作，并进行事务处理（可以交给线程池） Java AIO 模型：由于此时采用的是订阅-通知方式，不需要 slector 了，改为 channel 直接到操作系统注册监听，windows 底层通过 IOCP 支持，linux 底层通过 epoll 模拟实现 Java NIO 框架： 原生 Java NIO 框架：基于 IO 多路复用 Mina：在 Java NIO 基础上提供了抽象的事件驱动程序 API Netty：提供异步的、事件驱动的网络应用程序框架和工具，综合性能最优 Grizzly：使用JAVA NIO作为基础，并隐藏其编程的复杂性 Java NIO 零拷贝基础： 通道：相当于操作系统的内核空间的缓冲区，全双工的 缓冲区：相当于操作系统的用户空间的缓冲区，分为堆内存和堆外内存 堆内存：在 GC 的时候可能会被自动回收，在 NIO 读写数据时，会将其临时拷贝到堆外内存 堆外内存（DirectBuffer）：在使用后需要手动回收，通过 malloc 实现 MappedByteBuffer：基于内存映射实现，继承自 ByteBuffer，如 FileChannel 中的 map 方法 写文件数据：put &amp; fore 读文件数据：get 实现原理：void *mmap64(void *addr, size_t len, int prot, int flags, int fd, off64_t offset) DirectByteBuffer：通过 DirectByteBuffer 静态方法 allocateDirect 分配内存，是 MappedByteBuffer 的具体实现类，因此，其本身也具有文件内存映射的功能 FileChannel：用于文件读写，映射和操作的通道，并且是线程安全的 tranferTo：把文件里面的源数据写入一个 WritableByteChannel 的目的通道 tranferFrom：把一个源通道 ReadableByteChannel 中的数据读取到当前 FileChannel 的文件里面 tranferTo 底层实现和 sendfile64 相关 RocketMQ 和 Kafka 对比： 10 Java 虚拟机 字节码文件：java 文件首先被编译为字节码文件，然后 JVM 在不同操作系统运行字节码文件，优点： 一次编写，处处执行 由于 JVM 直接运行字节码文件，可支持其他语言，如 Kotlin，scala，groovy 语言 字节码文件结构： 魔数和文件版本：开头四个字节为魔数，预期值是 0xCAFEBABE 常量池：字节码文件的资源仓库，主要存放字面量和符号引用等 访问标志：表示字节码文件的类型（类&#x2F;接口），访问类型，是否标记为 final 类索引，父类索引，接口索引 字段表属性：描述接口或类中声明的变量，如作用域，是否 static，final，数据类型 方法表属性：和字段表类似 反编译字节码文件：javac -g &lt;javafile&gt; &amp;&amp; javap -v -p &lt;classfile&gt;，相关信息解释 访问标志：如 ACC_SUPER：是否允许使用invokespecial字节码指令的新语义 ACC_SYNTHETIC：标志这个类并非由用户代码产生 ACC_ANNOTATION：标志这是一个注解 类型信息：基本类型通常首字母表示，但是存在特例： long 类型：J boolean 类型：Z 对象类型：L，如 Ljava&#x2F;lang&#x2F;Object; 数组：[，如定义一个String[][]类型的数组，记录为[[Ljava/lang/String; 方法表：Code 段里面的属性： stack：最大操作数栈，JVM 根据这个分配栈帧的深度 locals：局部变量所需的存储空间，以 Slot 为单位，4 个字节，注意 Slot 可以复用 args_size：方法参数个数，包含有隐藏参数 this 在内 LineNumberTable：描述源码行号与字节码行号(字节码偏移量)之间的对应关系 LocalVariableTable：描述帧栈中局部变量与源码中定义的变量之间的关系 类加载过程：类加载通常包括加载，验证，准备，解析和初始化五个阶段，其中解析阶段可以在初始化之后开始，这是为了支持 Java 的动态绑定，其他四个阶段按照顺序开始，但不一定按顺序结束 加载：可以使用系统的类加载器，也可以使用自定义类加载器，并且允许加载器提前加载某个类 通过类的全限定名获取其二进制字节流 将字节流代表的静态结构转化为方法区的运行时数据结构 在堆区生成代表该类的 java.lang.Class 对象 连接： 验证：确保被加载的类的正确性，包括文件格式验证，元数据验证，字节码验证和符号引用验证 准备：为类的静态变量分配内存，并将其初始化为默认值，注意并不是在 Java 代码中被显式赋予的值，但是，如果是 ConstantValue（static final），则在准备阶段就会被赋值 解析：把类中的符号引用转换为直接引用 初始化：为类的静态变量赋予正确的值，只有当对类的主动使用的时候才会导致类的初始化 如果该类还没有加载和连接，则先加载并连接 如果该类的直接父类还没有初始化，则先初始化直接父类 如果类中有初始化语句，则依次执行初始化语句 类加载器划分： 启动类加载器：Bootstrap ClassLoader，负责加载存在 JDK\\jre\\lib 或者被 -Xbootclasspath 参数指定路径的类库 扩展类加载器：Extension ClassLoader，负责加载 JDK\\jre\\lib\\ext 或者系统变量 java.ext.dirs 指定的所有类库 应用程序加载器：Application ClassLoader，负责加载用户路径（ClassPath）下指定的类库 类加载方式： 命令行启动时 JVM 初始化加载 Class.forName：将字节码加载到 JVM 中，并且默认执行类的 static 块，可通过参数控制 ClassLoader.loadClass：只会将字节码文件加载到 JVM 中 JVM 类加载机制： 全盘负责：当一个类加载器负责加载某个字节码时，其所依赖的和引用的字节码文件也将会交给该类加载器负责，除非显式指定 父类委托：先让父类试图加载该类，没有加载成功时，才尝试从自己的类路径中加载该类 缓存机制：保证所有加载过的字节码都会被缓存，这就是为什么修改了 Class 文件后，需要重启 JVM 双亲委派机制：将加载请求向上传播，只有当直接或者间接父类加载器无法加载时，才开始尝试加载 防止内存中出现多个同样的字节码 保证 Java 程序安全稳定的运行 自定义类加载器：继承自 ClassLoader，只需要重写 findClass 即可，注意不要重写 loadClass 方法，这样的话可能破坏双亲委派模式 运行时数据区：规定了 Java 在运行过程中内存申请，分配，管理的策略，保证了 JVM 高效运行 线程私有：程序计数器，虚拟机栈，本地方法区 线程共享：堆，方法区，堆外内存（Java7 中的永久代或 Java8 中的元空间） 程序计数器：用来存储指向下一条指令的地址，如果执行的是 Java 方法，记录的是 JVM 字节码指令地址，如果是 native 方法，则是未指定值（undefined） 虚拟机栈：每个线程创建时都会创建一个虚拟机栈，内部保存有一个个栈帧，对应着一次次的 Java 方法调用，不存在垃圾回收问题 栈的基本单位：栈帧，表示每次 Java 方法调用，保存方法执行中的各种数据信息 栈运行原理：方法调用对应栈帧入栈，方法退出或者异常退出对应栈帧出栈 栈帧内部结构： 局部变量表：基本存储单元是 Slot，32 位，如果是对象方法，this 存储在 0 号 Slot 处，其余参数按照顺序继续排列，注意 Slot 可重用 操作数栈：根据字节码指令，往操作数栈中写入数据或者提取数据，JVM 虚拟机解释引擎是基于栈的，但是这样的话可能带来性能问题，HotSpot JVM 提出栈顶缓存技术，将栈顶元素全部缓存在物理 CPU 的寄存器中，以此降低对内存的读&#x2F;写次数，提升执行引擎的执行效率 动态链接：将符号引用转换为调用方法的直接引用 非虚方法：如果方法在编译器就确定了具体的调用版本，并且该版本在运行时是不可变的，比如静态方法，私有方法，实例构造器，final 方法 虚方法：其他方法称为虚方法 方法返回地址：用来存放调用该方法的程序计数器的值 附加信息：携带与 Java 虚拟机实现相关的一些附加信息 本地方法栈：类似虚拟机栈，不过其用于管理本地方法的调用，本地方法就是 Java 调用非 Java 代码的接口，如 Unsafe 类中的本地方法，使用本地方法通常使用因为效率或者 Java 语言难以实现的问题，在 HotSpot JVM 中，直接将本地方法栈和虚拟机栈合二为一 堆内存： 内存划分：为了优化 GC 性能，逻辑上划分为三块内存 新生代：用于分配新对象和没到达一定年龄的对象，包括伊甸园（Eden Memory）和两个幸存区（Survivor Memory），默认比例 8：1：1，Minor GC 检查 Eden 空间和其中一个幸存区中的对象，并将他们移动到另一个幸存者空间 老年代：存放长时间使用的对象，也存储大对象，防止发生大量拷贝 元空间：Java8 之前称作是永久代， JDK8 及以后的元空间 设置堆内存大小：-Xms 设置堆的初始内存，-Xmx 表示堆的最大内存，通常两者配置相同，保证 GC 完成后不需要再重新分割计算堆的大小，提高性能，-XX:+UseAdaptiveSizePolicy 可以会动态调整 JVM 堆中各个区域的大小以及进入老年代的年龄 TLAB：对 Eden 区继续划分，JVM 为每个线程分配了一个私有缓存区域 避免了多线程使用同一个地址，需要使用加锁机制，降低性能 能够提升内存分配的吞吐量 逃逸分析：能够有效减少同步负载和内存堆分配压力的跨函数全局数据流分析算法 栈上分配：如果对象没有逃逸，直接在栈上分配对象 同步省略：一个对象只有一个线程访问，则不需要同步 标量替换：将聚合量变为多个标量表示，而不会创建对象，降低消耗 逃逸分析带来的性能提升不一定高于其带来的性能消耗，其本身是一个相对耗时的过程 方法区： 方法区，永久代，元数据区：永久代和元数据区可以当作是方法区的落地实现 方法区是 JVM 规范定义的一个概念，用于存储类信息，常量池，静态变量等 永久代是 HotSpot 虚拟机特有的概念，和老年代地址空间连续，可以被 GC 元空间则是永久代的替换，存在于堆外内存，不受限于 GC 方法区内部结构： 类型信息：保存每个被加载类型（类，接口，枚举，注解）的信息 运行时常量池：保存类加载后的常量池表，也可以运行期间放入，如 String 类 intern 方法 域信息：保存所有域的相关信息以及域的声明顺序 方法信息：保存方法的相关信息 注意，HotSpot JVM 中类型信息、字段、方法、常量保存在本地内存的元空间，但字符串常量池、静态变量仍在堆中 方法区的垃圾回收：常量池中废弃的常量和不再使用的类型 Java 内存模型：JVM 通过栈独占，堆共享来划分内存，方法的基本类型局部变量和对象引用栈上分配，而对象则分配在堆上，当 JMM 和现代硬件内存连接时，会产生以下问题： 对象共享后的可见性：由于高速缓存的存在，更新后的值可能其他线程不能看到，使用 volatile 竟态条件：两个线程对共享对象都执行加一操作，结果实际上值只加一，使用 synchronized 并发编程模型：主要分为共享内存和消息传递，Java 采用共享内存实现线程之前的通信，但需要程序员的显式同步操作 重排序：为了提高程序执行时性能，JMM 对于处理器重排序，会在必要时生成内存屏障 编译器优化的重排序：调整语句的执行顺序 指令级并行的重排序：多个指令重叠执行 内存系统的重排序：调整加载和存储指令的顺序 内存屏障指令：用来禁止特定类型的处理器重排序，其中，StoreLoad 屏障同时具有其他三个屏障的效果，实现原理是处理器要把写缓冲的数据刷写到内存，开销较大 屏障类型 指令示例 说明 LoadLoad Barriers Load1; LoadLoad; Load2 确保 Load1 数据的装载，之前于 Load2 及所有后续装载指令的装载。 StoreStore Barriers Store1; StoreStore; Store2 确保 Store1 数据对其他处理器可见（刷新到内存），之前于 Store2 及所有后续存储指令的存储。 LoadStore Barriers Load1; LoadStore; Store2 确保 Load1 数据装载，之前于 Store2 及所有后续的存储指令刷新到内存。 StoreLoad Barriers Store1; StoreLoad; Load2 确保 Store1 数据对其他处理器变得可见（指刷新到内存），之前于 Load2 及所有后续装载指令的装载。 happens-before：用来阐述操作之间的内存可见性，a hanppens-before b 表示 a 操作的结果对 b 操作来说是可见的，其满足传递性 as-if-serial：不管怎么重排序，单线程执行的结果不能被改变 Java 内存模型：将顺序一致性模型做为参考，同时对不存在数据依赖性的操作进行重排序 TSO（total store ordering）：允许写-读操作的重排序 PSO（partial store ordering）：在 TSO 基础上，允许写-写操作的重排序 RMO（elaxed memory ordering）：在 TSO 基础上，放松程序中读 - 写和读 - 读操作的顺序 对象回收算法： 引用计数算法：给对象增加一个引用计数器，表示当前引用的个数，尽管使用 Recycler 算法可以解决循环引用的问题，但是其性能消耗难以预测 可达性分析：通过 GC Roots 做为起始点进行搜索，不可达的对象可被回收，JVM 中的 GC Roots 虚拟机栈中引用的对象 本地方法栈中引用的对象 方法区中类静态属性引用的对象 方法区中的常量引用的对象 方法区的回收：主要存放永久代对象，对对象回收的效益不高，主要对常量池的回收和对类的卸载 finalize：类似析构函数，用于资源释放，但是 try-with-resources 方法更优，而且其执行时机是不确定的，可能还会由于自救机制导致对象不能回收 引用类型：不论是引用计数，还是可达性分析，都与引用相关 强引用：被强引用关联的对象不会被回收，new 软引用：被软引用关联的对象只有在内存不够的情况下才会被回收，SoftReference 弱引用：被弱引用关联的对象一定会被回收（下一次 GC 时），WeakReference 虚引用：一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，唯一目的是在该对象被回收的时候收到一个系统通知，PhantomReference 垃圾回收算法： 标记-清除：将存活的对象进行标记，然后清理掉未被标记的对象，碎片化现象严重 标记-整理：让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存 标记-复制：划分内存大小为相同的两块，每次只使用其中一块，另一块用于下一次复制操作 分代收集：根据对象存活周期，采用不同的收集算法，新生代使用复制算法，老年代使用标记-清除或者标记-整理算法 垃圾回收器： Serial 收集器和 Serial Old 收集器：前者新生代收集器，使用复制算法，后者老年代收集器，使用标记-整理算法 ParNew 收集器：是 Serial 收集器的多线程版本，是 Server 模式下的虚拟机首选新生代收集器 Parallel Scavenge 收集器和 Parallel Old 收集器：在注重吞吐量以及 CPU 资源敏感的场合可以使用 CMS（Concurrent Mark Sweep） 收集器：分为四个阶段，其中只有初始标记和重新标记需要 STW 缺点： 吞吐量低：低停顿时间是以牺牲吞吐量为代价的 无法处理浮动垃圾，容易引发 Concurrent Mode Failure：浮动垃圾指的是并发清理阶段用户线程产生的垃圾，需要等到下一次 GC 才能被回收，由于浮动垃圾，需要预留出一部分的内存，如果不够，则出现 Concurrent Mode Failure 标记-清除容易导致空间碎片，大对象可能分配失败，因此需要提前触发 Full GC G1（Garbage First） 收集器：面向服务端应用的垃圾收集器，开发目的用于替换 CMS 收集器。G1 可以直接对新生代和老年代一起回收，G1 把堆划分成多个大小相等的独立区域(Region)，新生代和老年代不再物理隔离，每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region，可达性分析的时候可以避免全堆扫描 特点： 空间整合：整体上是基于标记-整理算法，局部（两个 region 之间）上基于复制算法 可预测的停顿 Epsilon 收集器：不执行任何垃圾回收动作的回收器，主要用作性能分析 回收策略： Minor GC：发生在新生代上，执行相对比较频繁 触发条件：Eden 空间满，就触发一次 Major GC：发生在老年代上，较少执行 触发条件： 调用 System.gc：建议虚拟机执行 Major GC 老年代空间不足：大对象或者大数组创建 空间分配担保失败：使用复制算法的 Minor GC 需要老年代的内存空间作担保 Concurrent Mode Failure Full GC：整个堆上的垃圾回收 G1 收集器：是一个分代的，增量的，并行与并发的标记-复制垃圾回收器，是为了适应现在不断扩大的内存和不断增加的处理器数量，进一步降低暂停时间（pause time），同时兼顾良好的吞吐量，相对于 CMS： G1 垃圾回收器是 compacting 的，回收得到的空间是连续的 G1 回收器的内存模型是分块的，其将内存划分为一个个的 Region，Region 为回收的基本单位 软实时：用户可以指定垃圾回收时间的限时，G1 会努力但不保证在这个时限内完成垃圾回收 G1 内存模型： 分区概念： 分区 Region：将整个堆空间按照分区进行划分，分区可以按需在年轻代和老年代之间切换 卡片 Card：每个分区内部划分为一系列的 Card，标识可用分区的卡片将会被记录在全局卡片表中 堆 Heap：同样可以通过 -Xms&#x2F;-Xmx 来指定堆空间大小 分代模型： 分代垃圾回收：可以将重点放在最近分配的对象上，而无需扫描整个堆空间，同样的，G1 采用了年轻代和老年代来划分，其中年轻代由 Eden，S1，S2 空间构成 本地分配缓冲：由于分区，每个线程可以认领某个分区用于线程本地的内存分配，减少了同步时间 分区模型：G1 对内存的使用以分区(Region)为单位，而对对象的分配则以卡片(Card)为单位 巨型对象：大小超过分区大小一半的对象，分配的时候直接在老年代分配，所占用的空间称为巨型分区（Humongous Region） 已记忆集合（Remember Set）：为了避免 STW 式的整堆扫描，在每个分区记录了一个已记忆集合(RSet)，内部类似一个反向指针，记录引用分区内对象的卡片索引，当回收该分区时，只需要遍历 RSet 内的卡片索引就可以了 Per Region Table (PRT)：Rset 需要占用部分空间，如果一个分区被引用的次数很多，可能需要大量空间保存卡片索引，为此，其内部使用的 PRT 提供三种粒度的记录： 稀少：直接记录引用对象的卡片索引 细粒度：记录引用对象的分区索引 粗粒度：只记录引用情况，每个分区对应一个比特位，需要遍历整个堆才能找到所有引用 收集集合（CSet）：代表每次 GC 暂停时回收的一系列目标分区，在任意一次收集暂停中，CSet所有分区都会被释放，内部存活的对象都会被转移到分配的空闲分区中 并发标记算法：使用的是三色标记法，白色是未标记；灰色自身被标记，引用的对象未标记；黑色自身与引用对象都已标记。最终黑色的对象是存活对象，白色的是垃圾 漏标问题：在标记过程中，如果发生白色对象被黑色对象引用，并且灰指向白引用的消失，此时白色对象可能被当作垃圾清理，可以通过： 跟踪黑指向白的增加（增量更新）：关注引用的增加，把黑色重新标记为灰色，需要重新扫描属性，效率较低 记录灰指向白的消失（起始快照算法）：当该引用删除时，将被引用对象推到 GC 的推栈，G1 采用该方法，因为其效率较高，不需要再次扫描 G1 活动周期： 垃圾活动周期图： RSet 的维护： 栅栏 Barrier：obj1.a = obj2 ，通过写前栅栏标记原来丧失引用的对象，通过写后栅栏更新新的被引用的对象所在分区，并不是每次栅栏操作后都需要更新 RSet，可以在 SATB 或者并发优化线程中批量执行 起始快照算法（SATB）：在清理工作开始时创建堆的逻辑快照，从而确保所有垃圾对象被鉴别出来，写前栅栏会在引用变更前，将值记录在 SATB 日志中 并发优化线程（Concurrence Refinement Threads）：写后栅栏会先通过 G1 的过滤技术判断是否是跨分区的引用更新，将跨分区更新对象的卡片加入缓冲区序列，即更新日志缓冲区或脏卡片队列 并发标记周期：这个阶段将会为混合收集周期识别垃圾最多的老年代分区 初始标记：负责标记所有能被直接可达的根对象(原生栈对象、全局对象、JNI对象) 根分区扫描：为了保证标记算法的正确性，所有新复制到Survivor分区的对象，都需要被扫描并标记成根 并发标记：根据根对象，进行并发标记 重新标记：去处理剩下的 SATB 日志缓冲区和所有更新，找出所有未被访问的存活对象 收集类型： 年轻代收集：在年轻代满时，触发年轻代收集 混合收集周期：随着老年代内存增长，当到达IHOP阈值(老年代占整堆比，默认45%)时，G1开始着手准备收集老年代空间 ZGC：JDK11 中引入，适用于大内存低延迟服务的内存管理和回收，设计目标是 停顿时间不超过 10ms 停顿时间不会随着堆的大小，或者活跃对象的大小而增加 支持最高 4TB 级别的堆 CMS 和 G1 停顿分析：都使用了标记-复制算法，三个阶段中 标记阶段：初始标记阶段和再标记阶段是 STW，并发标记阶段不是 STW 清理阶段：清点出有存活对象的分区和没有存活对象的分区，该阶段是 STW 复制阶段：复制耗时和对象数量成正比，也是 STW ZGC 原理： 全并发的 ZGC：采用标记-复制算法，不过在标记，复制和重定向阶段几乎都是并发的，只有初始标记，再标记和初始转移是 STW ZGC 关键技术：通过着色指针和读屏障技术，解决了转移过程中准确访问对象的问题，实现了并发转移 着色指针：将信息存储在指针中的技术，指针中第42~45位存储元数据，分别表示 Marked 0，Marked 1，Remapped，Finalized 信息 读屏障：当应用线程从堆中读取对象引用时，就会执行读屏障代码，其作用是在对象标记和转移过程中，用于确定对象的引用地址是否满足条件，并作出相应动作 ZGC 调优： ZGC 调优参数： 1234567-Xms10G -Xmx10G -XX:ReservedCodeCacheSize=256m -XX:InitialCodeCacheSize=256m -XX:+UnlockExperimentalVMOptions -XX:+UseZGC -XX:ConcGCThreads=2 -XX:ParallelGCThreads=6 -XX:ZCollectionInterval=120 -XX:ZAllocationSpikeTolerance=5 -XX:+UnlockDiagnosticVMOptions -XX:-ZProactive -Xlog:safepoint,classhisto*=trace,age*,gc*=info:file=/opt/logs/logs/gc-%t.log:time,tid,tags:filecount=5,filesize=50m ZGC 触发时机： 阻塞内存分配请求触发：垃圾占满堆空间 基于分配速率的自适应算法：根据近期对象分配速率以及 GC 时间，计算下一次触发 GC 内存阈值 基于固定间隔：通过 ZCollectionInterval 控制，适合应对突增流量场景 主动触发规则：类似于固定间隔规则，但时间间隔不固定，是ZGC自行算出来的时机 预热规则：服务刚启动时出现，一般不需要关注 外部触发：代码中显式调用 System.gc() 触发 元数据分配触发：元数据区不足时导致，一般不需要关注 GC 考虑指标： 吞吐量：业务线程占用 CPU 的时间和系统总运行时间比例 停顿时间：垃圾收集过程中一次 STW 的最长时间，越短越好 JVM 常见调优参数： -Xms 和 -Xmx：堆初始值和堆最大值，通常设置相同，避免动态扩容的开销 -Xmn：新生代大小 -XX:newRatio：设置新生代与老年代比值 -XX:SurvivorRatio：Eden 区与 Survivor 区大小的比值 -XX:PermSize 和 -XX:MaxPermSize：永久代初始值和永久代最大值，Java7 参数 -XX:MetaspaceSize 和 -XX:MaxMetaspaceSize：元空间初始值和元空间最大值 -XX:MaxTenuringThreshold：新生代转移到老年代的年龄阈值 -XX:+AggressiveOpts：加快编译速度 -XX:PretenureSizeThreshold：对象超过多大值时直接在老年代中分配 JVM 回收期参数： -XX:+UseSerialGC：串行垃圾回收，很少使用 -XX:+UseParNewGC：新生代使用并行，老年代使用串行 -XX:+UseConcMarkSweepGC：新生代使用并行，老年代使用 CMS -XX:ParallelGCThreads：指定并行的垃圾回收线程的数量 -XX:+DisableExplicitGC：禁用 System.gc()，因为它会触发Full GC，这是很浪费性能的 -XX:CMSFullGCsBeforeCompaction：在多少次 GC 后进行内存整理，减少碎片化 -XX:+UseCMSCompactAtFullCollection：在每一次Full GC时对老年代区域碎片整理 -XX:+CmsClassUnloadingEnabled：卸载类信息，也就是对永久带清理 -XX:+PrintGCDetails 和 -XX:+PrintGCDateStamps：打印 GC 信息和时戳 内存溢出问题： 堆内存溢出： 堆内存溢出：大量创建对象，并且让 GC Roots 引用到它们 大量时间用于 GC，表示即将发生上类错误：98% 时间只回收了 2% 的垃圾 元空间区内存溢出： 不停加载类，导致元空间内存溢出 堆内存 dump 分析： 通过 OOM 获取：-XX:+HeapDumpOnOutOfMemoryError 主动获取：-XX:+HeapDumpOnCtrlBreak 使用 HPROF agent：-agentlib:hprof&#x3D;heap&#x3D;dump,format&#x3D;b，在结束时生成 Dump 文件 jmap 获取：jmap -dump:format=b file=&lt;文件名XX.hprof&gt; &lt;pid&gt; 堆内存分析：JConsole 和 Jprofile Thread Dump 分析：诊断 Java 应用问题的工具，提供了当前活动线程的快照，以及 JVM 中所有 Java 线程的堆栈跟踪信息 Thread Dump 抓取： jps &amp;&amp; jstack [-l] &lt;pid&gt; | tee -a jstack.log Thread 状态分析： NEW：刚刚在堆中创建 Thread 对象，但是没有调用 start 方法前 RUNNABLE：该状态表示线程具备所有运行条件，在运行队列中准备操作系统的调度，或者正在运行 BLOCKED：线程正在等待获取 java 对象的监视器，即线程正在等待进入由 synchronized 代码块 WAITING：只有特定的条件满足，才能获得执行机会，如 Object.wait TIMED_WAITING：定时器等待 TERMINATED：执行完 run 方法正常返回，或者抛出了运行时异常而结束 异常情况： 死锁：表现为程序的停顿，或者不再响应用户的请求，线程 dump 中可以直接报告出 Java 级别的死锁 热锁：往往是导致系统性能瓶颈的主要因素，表现为由于多个线程对临界区，或者锁的竞争，出现 频繁的线程的上下文切换 大量的系统调用 随着CPU数目的增多，系统的性能反而下降 Java 问题排查工具： Linux 命令 文本操作：grep，awk，sed 文件操作：tail，find 网络和进程：ifconfig，iptables，route，netstat，ps，top 磁盘和内存：free，df，du，&#x2F;proc&#x2F;meminfo，fdisk，swapon，swapoff 用户和组：w，id，last，cut -d: -f1 &#x2F;etc&#x2F;passwd 服务模块和包：crontab -l，lsmod 系统版本信息：uptime，uname，procfs Java 工具 jps：获取当前 java 进程的工具 jstack：线程堆栈分析工具，导出 Java 应用程序线程堆栈信息 jinfo：用来查看正在运行的 java 应用程序的扩展参数，也可以动态的修改正在运行的 JVM 参数 jmap：可以生成 java 程序的 dump 文件， 也可以查看堆内对象示例的统计信息、查看 ClassLoader 的信息 jstat：输出进程的统计信息 jdb：可以远程 debug btrace：可以在运行中的java类中动态的注入trace代码，在不停机下得到方法参数，返回值，起到监控作用 Greys：用来分析运行中的java类、方法等信息 Arthas：在线调试，基于 Greys javOSize：可以修改字节码，并且即时生效，但是侵入性太大 可视化工具： JConsole：自带的基于 JMX 的可视化监视、管理工具 Visual VM：免费的，集成了多个 JDK 命令行工具的可视化工具，它能为您提供强大的分析能力，对 Java 应用程序做性能分析和调优 Visual GC：visualvm 中的图形化查看 gc 状况的插件 JProfiler：通过实时的监控系统的内存使用情况，随时监视垃圾回收，线程运行状况等手段，从而很好的监视 JVM 运行情况及其性能 Eclipse Memory Analyzer (MAT)：快速且功能丰富的 Java 堆分析器，可帮助你发现内存泄漏并减少内存消耗 11 多线程和并发并发出现问题的根源： 可见性：CPU 缓存引起 原子性：分时复用引起 有序性：重排序引起 使用 JMM 解决并发问题： 三个关键字：volatile，synchronized，final Happens-Before 规则 线程安全级别： 不可变：不可变对象一定是线程安全的，不需要再采取任何线程安全保障措施，包括： final 关键字修饰的基本数据类型 String 枚举类型 Number 部分子类，如 Long 和 Double 等包装类型，BigInteger 和 BigDecimal 等大数据类型 对集合，可以使用 Collections.unmodifiableXXX() 来获取一个不可变集合 绝对线程安全：不管运行时环境如何，调用者都不需要任何额外的同步措施 相对线程安全：单独操作时候，不需要额外的同步措施，但是对一些特定顺序的连续调用，需要在调用段使用额外的同步手段，包括 Vector、HashTable、Collections 的 synchronizedCollection() 方法包装的集合 线程兼容：对象本身不是线程安全的，可以在调用端正确使用同步手段安全使用，如 ArrayList 和HashMap 等 线程对立：无论是否在调用段采取了同步措施，都无法在多线程环境中并发安全使用的代码 线程安全实现： 互斥同步：synchronized 和 ReentrantLock，线程阻塞和唤醒可能带来性能问题，悲观并发策略 非阻塞同步： CAS：乐观并发控制，先进行操作，如果没有其它线程争用共享数据，那操作就成功了，否则采取补偿措施，如不断重试。硬件支持，CAS 指令包括三个操作数，内存地址 V、旧的预期值 A 和新值 B，当执行操作时，只有当 V 的值等于 A，才将 V 的值更新为 B AtomicInteger：使用了 Unsafe 类的 CAS 操作封装而成 ABA 问题：如果一个变量初次读取的时候是 A 值，它的值被改成了 B，后来又被改回为 A，那 CAS 操作就会误认为它从来没有被改变过吗，该问题可以通过 AtomicStampedReference 来解决 无同步方案： 栈封闭：如局部基本类型变量，属于线程私有 线程本地存储：使用 ThreadLocal 类实现，底层实现是每个 Thread 都有一个 ThreadLocalMap 可重入代码：在代码执行的任何时刻中断它，转而去执行另外一段代码，在控制权返回后，原来的程序不会出现任何错误 线程状态转换： 线程使用方式：实现接口或者继承 Thread，推荐使用实现接口方案 实现 Runnable 接口：实现 run 方法 实现 Callable 接口：实现 带有返回值的 call 方法 继承 Thread 类：同样实现 run 方法，其实现了 Runnable 接口 基础线程机制： Executor：管理多个异步任务的执行，此处异步表示不需要同步操作 CachedThreadPool：一个任务一个线程 FixedThreadPool：所有任务使用固定数量的线程 SingleThreadExecutor： 相当于大小为 1 的 FixedThreadPool Daemon：守护线程，是程序运行时在后台提供服务的线程，当所有非守护线程结束时，程序终止，同时杀死所有守护线程，main() 属于给守护线程，使用 setDaemon 设置某个线程为守护线程 sleep：休眠当前正在执行的线程 yield：声明了当前线程已经完成了生命周期中最重要的部分，可以切换给其它线程来执行，只是对线程调度器的一个建议，而且也只是建议具有相同优先级的其它线程可以运行 线程中断： InterruptedException：使用 interrupt 来中断线程，如果线程处于阻塞，等待状态，就会抛出该异常而提前结束，但是不能中断 IO 阻塞和 synchronized 锁阻塞 interrupted：如果线程一直运行一个循环，那么 interrupt 并不会导致其提前结束，可以通过 interrupted 方法检测线程是否被中断过，从而自己合理响应中断 Executor 中断操作：shutdown 会等待所有线程执行完毕后再关闭，但是 shutdownNow 则直接调用每个线程的 interrupt 方法，另外，如果只想中断其中某个线程，可以使用 submit 方法得到 Future&lt;?&gt; 对象，然后调用其 cancel 方法中断 线程互斥同步： synchronized：JVM 实现 同步代码块：自定义同步对象 同步实例方法：作用于同一个对象实例 同步静态方法：作用于同一个类 同步一个类：作用于同一个类 ReentrantLock：JDK 实现，主要通过 lock 和 unlock 实现同步 比较： 实现方式：synchronized 是 JVM 实现，ReentrantLock 是 JDK 实现 性能：新版本对 synchronized 进行了很多优化，两者大致相同 等待可中断：ReentrantLock 可以，但是 synchronized 不行 公平锁：默认非公平，但是 ReentrantLock 可以配置为公平的 锁绑定条件：一个 ReentrantLock 可以同时绑定多个 Condition 对象 使用：优先使用 synchronized， 除非需要 ReentrantLock 的高级功能，使用 synchronized 所有版本支持，并且不用担心没有释放锁而导致死锁问题 线程协作： join：在线程中调用另一个线程的 join 方法，会将当前线程挂起，直到目标线程结束 wait &amp; notify &amp; notifyAll：属于 Object 一部分，只能在同步方法或者代码块中使用，并且在 wait 期间，线程会释放锁，否则可能会造成死锁 await &amp; signal &amp; signalAll：Condition 一部分，相较于 wait，await 可以指定等待的条件，更加灵活 synchronized 使用： 一把锁只能被一个线程获取，没有获取的线程进入阻塞状态 每个实例对应自己的一把锁，不同实例之间互不影响，锁对象是 *.class 和修饰类方法除外 修饰方法时，无论是正常退出还是抛出异常，都会释放锁 synchronized 实现原理： 对象在堆中的结构：对象头 + 实例变量 + 填充数据，JVM 采用两个字来记录对象头信息，如果是数组，则还有一个字用于储存数据的长度 Class Metadata Address：类型指针指向对象的类数据 Mark Word：存储对象的 hashcode，锁信息或分代年龄或 GC 标志等信息，无锁状态 001 结尾 字节码实现：主要通过 monitorenter 和 monitorexit 实现，并且字节码中还额外注入一条 monitorexit 指令，用于异常时能够释放 monitor 的所有权，每个对象都关联了一个 monitor 对象 可重入原理：在同一锁程中，线程不需要再次获取同一把锁，直接将 monitor 中计数器加一即可 JVM 锁优化： 锁粗化：减少不必要的连续的 unlock 和 lock 操作，将多个小范围的锁扩展成更大的锁 锁消除：通过 JIT 逃逸分析判断出并不需要同步的操作 轻量级锁：使用 CAS 来进行锁的获取和释放，失败则阻塞 偏向锁：为了在无锁情况下执行 CAS 带来的开销 适应性自旋：当线程在轻量级锁执行 CAS 操作失败时，在进入重量级锁前会忙等待一段时间 锁膨胀方向：无锁 → 偏向锁 → 轻量级锁 → 重量级锁 (此过程是不可逆的) 轻量级锁实现：在执行同步块前，JVM 会在栈帧里面创建一个 Lock Record 空间，用于存储锁对象目前的 Mark Word 拷贝，然后，虚拟机使用 CAS 操作将标记字段 Mark Word 拷贝到锁记录中，并且将 Mark Word 更新为指向 Lock Record 的指针，并且更新对象的锁标志位，如果 CAS 操作失败，则锁膨胀 偏向锁：大多数环境下，锁只由同一个线程多次获取，反复加锁和释放锁可能带来性能消耗。当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程 ID，以后该线程在进入和推出同步块时不需要进行 CAS 操作来加锁和解锁 锁的优缺点对比： 锁 优点 缺点 使用场景 偏向锁 加锁和解锁不需要CAS操作，没有额外的性能消耗，和执行非同步方法相比仅存在纳秒级的差距 如果线程间存在锁竞争，会带来额外的锁撤销的消耗 适用于只有一个线程访问同步块的场景 轻量级锁 竞争的线程不会阻塞，提高了响应速度 如线程成始终得不到锁竞争的线程，使用自旋会消耗CPU性能 追求响应时间，同步块执行速度非常快 重量级锁 线程竞争不适用自旋，不会消耗CPU 线程阻塞，响应时间缓慢，在多线程下，频繁的获取释放锁，会带来巨大的性能消耗 追求吞吐量，同步块执行速度较长 volatile 作用： 防止重排序：单例模式中的双重检查加锁（DCL） 实现可见性：防止 CPU 缓存造成线程不可见 保证单次读写的原子性：i++ 并不是原子性操作；共享 long 和 double 型变量时，需要使用 volatile volatile 实现原理： 可见性实现：基于内存屏障实现，起作用就是防止编译器和 CPU 对该条内存屏障指令重排序 lock 指令：对 volatile 变量写操作后，JVM 插入的指令，用于将缓存中的数据写到内存 缓存一致性：由于缓存的存在，使用 MESI 机制保证缓存一致性，其机制是利用了总线嗅探协议 有序性实现： volatile 的 happens-before 关系 禁止重排序：JMM 采用保守策略来为每个 volatile 读写操作添加不同屏障 对于 volatile 写操作，在其前面加入 StoreStore 屏障，在其后面加入 StoreLoad 屏障 对于 volatile 读操作，在其后面依次加上 LoadLoad 屏障和 LoadStore 屏障 volatile 应用场景： 状态标志：用于指示发生了某个一次性事件 独立观察：单个写多个读情况 volatile bean 模式：JavaBean 的所有数据成员都是 volatile 类型的，并且 getter 和 setter 必须普通 双重检查：单例模式 DCL final 基础使用： 修饰类：表示该类不可被继承，可以通过组合关系实现扩展性，注意 final 类中的所有方法都为隐式 final 修饰方法：表示该类不可被子类重写，注意 private 方法是隐式的 final 方法 final 方法可以被重载 修饰参数：无法在方法中更改参数引用指向的对象，用于匿名类中传输数据 修饰变量：不一定是编译器常量，只是在被初始化后其值无法被更改 static final：必须在定义的时候进行赋值 blank final：必须在构造器中进行赋值 final 域重排序： 基本数据类型： final 域写：禁止 final 域写重排序到构造方法之外，从而保证该对象对所有线程可见时，final 域全部被初始化完毕，构造函数返回前插入 StoreStore final 域读：禁止初次读对象的引用与读该对象包含的 final 域的重排序，读操作前插入 LoadLoad 引用数据类型： 额外增加约束：禁止在构造函数对一个 final 修饰的对象的成员域的写入与随后将这个被构造的对象的引用赋值给引用变量重排序 final 深入理解： 对于存在 final 域初始化的构造函数中，不能让对象引用提前冲构造器中逃逸出去 final 对象引用只是表示引用不变，对象本身还是可变的 使用 final 将禁止 JVM 进行自动类型转换，如两个 byte 相加和两个 final byte 相加 CAS 问题：CAS 基于乐观锁，synchronized 为悲观锁，通常 CAS 性能更优，但是其存在以下问题： ABA 问题：CAS 检测不到变化，但实际上发生了变化，使用 AtomicStampedReference 循环时间开销大：自旋 CAS 如果长时间不成功，会给 CPU 带来非常大的执行开销 只能保证一个共享变量的原子操作：使用对象保证多个变量的原子性，使用 AtomicReference Unsafe 类：Java 原子类通过 Unsafe 实现，其主要提供一些用于执行低级别、不安全操作的方法，这些方法在提升 Java 运行效率、增强 Java 语言底层资源操作能力方面起到了很大的作用，但是使用 Unsafe 类方法会使得 Java 语言不再安全，应该慎用 Unsafe 类 Unsafe 类部分功能： CAS：实际上只提供了三种 CAS 本地方法：compareAndSwap&#123;Object, Int, Long&#125; 提供域偏移量：staticFieldOffset 内存操作：allocateMemory &amp; reallocateMemory &amp; freeMemory Java 中原子类：实现方式基于 volatile 和 Unsafe 中的 CAS 方法，前者保证可见性，后者保证原子性 原子更新基本类型：Atomic&#123;Boolean, Integer, Long&#125; 原子更新数组：Atomic&#123;Integer, Long, Reference&#125;Array 原子更新引用类型：AtomicReference， Atomic&#123;Stamped, Markable&#125;Reference 原子更新字段类：Atomic&#123;Integer, Long, Stamped, Reference&#125;FieldUpdater LockSupport：用于创建锁和其他同步类的基本线程阻塞原语，基于 Unsafe 类中的 park &amp; unpark，相当于只有一个许可证的 Semaphore park：线程会阻塞，直到以下情况发生 其他线程将当前线程作为参数调用 unpark 其他线程中断当前线程 该调用毫无理由的返回 unpark：将等待获得许可的线程作为参数，好让参数线程继续运行 线程同步分析： sleep 和 wait 区别：sleep 不会释放锁，wait 会释放锁，当从 wait 状态唤醒时，还是需要进行锁的获取，如果没有获取到，线程进入阻塞状态 wait 和 await 区别：原理基本一致，且都释放锁资源，不过 await 底层通过 park 来阻塞线程 sleep 和 park 区别：都是阻塞当前线程执行，且都不释放占有的锁资源，不过 park 可以主动被唤醒 wait 和 park 区别： wait 需要在 synchronized 块中执行，park 可以在任何地方 wait 被唤醒后不一定立即执行后续内容，因为需要获取锁，park 唤醒后则会继续执行后续内容 wait &amp; notify 和 park &amp; unpark：前者必须先 wait 再 notify，否则一直等待，后者则不用 AQS（AbstractQueuedSynchronizer）：一个用来构建锁和同步器的框架，对资源共享方式： 独占：只有一个线程能执行，如 ReentrantLock，还可以分为： 公平锁：按照 FIFO 规则依次获取锁资源 非公平锁：当线程要获取锁时，无视队列规则直接去抢锁 共享：多个线程可同时执行，如 Semaphore&#x2F;CountDownLock 扩展 AQS：AQS 使用了模板方法设计模式，继承者只需要实现以下方法即可 tryAcquire &amp; tryRelease tryAcquireShared &amp; tryReleaseShared isHeldExclusively AQS 数据结构： sync queue：CLH 实现的双向链表 Node：表示每个被封装的线程，每个节点都有一个状态： CANCELLED，值为 1，表示当前的线程被取消 SIGNAL，值为 -1，表示当前节点的后继节点包含的线程需要运行，需要进行 unpark 操作 CONDITION，值为 -2，表示当前节点在等待 condition，也就是在 condition queue 中 PROPAGATE，值为 -3，表示当前场景下后续的 acquireShared 能够得以执行 值为0，表示当前节点在 sync queue 中，等待着获取锁 condition queue：单向链表实现 AQS 核心方法： acquire：以独占模式获取资源，并且忽略中断 tryAcquire 方法如果失败，则将该线程封装线程添加到 sync queue 中，其中，enq 方法会使用无限循环来确保节点的成功插入 acquireQueued 方法则是让 sync queue 中的第二个节点尝试获取资源，如果失败，则自旋等待 release：如果头节点不为空，则 unparkSuccessor AQS 总结： AQS 通过一个 int 同步状态码，和一个 FIFO 队列来控制多个线程访问资源 支持独占和共享两种模式获取同步状态码 当线程获取同步状态失败会被加入到同步队列中 当线程释放同步状态，会唤醒后继节点来获取同步状态 共享模式下的节点获取到同步状态或者释放同步状态时，不仅会唤醒后继节点，还会向后传播，唤醒所有同步节点 使用 volatile 关键字保证状态码在线程间的可见性，CAS 操作保证修改状态码过程的原子性 ReentrantLock 源码分析： 接口实现：实现了 Lock 接口，定义了 lock 和 unlock 相关操作，并且存在 newCondition 方法 三个内部类： Sync：继承自 AQS，未实现 lock 算法 NonfairSync &amp; FairSync：继承自 Sync，分别实现非公平锁和公平锁 锁控制：对该类的操作大部分直接转换为对 Sync 类的操作 可重入性：获取独占资源的线程，可以重复获取该独占资源，实现上通过计数器加一即可 ReentrantLock 和 synchronized 对比： 底层实现上：ReentrantLock 是 JDK 提供的，synchronized 是 JVM 提供的 手动释放：前者需要手动释放，最好配合 try-finally，后者不需要自己释放 是否可中断：前者可以通过 lockInterruptibly 响应中断，后者不可中断 是否公平锁：前者可以是公平锁，后者不是公平锁 是否可绑定 Condition：前者可以绑定 Condition 结合 await&#x2F;signal 实现线程精确唤醒，后者则不行 ReentrantReadWriteLock 源码分析： 接口实现：实现了 ReadWriteLock 接口 类的内部类： Sync：继承自 AQS，使用一个 int 表示写锁（16bit）和读锁（16bit）数量，存在以下两个内部类 HoldCounter：和读锁配套使用 ThreadLocalHoldCounter：和写锁配套使用 NonfairSync &amp; FairSync：继承自 Sync，分别实现非公平锁和公平锁 ReadLock &amp; WriteLock：实现了 Lock 接口 类属性：同步队列 sync 和两个锁资源 readLock &amp; wirteLock ReentrantReadWriteLock.Sync 关键方法： tryRelease：用于尝试释放写锁资源，若释放后资源数量（state）为 0，则成功释放该锁 tryAcquire：用于尝试获取写锁资源，如果当前资源数量（state）为 0，则成功获取，根据公平策略判断其是否会阻塞 tryReleaseShared：用于尝试释放读锁资源，使用无限循环保证释放成功 tryAcquireShared：用于尝试获取读锁资源，若存在写锁，则失败，否则，判断读线程是否需要被阻塞，若之前没有读锁，还需要设置第一个读线程 firstReader 和 firstReaderHoldCount 锁升降级： 锁降级：线程把持住（当前拥有的）写锁，再获取到读锁，随后释放（先前拥有的）写锁的过程 锁升级：RentrantReadWriteLock 不支持锁升级，目的是为了保证数据的可见性 HashTable：利用 synchronized 对 put 等操作进行加锁，从而加锁期间锁住的是整个哈希表，效率低下 ConcurrentHashMap - JDK7：使用分段锁机制实现，从而保证了并发度的提升 数据结构：整个 ConcurrentHashMap 由一组 Segment 组成，其通过继承 ReentrantLock 来加锁，外部 ConcurrentHashMap 的并发度由 concurrencyLevel 确定，实际上就是 Segement 的个数 put 操作： 根据 hash 值找到对应的 segment segment 内部首先尝试获取锁资源，获取成功后进行 put 操作 get 操作： 计算 hash 值，找到对应的 segment 根据 hash 找到该 segment 内部数组对应的位置 根据链表顺序查找 ConcurrentHashMap - JDK8： 数据结构：实现上选择和 HashMap 类似的数组 + 链表 + 红黑树的方式，而加锁采用 CAS 和 synchronized 实现，并发度相较于 JDK7 上升 CopyOnWriteArrayList： 概述：是 ArrayList 的一个线程安全的变体，其中所有可变操作)都是通过对底层数组进行一次新的拷贝来实现的，使用 lock 保证并发安全性 实现关系：实现了 List，RandomAccess，Clonable 接口 内部类：COWIterator，其存在一个 Object 类型的数组作为 CopyOnWriteArrayList 数组的快照，因此，在创建迭代器后，迭代器就不会反映列表的修改，同时，在迭代器上不支持修改操作 类属性：lock 字段用于保证线程安全访问，还有一个 Object 数组，用来存放具体元素，使用反射机制和 CAS 来原子更新 lock 字段 基于数组拷贝实现：add &amp; addIfAbsent &amp; set &amp; remove 缺陷： 由于写操作的时候，需要拷贝数据，可能会导致 young gc 或者 full gc 不能用于实时读的场景，因为拷贝数据本身需要时间，其能保持最终一致性，但是没法满足实时性 使用场景：合适读多写少的场景，但是慎用，因为不知道里面到底放置了多少数据 和 Vector 的比较：尽管 Vector 里面的每个方法都是同步的，但是使用上还需要额外在外层加上一层锁，双重锁会导致性能大幅降低，如 size &amp;&amp; remove &amp;&amp; remove(size - 1)，第一个线程第二次 remove 时会出现错误，可以使用 CopyOnWriteArrayList 来代替 Vector ConcurrentLinkedQueue： 简介：基于链接节点的无界线程安全队列，排序规则基于 FIFO，不允许插入 null 元素 数据结构： 内部类 Node(item, next)，其通过 CAS 和反射机制来原子更新 item 和 next 包含 head 和 tail 的属性，表示头尾节点，但是其并不是表示队列的第一个元素或者最后一个元素，因为延迟更新的存在，head 可能是之前的第一个节点，但是已经被移除，tail 可能是最后一个节点的前一个节点 核心函数： offer：采用无限循环确保数据被插入到队列中，并且需要判断 tail 是否指向最后一个节点，如果不是，此时插入后还需要更新 tail 节点值 poll：同样采用无限循环保证操作被正确进行，如果 head 的下一个节点为空，则需要更新 head 节点 非阻塞：当队列为空时，poll 将直接返回 null 而不会阻塞 延迟更新：只有当 head 和 tail 距离真正的第一个有效节点和最后一个有效节点大于等于 2 时才会更新，这样带来的好处是减少了 CAS 更新的操作，大大提升了操作效率，缺点是代码复杂性增大 使用场景：通过无锁实现了更高的并发量，是一个高性能的队列，但是使用场景没有阻塞队列常见，其通常使用在并发量特别大的情况下 BlockingQueue &amp; BlockingDeque： 简介：阻塞队列和双向阻塞队列接口，适用于生常-消费场景 方法：不同操作方法处理异常的情况不同 抛异常 特定值 阻塞 超时 插入 add(o) offer(o) put(o) offer(o, timeout, timeunit) 移除 remove(o) poll(o) take(o) poll(timeout, timeunit) 检查 element(o) peek(o) 实现类： ArrayBlockingQueue：有界阻塞队列，并不会自动扩容 DelayQueue：无界阻塞队列，用于放置实现了 Delayed 接口的对象，其中的对象只有在其到期时才能从队列中取走，常见使用如关闭空闲连接，缓存等 LinkedBlockingQueue：链阻塞队列，不定义长度时为 Integer.MAX_VALUE SynchronousQueue：内部只能容纳单个元素 PriorityBlockingQueue：无界的优先阻塞队列 LinkedBlockingDeque：双向链阻塞队列 FutureTask： 简介：为 Future 提供了基础实现，如异步获取任务的执行结果和取消任务等，使用 CAS 确保线程安全 Future 接口：通过该接口可以查看任务是否执行完成，获取执行结果或者取消执行 状态转换： 核心方法： run：如果任务状态是 NEW，则利用 CAS 修改线程为当前线程，执行完毕调用 set(result) 设置执行结果 get：如果任务完成，直接返回结果，否则的话则进行等待，见当前线程加入 waiters 节点中 cancel：尝试取消当前任务，如果任务已经完成或者取消，操作失败 使用： Future + ExecutorService FutureTask + ExecutorService FutureTask + Thread ThreadPoolExecutor： 简介：线程池能够对线程统一分配，调优和监控，能够提高线程的可管理性。本身实现上就是一个线程集合 workerSet 和一个阻塞队列 workQueue，workerSet 里面的线程在空闲时不断从 workQueue 里面获取任务执行，没有任务时则会阻塞 原理：构造方法如下 123456public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) corePoolSize：每次提交一个任务时，都会创建一个线程用来处理该任务，直到线程数为 corePoolSize，当达到该值时，此时新任务会被放到阻塞队列中 maximumPoolSize：当阻塞队列满并且有新任务时，则创建新线程执行任务，直到达到 maximumPoolSize keepAliveTime：线程空闲时的存活时间，只有在线程数大于 corePoolSize 时有效 workQueue：用来保存等待被执行的任务的阻塞队列，可以是有界或者无界的 handler：自定义的线程池的饱和策略，即当阻塞队列满并且没有空闲线程时，采取的处理策略 AbortPolicy：默认策略，直接抛出异常 CallerRunsPolicy: 用调用者所在的线程来执行任务 DiscardOldestPolicy: 丢弃阻塞队列中靠最前的任务，并执行当前任务 DiscardPolicy: 直接丢弃任务 Executors 提供的三种策略： newFixedThreadPool：饱和策略失效，并且 keepAliveTime 失效 12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; newSingleThreadExecutor：和 newFixedThreadPool 相同，不过只有单个线程 newCachedThreadPool：可能会导致线程数过大 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 关闭线程池： shutdown：不再接收新任务，并且中断所有没有正在执行任务的线程 shutdownNow：不再接受新任务，然后停止所有正在执行或暂停任务的线程 任务执行 execute：当线程数小于 corePoolSize 时，创建新的线程处理任务，否则将其放入到阻塞队列中；如果阻塞队列满，并且线程数小于 maximumPoolSize，则创建新的线程处理任务；如果线程数已经达到 maximumPoolSize，则使用 rejector 来处理 任务提交 submit： 不推荐使用 Executors 去创建线程池的原因：或者队列无限长，或者线程数量无限多，或者饱和策略失效；直接使用 ThreadPoolExecutor 的方式更加明确线程池的运行方式，规避资源耗尽的风险 监控线程池状态： getTaskCount &amp; getCompletedTaskCount getLargestPoolSize &amp; getPoolSize getActiveCount ScheduledThreadPoolExecutor： 简介：继承自 ThreadPoolExecutor，定时或者延时启动任务，其不同点在于： DelayedWorkQueue：使用无界延迟队列存储任务，保证了任务只有可以执行的时候，wokrer 才能从延迟队列中取到对应的任务来执行。它只能存储 RunnableScheduledFuture 对象，并且自己实现了二叉堆用于排序 ScheduledFutureTask：继承 FutureTask，并且实现了 Delayed 接口，表示延迟执行的任务 支持可选的 run-after-shutdown 参数，在池被关闭之后来决定是否执行周期或延迟任务 ScheduledFutureTask： run 方法：先检查是否已经到达可执行时间，然后检查是否是周期任务，如果不是，直接执行，否则将任务再次添加到队列中，并且重新设置任务的可执行时间点 run-afetr-shutdown 参数： continueExistingPeriodicTasksAfterShutdown executeExistingDelayedTasksAfterShutdown 构造函数： 123456public ScheduledThreadPoolExecutor(int corePoolSize, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue(), threadFactory, handler);&#125; 核心方法： scheduleAtFixedRate：第一次开始执行后，等待 delay 延迟执行第二次任务 scheduleWithFixedDelay：第一次执行完成后，等待 delay 延迟执行第二次任务 ThreadPoolExecutor 饱和策略不适用于 ScheduledThreadPoolExecutor 的原因：后者使用的是无界延迟队列，maximumPoolSize 不生效，因此饱和策略也不生效 Executors 提供的方法; newScheduledThreadPool：可指定核心线程数的线程池 newSingleThreadScheduledExecutor：只有一个工作线程的线程池，如果出现异常而导致线程终止，则创建新的线程来代替 ForkJoin 框架： 功能：可以将大任务划分为小任务来异步执行的工具 核心思想： 分治思想 工作窃取（work-stealing）算法：工作线程优先处理来自自身队列的任务，然后以FIFO的顺序随机窃取其他队列中的任务 三个模块： 任务对象：ForkJoinTask，用户定义的任务可以继承以下三类 RecursiveTask：有返回值的，可递归执行的任务 RecursiveAction：无返回值的，可递归执行的任务 CountedCompleter：任务完成执行后会触发执行一个自定义的钩子函数 执行 Fork&#x2F;Join 任务的线程: ForkJoinWorkerThread 线程池: ForkJoinPool 执行流程： 直接通过 FJP 提交的外部事务（external&#x2F;submissions task），存放在 workQueue 偶数槽位 invoke：会等待任务计算完毕并返回计算结果 execute：直接向池中提交来异步执行，无返回结果 submit：也是异步执行，但是会返回一个 Future 对象，在适当时候通过 get 得到结果 通过内部 fork 分割的子任务(Worker task)，存放在 workQueues 的奇数槽位 Fork&#x2F;Join 的注意事项： 避免不必要的 fork：划分成两个任务后，不要都 fork，然后 join，这样会造成性能下降，可以一个任务 fork，另一个任务直接 coompute，也就是复用工作线程，不过需要注意 fork &amp; compute &amp; join 顺序 选择合适的子任务粒度：官方文档给出的任务应该执行 100-10000 个基本步骤，并且需要 JNI 预热 创建方法： Executors.newWorkStealingPool ForkJoinPool.commonPool CountDownLatch： 简介：典型用法是将任务划分为 n 个独立的任务，并创建值为 n 的 CountDownLatch，通过 countDown 和 await 方法实现同步，底层通过 AQS 支持 核心函数： countDown：此函数递减锁存器的计数，如果计数值达到 0，则释放所有的等待线程 await：锁存器计数值为 0 时，立即返回，否则将线程加入 sync queue 中等待 CyclicBarrier： 简介：用于同步一组线程，只有所有线程达到屏障时，屏障才会被打开，线程才能继续任务，底层通过 AQS 和 ReentrantLock 支持 构造函数：CyclicBarrier(int parties, Runnable barrierAction)，第二个参数用于指定所有线程都进入屏障后的执行动作，该动作由最后一个进入屏障的线程执行 核心方法： await：表示线程已经达到屏障处，对应的 count 数减 1，若所有线程都到达，则 unpark 所有线程，并且恢复计数，否则线程 park 和 CountDownLatch 对比： CountDownLatch 是一次性的，CyclicBarrier 可以重用 CountDownLatch 下一步动作实施者是主线程，CyclicBarrier 下一步动作实施者还是其他线程本身 Semaphore： 简介：基于 AQS 实现，允许 n 个任务同时访问某个资源，可以将其看作是向外分发资源的许可证 内部类：Sync 继承自 AQS，NonfailSync 和 FailSync 继承自 Sync，用于控制是否是公平分发 核心方法： acquire：用于信号量中获取一个或多个许可，在提供一个许可前一直将线程阻塞，或者线程被中断 release：此方法释放一个或多个许可，将其返回给信号量 注意： 不可重入性：一个线程 n 次 acquire 信号量，信号量就需要分发 n 个许可给该进程，若不够，则阻塞该进程 不限制最大许可大小：只要有线程 release 了许可，就会将其添加到信号量中，即使超过初始许可数 Phaser： 简介：可以实现 CyclicBarrier 和 CountDownLatch 类似的功能，而且它支持对任务的动态调整，并支持分层结构来达到更高的吞吐量 运行机制： 注册机制：和其他 barrier 不同，在 Phaser 上注册的 parties 是可以动态改变的，既可以随时注册，也可以在抵达点取消注册，只会影响内部的 count 同步机制：既可以阻塞式等待也可以非阻塞式到达，每次所有任务达到同步点时，内部 phase 自增 终止机制：使用 isTerminated 来检查 phaser 的终止状态 分层机制：如果单个 Phaser 用来处理成千上万的任务，可能会造成因为竞争同步造成性能消耗，可以设置分层，相同层间的不同 phaser 是不存在竞争冲突的，父节点 phaser 会监控其所有孩子节点 phaser 的状态用于判断其是否可以 advance 状态监控：获取 parties 的数目，获取已经到达的 parties 数目 核心方法： register：为 phaser 添加一个新的 party，如果当前正在运行 onAdvance，那么就会等待它运行结束再返回结果 arrive：是当前线程达到 phase，不等待其他任务达到就返回 arriveAndAwaitAdvance：使当前线程到达 phaser 并等待其他任务到达 Exchanger： 简介：用于两个线程之间通过调用 exchange 进行数据交换 实现机制：当 A 线程调用 exchage 方法时，其会将数据放到 slot 中并且等待 B 线程调用 exchange；等 B 线程调用 exchange 时，此时先取 slot 中的值，并且将自己的数据放到 slot 中，最后 A 线程被唤醒，取到 slot 中的数据，就完成数据交换 核心属性和方法： arena 数组槽：slot 为单个槽，arena 为数组槽，当多个参与者使用同一个 Exchanger 时，会存在严重的伸缩性问题，通过该数组来安排不同的线程使用不同的 slot 来降低竞争问题，并且保证了最终一定会成对交换数据 exchange：等待另一个线程到达此交换点(除非当前线程被中断)，然后将给定的对象传送给该线程，并接收该线程的对象 和 SynchronousQueue 对比：Exchanger 是线程间安全交换数据的机制，交换了两个数据，而 SynchronousQueue 一次只交换一个数据 ThreadLocal： 简介：将在多线程中为每一个线程创建单独的变量副本的类，当使用 ThreadLocal 来维护变量时, ThreadLocal 会为每个线程创建单独的变量副本 原理：主要是利用了 Thread 对象中的 ThreadLocalMap 类型的 map 变量，其保存有 ThreadLocal 变量，第一次不存在的时候，其会调用 ThreadLocal 里面的 initialValue 创建一个初始值 使用场景：由于 SimpleDateFormat 并不是线程安全的，可以使用 ThreadLocal 来为每个线程创建一个 SimpleDateFormat 实例 12 Java 8 新增特性编程方式： 面向对象编程：对数据进行抽象，Java 原生支持 面向函数编程：对行为进行抽象，Java 引入lambda 表达式，流 Stream 和改进集合类的 API 支持 lambda 表达式： 使用场景： 预定义使用了 @Functional 注释的函数式接口 单个抽象方法类型（SAM） 方法引用：可以在 lambda 表达式内使用，使用 :: 连接对象和方法即可，但是此时不能修改参数 变量捕获：可以使用静态，非静态和局部变量。需要注意，lambda 内部不能修改外部的局部变量，外部的局部变量定义在栈上，JVM 规定了栈上的数据是不需要同步操作的，修改后将产生同步问题 JVM 实现：lambda 方法在编译器内部被翻译成私有静态方法，通过 invokedynamic 指令进行调用 内置四大函数接口： 消费型接口：Consumer&lt;T&gt; void accept(T t) 供给型接口：Supplier &lt;T&gt; T get() 断定型接口：Predicate&lt;T&gt; boolean test(T t) 函数型接口：Function&lt;T,R&gt; R apply(T t) 流：更高的抽象层次上对集合数据进行操作 分类：Stream 和 parallelStream 常用方法：filter，sorted，forEach，map，reduce，distinct，limit，count，min，max，collect，summaryStatistics Optional 类： 功能：可以为 null 的对象容器，提高 null 的安全性 主要方法： of，ofNullable isPresent，get ifPresent，orElse，orElseGet，orElseThrow map，flatMap filter 性能：可能没有传统方法 obj != null 高性能，但是大多数情况下不会有问题，可用在流处理上 默认方法： 简介：接口可以有实现方法，而且不需要实现类去实现该方法，使用关键字 default 即可 作用：为了兼容，之前的接口不支持默认方法的实现，如果想要修改接口，就需要修改所有的实现类，但是默认方法就可以实现扩展接口并且不修改对应的实现类，实现了向下兼容 多重实现的冲突：同一个方法可以在不同接口引入，会有冲突产生，判定规则： 一个声明在类里面的方法优先于任何默认方法，可以使用 interfaceName.super.methodName() 否则，则会优先选取路径最短的，若路径最短的有多个，则报错 类型注解： 简介：Java 8 之前注解只能用在类，方法，属性上，而在 Java 8 中，注解可以用在更大的范围上。类型注解只是语法而不是语义，并不会影响 java 的编译时间，加载时间，以及运行时间，也就是说，编译成 class 文件的时候并不包含类型注解 作用：用来支持在 Java 程序中做强类型检查，如 @NonNull，配合插件式的 check framework，可以检测出一些 runtime error，从而提高代码质量 向下兼容：在 Java 8 版本下，将类型注解使用块注释注释掉 重复注解： 简介：允许在同一声明类型中多次使用同一个注解 实现： Java 8 之前：由 A 注解保存重复注解 B，使用 A 即可 Java 8：创建重复注解 B 时，使用 @Repeatable 指向 A 注解的 Class 对象，使用时多次使用 B 即可，可读性更强 类型推断优化：支持菱形语法 JRE 精简： 优点：需要更少的计算资源，启动时间增加，消除未使用的代码安全性提升，适应物联网 划分：compact1 &lt; compact2 &lt; compact3 &lt; Full JRE 移除永久代（PermGen）： 简介：在 Java 7 中，永久代主要用来存储一些 Class 和 Meta 的信息，在 Java 8 中被移除，取而代之的是元空间，JVM 内存结构如图： Java8 元空间： 大部分类元数据都在本地内存中分配 通过 MaxMetaspaceSize 限制元空间大小 只有达到 MaxMetaspaceSize 阈值才会进行垃圾回收 一些杂项数据移动到 Java 堆空间中，堆空间容量可能会有所增加 StampedLock： 同步实现方法： synchronized：重量级锁，在 Java6 中有所优化，通过 JVM 实现，代码异常时能够自动释放锁 Lock：接口，核心方法是 lock，unlock，trylock，实现类基本通过 AQS 来实现，相较于 synchronized，提供了定时锁等候和中断锁等候，unlock 必须放在 finally 语句里面 StampedLock：支持乐观读、悲观读锁和写锁，支持多个线程申请乐观读的同时，还允许一个线程申请写锁，底层并不是通过 AQS 来实现的，而是通过 state 和一个队列实现，不支持重入 优点：相较于 Lock，其在读多写少的场景下性能提升较大 LocalDate&#x2F;LocalDateTime： Java 中时间类演化： Java 1.0：Date 既要承担日期信息，又要做日期之间的转换，还要做不同格式日期的显示 Java 1.1：日期时间类分开，Date + Calendar + DateFormat，但是存在以下问题： year 和 month 是从 1900 开始偏移的，month 是从 0 开始偏移的 Date 和 Calendar 所有属性是可变的 SimpleDateTimeFormat 非线程安全 Java 1.8：引入 java.time 包，里面的类是不可变的并且是线程安全的，其中的类： Instant：时间戳 LocalDate：日期 LocalTime：时间 LocalDateTime：包含日期和时间 ZonedDateTime：包含时区的完整的日期信息 面试合集 Java 中应该使用什么数据类型来代表价格？ 如果不是特别关心内存和性能的话，使用 BigDecimal，否则使用预定义精度的 double 类型 怎么将 byte 转换为 String？ 使用 new String(byte[] bytes) 创建，注意需要使用正确的编码 Java 中怎样将 bytes 转换为 long 类型？ 先将其转换为 String，再使用 Long.parseLong 存在两个类，B 继承 A，C 继承 B，我们能将 B 转换为 C 么? 如 C &#x3D; (C) B； 可以，向下转型，不安全，容易出现转型异常 哪个类包含 clone 方法? 是 Cloneable 还是 Object？ Object，Cloneable 只是一个标识性接口，不包含任何方法 a &#x3D; a + b 与 a +&#x3D; b 的区别？ +&#x3D; 隐式的将加操作的结果类型强制转换为持有结果的类型，如果两个整型（byte，short，int）相加，首先会将其提升到 int 类型，再执行加法操作 int 和 Integer 哪个会占用更多的内存？ Integer，其需要额外存储对象的元数据 我们能在 Switch 中使用 String 吗？ 从 Java 7 开始可以，但实际上是语法糖 我们可以在 hashcode() 中使用随机数字吗？ 不可以，相同对象的哈希值必须相同 Java 中，Comparator 与 Comparable 有什么不同？ Comparable 接口用于定义对象的自然顺序，而 comparator 通常用于定义用户定制的顺序。Comparable 总是只有一个，但是可以有多个 comparator 来定义对象的顺序。 final、finalize 和 finally 的不同之处？ final 是一个修饰符，可以修饰变量、方法和类。如果 final 修饰变量，意味着该变量的值在初始化后不能被改变 Java 技术允许使用 finalize() 方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。这个方法是由垃圾收集器在确定这个对象没有被引用时对这个对象调用的，但是什么时候调用 finalize 没有保证 finally 是一个关键字，与 try 和 catch 一起用于异常的处理。finally 块一定会被执行，无论在 try 块中是否有发生异常 Java 中，Serializable 与 Externalizable 的区别？ Serializable 接口是一个序列化 Java 类的接口，以便于它们可以在网络上传输或者可以将它们的状态保存在磁盘上，是 JVM 内嵌的默认序列化方式，成本高、脆弱而且不安全。Externalizable 允许你控制整个序列化过程，指定特定的二进制格式，增加安全机制。 异常关键字：throw、throws、try…catch、finally？ throws 用在方法签名上，方法内部通过 throw 抛出异常，try 用于检测包住的语句块, 若有异常, catch子句捕获并执行catch块 finally 执行时机？ 当 try 和 catch 中有 return 时，finally 仍然会执行，finally 比 return 先执行，finally是在 return 后面的表达式运算后执行的（此时并没有返回运算后的值，而是先把要返回的值保存起来，管finally中的代码怎么样，返回的值都不会改变，仍然是之前保存的值），所以函数返回值是在 finally 执行前确定的，通常，finally 里面不要包含 return，否则程序会提前退出 如何创建内部类和静态内部类对象？ 内部类：new OuterClass.new InnerClass；静态内部类：new OuterClass.StaticInnerClass 不需要序列化的字段？ 声明为static和transient类型的数据不能被序列化， 反序列化需要一个无参构造函数 局部变量为什么要初始化？ 局部变量分布在栈上，生命周期短，JVM 并不会主动初始化而降低自己的性能，因此，需要程序员在使用变量前给变量赋值","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.zsstrike.tech/tags/Java/"}]},{"title":"《REDIS》备忘录","slug":"《REDIS》备忘录","date":"2022-03-08T02:04:29.000Z","updated":"2022-12-06T05:25:37.333Z","comments":true,"path":"2022/03/08/《REDIS》备忘录/","link":"","permalink":"http://blog.zsstrike.tech/2022/03/08/%E3%80%8AREDIS%E3%80%8B%E5%A4%87%E5%BF%98%E5%BD%95/","excerpt":"本文用于记录 Redis 相关知识，以备查阅。","text":"本文用于记录 Redis 相关知识，以备查阅。 01 Redis 数据结构Redis 数据类型是其值的数据类型，这些数据类型会使用相应的数据结构来实现，对应关系如下： Redis 本身使用了哈希表来保存所有的数据，哈希桶存放的是键值对的指针，指针的类型都通过对象结构来解码，里面包含了 type，encoding，ptr 等信息，整个映射过程如下： SDS：保存为 (len, alloc, flags, buf[]) ，其有以下优点：常量时间复杂度内获取字符串长度，二进制安全，不会发生缓冲区溢出，节省内存空间，另外为了节省空间，使用了 __attribute__ ((packed)) 双向链表：实现为具有头节点的双向链表，能够快速获取到头尾节点和链表长度等信息，但是存在无法很好利用 CPU 缓存和在数据较小时，内存开销较大的问题 压缩列表：内存紧凑型的数据结构，占用一块连续的内存空间，不仅可以利用 CPU 缓存，而且会针对不同长度的数据，进行相应编码，节省内存空间。但是其不能保存过多的元素，并且新增或者修改元素时，可能引发连锁更新的问题（通过 prevlen 长度改变造成） 哈希表：内部保存有两张哈希表，采用链式哈希来解决哈希冲突，当 Redis 中的数据过大时，此时就会进行 rehash，其采用的是渐进式 rehash，即在 rehash 进行期间，每次哈希表元素进行查找或者删除操作时，Redis 除了会执行对应的操作之外，还会顺序将哈希表 1 中 rehashidx 位置上对应的桶中的 key-value 迁移到哈希表 2 上，rehash 触发条件和负载因子有关 整数集合：包含有 (enconding, length, contents) 信息，当 Set 对象只有整数，并且元素数量不大时，就会采用该数据结构，当新的数据远大于已存在的数据时，会执行升级操作，升级后不会降级 跳表：在 Redis 中只有 Zset 对象底层同时使用了两个数据结构，一个是哈希表，一个就是跳表。跳表实际上是多层的有序链表，通常越高层的跨度越大，跨度还有一个作用，就是计算该节点在跳表中的排位，Redis 实现中，为跳表增加了一个表头，其中有头尾节点，表长度和最大层数等信息，在新增节点的时候，会随机生成每个节点的层数，每层高度晋升机会是 0.25，得到的跳跃表更加扁平 quicklist：3.2 版本后，List 对象的底层数据结构。其实际上就是双向链表和压缩列表的组合，也就是每个链表节点数据元素是压缩列表，为了解决连锁更新的问题，其会控制每个链表节点中的压缩列表的大小或者元素个数（quicklist.fill 属性），也就是降低连锁更新带来的影响 listpack：quicklist 并没有完全解决连锁更新的影响，因为其还是用压缩列表来保存元素。listpack 的目的便是取代压缩列表，其最大不同就是每个内部节点不再包含前一个节点的长度，从而避免了连锁更新 02 Redis 数据类型Redis 对象中保存了 type 和 encoding 信息，前者表示对象的类型，后者表示使用的编码，通过 void* ptr 指向对应的真实数据类型，对象类型有： String 类型： 编码：int，raw，embstr embstr 表示 redisObject 和 SDS 使用连续的内存空间，适用于字符串较短情况 raw 则将 SDS 和 redisObject 分离存储，使用内部 ptr 指向 SDS 对象 常用指令：SET&#x2F;MSET，GET&#x2F;MGET，EXISTS，DEL，INCR&#x2F;INCRBY，EXPIRE&#x2F;TTL，SETNX 应用场景： 缓存对象：缓存对象的 JSON 格式，或将 key 进行组合进行存储，如 user:&lt;id&gt;:name 常规计数：通过 INCR&#x2F;INCRBY 分布式锁：SET lock_key unique_value NX PX 10000 ，unique_value 表示某个客户端独占，设置过期时间方式客户端崩溃而不能及时释放资源，解锁可以通过 DEL 命令实现，但是由于需要先判断该锁是否是自己的锁，为了保证原子性，需要使用 LUA 脚本 List 类型： 编码：ziplist，linkedlist，quicklist（3.2 版本） 常用指令：LPUSH&#x2F;RPUSH，LPOP&#x2F;RPOP，LRANGE，BLPUSH&#x2F;BRPOP 应用场景： 消息队列：消息队列三要素： 消息保序：LPUSH + RPOP，LPUSH + BRPOP 重复消息处理：生产者自己实现全局唯一 ID 消息可靠性：使用 BRPOPLPUSH Hash 类型： 编码：ziplist，hash，listpack（7.0 版本） 常用命令：HSET&#x2F;HGET，HDEL，HLEN，HGETALL，HINCRBY 应用场景： 缓存对象：field + value 表示对应属性和值，key 则表示对象 id，不过通常使用 String + JSON 方式存储对象，如果对象某些属性变化频繁，可以使用 Hash 类型 购物车：以用户 id 为 key，商品 id 为 field，商品数量为 value，构成购物车的 3 个要素 Set 类型： 编码：intset，hash 常用命令：SADD，SREM，SMEMBERS，SCARD，SISMEMBER，SINTER，SUNION，SDIFF 应用场景： 点赞：按照文章 id 为 key，用户 id 为 value 共同关注：主要使用交集运算 SINTER，注意集合运算复杂度较高 抽奖活动：SRANDMEMBER，SPOP key count Zset 类型： 编码：ziplist，skiplist 和 hash，listpack（7.0 版本） 常用操作：ZADD，ZREM，ZSCORE，ZCARD，ZINCREBY，ZRANGE，ZRANGEBYSCORE，ZRANGEBYLEX，ZUNIONSTORE，ZINTERSTORE 应用场景： 排行榜，电话姓名排序 BitMap 类型： 实现：使用 String 类型实现，保存为二进制的字节数组 常用命令：SETBIT，GETBIT，BITCOUNT，BITOP，BITPOS 应用场景：签到统计，判断用户登陆状态，连续签到用户总数 HyperLogLog 类型： 目的：提供不精确的去重计数，误差率大约是 0.81% 常用命令：PFADD，PFCOUNT，PFMERGE 应用场景：百万级网页 UV 计数 GEO 类型： 目的：存储地理信息，并且对存储的信息操作，用于位置信息服务 实现：通过 GeoHash 编码将经纬度转换为 Zset 中元素的权重分数，关键机制在于对二维地图做区间划分和对区间进行编码 常用命令：GEOADD，GEOPOS，GEODIST，GEORADIUS 应用场景：嘀嘀打车 Stream 类型： 目的：专门为消息队列设计的数据类型，支持自动生成全局唯一 ID，并且以消费组消费数据 常用命令：XADD，XREAD，XREADGROUP，XPENDING&#x2F;XACK 消息队列： 使用 XADD 会生成全局唯一 ID，如 1654254953808-0，通过 XREAD 实现消息读取，XREADGROUP 可以实现负载平衡 Stream 会使用 PENDING list 留存消费组里每个消费者读取的消息，直到收到对应的 XACK；消费者可以在重启后，使用 XPENDING 命令查看已经读取，但尚未确认的消息 问题： Redis 队列中间件（Stream）存在数据丢失的问题，主要原因在于 AOF 先执行命令，再写文件，其次主从复制再进行主从切换时，也存在数据丢失的问题 面对消息积压，内存资源紧张 03 AOF 日志AOF 日志：会保存写操作的命令到日志中，需要手动开启，里面保存的就是一条条用户的写命令 Redis 先执行写操作，完成写操作后才将该命令记录到 AOF 日志中，好处有： 避免恢复时额外的检查开销 不会阻塞当前写操作命令的执行 但是这样的话存在数据丢失的风险，这和 AOF 日志写回硬盘的时机有关（appendfsync）： 当调用 IO write 操作时，操作系统实际上会将数据放在内核缓冲区中，其会等待到一定的时机将数据写到硬盘上，或者通过用户的 fsync 的显式调用将其数据落盘 AOF 重写机制：为了避免日志文件越来越大，对于设置相同的键，其会先创建一个新文件，然后扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志 AOF 后台重写：AOF 重写过程是通过后台子进程 bgrewriteaof 来完成的，好处有： 子进程在 AOF 重写期间，主进程依旧可以响应用户命令 子进程带有主进程的数据副本，采用写时复制策略，减少内存开销 子进程修改共享内存数据时，直接复制，如果采用线程方式实现，则需要加锁 为了记录重写过程中新到来的指令，存在一个 AOF 重写缓冲区，用于保存重写过程中的用户指令，该 AOF 缓冲区最终将会追加到新的 AOF 文件中，最终原子替换原来的 AOF 日志文件 AOF 缓冲区用于保证即使是在子进程发生故障时仍然能够原来的 AOF 时正确的（按照 appendfsync 策略），AOF 重写缓冲区则保证了子进程在替换原来的 AOF 文件后，上面的文件内容能反映数据库当前的状态 04 RDB 快照RDB 快照记录的是某个时间点的内存数据，采用的是全量备份，提供了两个命令用来生成 RDB 文件，分别是 save 和 bgsave，前者会阻塞主线程，后者则创建了一个子进程专门生成 RDB 文件。 在执行 bgsave 的过程中，由于采用的是子进程处理，并且采用写时复制技术，当有新的命令到来时，此时原来的主进程会复制一块内存用于修改，而子进程读到的还是原来的数据。当系统宕机时，可能会丢失上次快照到现在时刻的数据。 混合持久化：RDB 数据恢复的速度快，但是存在大量数据丢失的问题，AOF 虽然解决了大量数据丢失的问题，但是在文件很大的时候，恢复过程缓慢，在 4.0 以后，可以通过配置 aof-use-rdb-preamble 使用混合持久化功能，其在 AOF 重写过程时，先将 RDB 数据写入 AOF 文件中，然后追加持久化过程中的 AOF 重写缓冲区里面的内容。 05 主从复制主从复制可以避免单点故障，主服务可以进行读写操作，并且将写操作同步给从服务器，从服务器一般只读，并且执行主服务器传过来的写操作。 第一次同步过程：使用 replicaof（slaveof） 可以形成主从关系，采用的是全量复制，并且缓存生成 RDB 文件过程中的写操作到 replication buffer 中，之后将其发送给从服务器。 分摊主服务器的压力：在第一次同步过程中，耗时过程主要在生成 RDB 文件和传输 RDB 文件，如果一个主服务器有很多从服务器，可能就会占用大量网络带宽。可以让其中一个服务器成为经理角色，其会同步写操作到其负责的从服务器上。 命令传播：主从服务器完成同步后，双方维护了一个 TCP 连接，该连接是长连接的，这是为了避免 TCP 连接和断开的性能开销。 增量复制：如果主从服务器网络发生断开，之后又重新连接，此时就会根据 psync &#123;runid&#125; &#123;offset&#125; 来进行增量复制。该实现过程通过环形缓冲区 repl_backlog_buffer 实现，主从服务器分别记录自己的 offset，从而实现增量同步，如果从服务器 offset 对应的数据已经被覆盖，则通过全量复制实现，可以通过 repl_backlog_size 设置该环形缓冲区大小。 repl_backlog_buffer：为了从库断开之后，如何找到主从差异数据而设计的环形缓冲区，避免全量复制带来的性能开销 replication_buffer：从库相当于一个 client，和 client 通信所需的缓冲区，在主从复制中特称为 replication_buffer 相关问题： 主服务器不进行持久化时复制的安全性：若主服务器没有开启持久化，应该将其配置为避免自动重启，否则自动重启后主节点上只有一个空数据集，并且会复制到其它从节点上 为什么全量复制使用 RDB 而不是 AOF：RDB 文件内容相对较小，可以降低网络贷款的消耗，并且其还原数据速度快；另外如果使用 AOF 做全量复制，对文件刷盘策略需要仔细选择，选择不当比较影响性能 无磁盘复制模式：Redis 默认使用磁盘复制，但是其性能收到磁盘速度的限制，无磁盘复制模式即创建一个新进程直接 dump RDB 到 slave 的 socket，不经过硬盘，适用于网络较快的情况 从库的从库：对于主库来说，如果所有的从库直接连接到主库上，会给主库带来网络和磁盘上的消耗 数据过期问题：在主从复制中，从节点不会主动删除数据，而是通过主节点控制从节点来删除过期数据，在 Redis 3.2 后，从节点在读取数据的时候，增加了对数据是否过期的判断（惰性删除） 06 哨兵机制目的：提供主从节点自动进行故障转移的功能 哨兵节点：运行在特殊模式下的 Redis 服务器，组成哨兵集群，功能是监控，选主，通知 监控：通过 ping 命令判断主从节点是否主观下线，一旦检测到主观下线，就会和其他的哨兵节点协商，达到 quorum 值便可确定其为客观下线 不参与 leader 竞选的节点只有一次投票权，防止出现两个哨兵节点同时观测到主观下线，同时竞选成为 leader 的情况 选主： 在已下线主节点的所有从节点中选取一个从节点，将其当作新的主节点 首先过滤掉网络状态不好的从节点，通过 down-after-millseconds 参数 按照优先级，复制进度，ID 号挑选从节点作为新的主节点 哨兵节点给新的主节点发送 SLAVEOF no one 命令，提高 INFO 命令发送频率 INFO 命令用于获取该节点的角色信息，如其对应的从节点列表等 让已下线主节点的所有其他从节点修改复制目标，修改其为新的主节点 哨兵节点给其他从节点发送 SLAVEOF 命令 将新主节点的 IP 地址和信息，通过发布订阅机制通知给客户端 客户端和哨兵建立连接后，就会订阅哨兵提供的频道，主从切换完成后，哨兵会向 +switch-master 频道发布新的主节点的 IP 和端口，用以通知客户端 继续监视原来的旧主节点，但这个旧主节点重新上线时，修改其为从节点 哨兵集群： 哨兵发现：通过命令 sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt; 进行哨兵配置，主节点中存在 __sentinel__:hello 频道，其他烧饼可以通过订阅该频道，用以发现其他哨兵并进行连接 从节点发现：通过 INFO 命令，主节点会返回所有的从节点信息 主从集群脑裂现象： 产生原因：由于网络问题，导致集群节点之间失去联系，主从节点间数据不同步，哨兵重新选举，产生两个主节点，等待网络恢复，旧主节点会降级为从节点，由于其与新节点进行同步复制的时候，会清空自己的缓冲区，导致之前客户端写入的数据被丢失 解决方案：当主节点发现从节点下线或者通信延迟过大时，那么禁止主节点进行写数据，直接把错误返回给客户端，以减少数据丢失，可以通过以下两个参数控制 min-slaves-to-write x，主节点必须要有至少 x 个从节点连接，如果小于这个数，主节点会禁止写数据 min-slaves-max-lag x，主从数据复制和同步的延迟不能超过 x 秒，如果超过，主节点会禁止写数据 07 切片集群模式目的：当 Redis 缓存数据量大到一台服务器无法缓存时，就需要使用切片集群模式，其将数据分布在不同的服务器上，以此降低系统对单节点的依赖，提高读写性能，实际上就是服务器 Sharding 技术 实现：采用哈希槽来处理数据和节点之间的映射关系，一个切片集群共有 16384 个哈希槽，通过 CRC16 和取模实现 key 到哈希槽的映射。哈希槽映射到具体的 Redis 节点： 平均分配：Redis 集群平均分配哈希槽 手动分配：使用 cluster meet 手动建立节点之间的链接，组成集群，然后通过 cluster addslots 命令进行分配 哈希槽和一致性哈希： 哈希槽指的是先将用户数据划分到槽里面，然后将槽划分到不同服务器节点上，可以为不同硬件服务器自定义槽数目大小 一致性哈希则是将服务器节点划分在一个哈希环上，主要目的是减少新增节点导致的重新哈希的问题，容易产生数据倾斜问题，可以引入虚拟节点映射减少该问题 08 缓存雪崩、击穿、穿透Redis 通常用做数据库的缓存中间件，用户先访问 Redis，如果命中直接返回，否则就查询数据库获得数据，并且更新缓存。 缓存雪崩：大量缓存数据在同一时间过期，或者 Redis 故障宕机时，此时大量的请求访问数据库，从而导致数据库压力骤增，严重的会造成数据库宕机，而造成系统崩溃。 对于大量数据在同一时期过期的方案： 均匀设置过期时间 双 key 策略：主 key 会设置过期时间，备 key 不会过期，如果主 key 失效，则返回备 key 数据，存在一定的数据延迟 后台更新缓存：后台线程定期更新缓存 对于 Redis 宕机的情况： 服务熔断或请求限流机制 构建 Redis 缓存高可用集群 缓存击穿：如果缓存中的某个热点数据过期，此时大量的请求访问该热点数据，就无法从缓存中读取，直接访问数据库，数据库很容易就被高并发的请求冲垮。 互斥锁：保证同一时间内只有一个请求来构建缓存，同时设置超时时间防止死锁 永远不过期：后台线程会定时更新某个 key，存在脏数据的问题 缓存预热：业务上线前，提前加载数据到缓存中 缓存穿透：当用户访问的数据，既不在缓存中，也不在数据库中，导致请求在访问缓存时，发现缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据，没办法构建缓存数据，来服务后续的请求，就到导致服务器的压力骤增。一般是业务误操作或者黑客恶意攻击，常见应对方案： 非法请求的限制 缓存空值或者默认值 使用布隆过滤器快速判断数据是否存在 布隆过滤器：使用位图和多个哈希函数快速判断数据是否存在数据库中，查询布隆过滤器说数据存在，并不一定证明数据库中存在这个数据，但是查询到数据不存在，则数据库中一定就不存在这个数据。 09 缓存的读写策略Write Update 策略局限：更新数据时，不论是先更新数据库，再更新缓存，还是先更新缓存，再更新数据库，都会产生不一致现象。 Cache Aside 策略：在更新数据时先更新数据库，再删除缓存中的数据，在读取数据时，发现缓存中没了数据之后，从数据库中读取数据，并更新到缓存中。注意写操作时不能先删除缓存中的数据，再更新数据库，这在写读并发可能带来不一致现象。而在读写并发时，存在极小概率会产生不一致，原因是缓存的写入通常远远快于数据库的写入。 由于 Cache Aside 采用 Write Invalidate 策略，可能会对数据命中率产生影响，可以： 更新数据时同样更新缓存，不过需要加分布式锁 更新数据时同样更新缓存，不过需要加过期时间，保证最终一致性 Cache Aside 策略中如何保证先更新数据库，再删除缓存的操作都能成功： 重试机制：引入消息队列，如果删除缓存的操作未成功，那就重试 订阅 MySQL binlog，再操作缓存：阿里巴巴开源 Canal 中间件的实现方式 Write&#x2F;Read Through 策略：核心原则是用户只与缓存打交道，由缓存和数据库通信，写入或者读取数据。当写操作未命中时，此时可以采用 Write Allocate 或者 No Write Allocate，一般采用 No Write Allocate，因为其具有更高的写入性能。 Write Back 策略：Write Through 在未命中时，需要同步更新数据库，Write Back 写策略则只需标记缓存数据为脏数据之后，直接返回即可，在写未命中时，采用 Write Allocate 方式。读策略在未命中的情况下如果发现被置换的缓存块是脏块，则需要将其写入数据库。该策略不能被应用到我们常用的数据库和缓存的场景中，因为缓存一般是非持久化的，如果缓存机器掉电，脏块数据就会丢失。 10 Redis 分布式锁分布式锁特性：互斥性，安全性，对称性，可靠性 Redis 实现分布式锁： 最简化版本：使用 setnx key value 和 delete key 实现加锁和释放锁 支持过期时间：防止获取了锁的服务挂掉而没有释放锁资源，使用 set key value nx ex seconds 加上 owner：防止其他服务删除该服务已获取到的锁，可以设置不同的 value 表示不同的 owner 整合原子操作：加上 owner 后需要先检查，如果是自己的锁再释放，该过程不是原子性的，使用 Lua 可靠性保证： 容灾考虑： 主从容灾：Redis 的哨兵模式可以灵活切换，但是存在同步时延的问题 多机部署：使用 Redis 中的 RedLock，只有超过半数同意才能算请求成功 可靠性深究：由于分布式系统的三大困境（NPC），没有完全可靠的分布式锁，三大困境： 网络延迟：获取到锁后，可能很快过期 进程暂停：发生 GC 导致锁超时，其他进程能够获取该锁，导致多个进程同时获取锁 时钟漂移：机器物理本身的误差 11 Redis 为什么这么快Redis 处理快速的原因： 基于内存实现，没有 IO 开销 高效的数据结构：如 SDS，快表，跳跃表 合理的数据编码：如 String 对象存储数字的时候，采用 int 类型编码，非数字采用 SDS 编码 合适的线程模型：采用 Reactor 单线程模型，省去了上下文切换的开销 12 过期删除策略与内存淘汰策略过期删除策略：对数据库中已经过期的键值对进行删除，存在过期字典用于快速判断 过期时间设置：EXPIRE&#x2F;SETEX，TTL，PERSIST 过期策略： 定时删除：每次设置了 key 的过期时间时，同时创建一个定时事件用于删除 惰性删除：不主动删除过期键，每次访问 key 的时候，才检查 key 是否过期 定期删除策略：每隔一段时间随机从数据库中取出一定数量的 key 进行检查 Redis 过期删除策略： 惰性删除：通过函数 expireIfNeeded 实现 定期删除：每隔一段时间从过期字典中挑选一定的数据检查其是否过期，如果过期率大于 25% 并且此次运行时间小于 25ms，则继续上述操作 内存淘汰策略：当运行内存达到最大值时，需要淘汰某些 key 用于缓存新的键值对 不进行数据淘汰：noeviction，返回错误通知客户端 进行数据淘汰： 在设置了过期时间的数据中进行淘汰： volatile-random volatile-ttl volatile-lru volatile-lfu 在所有数据范围内淘汰： allkeys-random allkeys-lru allkeys-lfu LRU 算法：每个键维护了上一次的访问时间，每次通过随机采样的方式来进行淘汰 LFU 算法：每个键维护了一个该数据的访问次数和上一次衰减时间，其会随着时间衰减 12 Redis 不同版本特性Redis 4.0 新增特性： PSYNC 2.0：可以在更多种的情况下使用增量复制，减少全量复制的开销，如星形复制和链式复制场景 缓存逐出策略优化：新增了 LFU 逐出策略，同时优化其他策略 Lazy Free：添加了 UNLINK 命令，用于异步执行删除操作，防止阻塞主线程，另外可以配置 lazyfree-lazy-server-del yes 改变 DEL 默认行为 交换数据库：新增 SWAPDB 命令 混合持久化：通过 aof-use-rdb-preamble 开启 内存命令：新增 MEMORY 命令，查看 Redis 的内存使用情况 兼容 NAT 和 Docker：新增了配置项用于支持不工作在 host 模式下的 docker 环境中 Redis 5.0 新增特性： Stream 类型：用于消息队列，支持自动生成消息 ID 和消费组消费 集群管理器更改：集群管理功能移植到 redis-cli 里面 Lua 改进：将 Lua 脚本更好地传播到 replicas&#x2F;AOF RDB 格式改进：增加存储 key 的逐出策略，向下兼容 动态 Hz：平衡 CPU 利用率和响应能力 ZPOPMIN 和 ZPOPMAX 命令 CLIENT UNBLOCK 命令，用于终端其他客户端的阻塞命令，如 BRPOP Redis 6.0： 多线程 IO：多线程用于处理网络 IO，用户命令的执行还是在单个线程之中，通过 io-threads-do-reads yes 和 io-threads 控制 SSL 支持 ACL 支持：防止任何用户都能执行 FLUSHDB 这样的危险操作，之前的版本通过 RENAME 方式改变危险操作的名称，现在支持权限控制表了 RESP 3：服务端和客户端通信的协议 客户端缓存：将经常使用的数据 cache 到客户端本地 集群代理：Redis 集群本身是去中心化的，修改某个节点的地址需要同步通知客户端，较为复杂，引入 Proxy，通过代理和集群本身通信，支持自动化路由以及多线程等多种特性 13 Redis 运维监控Redis 自身状态及命令： INFO 命令：查看所有状态信息 MONITOR 命令：监视服务端收到的命令 LATENCY 命令：监控延迟 Redis 可视化监测工具： 可视化工具：redis-stat，RedisLive，redmon 可视化监测工具：基于 redis_exporter，prometheus，grafana 14 Redis 性能调优Redis 变慢的原因以及解决方案： 使用复杂度过高的命令：减少服务端合并操作或者减少值的元素个数大小 操作 bigkey：使用 UNLINK 命令或者开启 lazy-free 集中过期：过期时间增加随机性 内存达到上限：减少存储 bigkey，更换淘汰策略 fork 耗时严重：优化 RDB 备份策略，调大 repl-backlog-size，降低全量同步的概率 开启内存大页：会造成写放大的情况，关闭内存大页 开启 AOF：合理配置 AOF 刷盘时机，配置 no-appendfsync-on-rewrite 使用 SWAP：增加内存 碎片整理：关闭碎片自动整理，合理配置碎片整理参数 网络带宽过载：排查流量大的实例，选用更好的网络设备 频繁短链接：使用长连接连接服务器","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.zsstrike.tech/tags/Redis/"}]},{"title":"《MYSQL》备忘录","slug":"《MYSQL》备忘录","date":"2022-03-02T06:17:22.000Z","updated":"2022-12-07T05:11:03.970Z","comments":true,"path":"2022/03/02/《MYSQL》备忘录/","link":"","permalink":"http://blog.zsstrike.tech/2022/03/02/%E3%80%8AMYSQL%E3%80%8B%E5%A4%87%E5%BF%98%E5%BD%95/","excerpt":"用于整理 MySQL 的相关知识，以备查询。","text":"用于整理 MySQL 的相关知识，以备查询。 01 MySQL 概览存储引擎： InnoDB：支持事务，支持行级锁，支持外键，采用 MVCC 支持高并发，支持四个隔离级别 MyISAM：不支持事务，只支持表锁，支持压缩表和空间数据索引，数据恢复困难 索引数据结构： AVL 树：左右子树树高相差不超过 1 的平衡二叉树，插入和删除会引入旋转操作，适合查找多的场景 红黑树：确保没有一条路径会比其他路径长 2 倍，近似平衡，适合插入删除操作多的场景 跳表：平均性能和红黑树相当，但实现更为简单 B 树：多路查找树，中间节点也会携带实际数据 B+ 树：多路查找树，只有叶子节点才会携带实际数据，查询性能稳定 B+ 树和红黑树比较： B+ 树一个节点可以存储多个元素，相较于红黑树树高更低，磁盘 IO 次数小 磁盘预读功能对 B+ 树产生优化功能 B+ 树和 B 树比较： B+ 树磁盘 IO 更低，其内部节点全部是指针，没有实际数据，能够更快找到元素 B+ 树查询效率更加稳定 B+ 树遍历效率高 MySQL 索引：在存储层实现，不同的存储引擎具有不同的索引类型 B+ 树索引：主索引通常是聚簇索引，辅助索引通常需要回表，覆盖索引，组合索引 哈希索引：只支持点查询，不支持范围查询，同时不支持排序和分组操作 全文索引：用于查找文本中的关键词，通常采用倒排索引实现 空间数据索引（R-Tree）：多维索引，可用于存储地理数据 索引优化： 独立的列：查询条件中，索引列不能是表达式的一部分 多列索引：比单列索引更好 索引列顺序：让选择性强的索引放在前面 前缀索引 覆盖索引：索引包含所有需要查询的字段的值，不需要进行回表操作 查询语句分析：使用 explain 进行分析，其结果参数 select_type：常用的有 SIMPLE 简单查询，UNION 联合查询，SUBQUERY 子查询 possible_keys：可选择的索引 key：实际使用的索引 rows：扫描的行数 type：关联类型，决定通过什么方式找到每一行数据 system：只有一条数据的系统表 const：主键或者唯一索引查询 eq_ref：在进行联接查询的，使用主键或者唯一索引并且只匹配到一行记录的时候 ref：使用非唯一索引 range：索引范围查询 index：跟全表扫描类似，只是扫表是按照索引顺序进行 all：全表扫描，不走索引 查询语句优化： 减少请求的数据量，只返回必要的行数据和列数据 减少服务器扫描的行数，尽量通过覆盖索引 重构查询语句：切分大查询，分解大连接查询，将连接任务交给上层 事务： 特性：ACID 隔离级别：未提交读，提交读，可重复读，可串行化，快照 非一致性问题：脏读，不可重复读，幻读，丢失更新 并发控制： 锁：共享锁，排他锁，意向共享锁，意向排他锁 锁粒度： Record Lock：锁定一个记录对应的索引，而不是本身 Gap Lock：锁定索引之间的间隙，但是不包含索引本身 Next-Key Lock：左开右闭区间，Gap Lock + Record Lock MVCC：在每个记录后面加上两个隐藏的列，记录创建版本号和删除版本号，通过 undo 日志串联版本 快照读：读取历史数据 当前读：读取数据库当前版本数据，可以通过加锁 lock in share mode，for update 实现 分库分表数据切分：水平切分（Sharding）和垂直切分 水平切分： 策略：哈希取模，范围划分，映射表 问题： 事务问题：改为分布式事务 连接：多个单表查询，在用户程序中连接 唯一性：使用全局唯一 GUID MySQL 集群： 主从复制：主要涉及三个线程，binlog 线程，IO 线程和 SQL 线程 读写分离：主服务器处理实时性要求高的读写操作，从服务器处理读操作，可以减轻主服务器的负载 关系数据库设计范式： 1NF：每个属性不可分割，可能存在数据冗余问题 2NF：消除非主属性对码的部分函数依赖 3NF：消除非主属性对于码的传递函数依赖，基本消除了各种异常 BCNF：主属性不依赖于主属性 4NF：消除多值依赖 02 MySQL 索引索引的优缺点： 优势： 可以提高数据检索的速度，降低 IO 成本 索引还能对数据进行排序，降低排序查询语句的执行时间 劣势： 索引本身会占用部分空间 索引虽然提高了查询效率，但是降低了数据更新的效率，因为更新数据的同时需要更新数据 索引类型：主键索引，普通索引（辅助索引），唯一索引，全文索引，空间索引，前缀索引，单列索引，联合索引，聚簇索引 索引的数据结构： Hash 表：单点查询性能好，但是不支持范围查询 平衡二叉树：查询性能良好，但是树高太大，IO 次数多 B 树：改造二叉树，每个节点上有多个数据项，同时对应多个分支，中间节点上可以存储数据 B+ 树：只有叶子节点上存在数据，中间节点只有索引和节点值，进一步降低查找时的 IO 开销 MyISAM 索引：索引和数据分开存储在不同的文件中，叶子节点记录的是磁盘地址 InnoDB 索引：叶子节点存储的数据是整行的数据，称之为聚簇索引，为此，普通索引需要进行回表操作 覆盖索引：在使用辅助索引的时候，需要经过回表操作才能拿到整行数据，可以创建组合索引避免回表 优化： 避免回表：使用覆盖索引 正确使用联合索引，遵循最左匹配原则 03 组合索引的特殊情况假设存在表 t(a, b, c, d)，其中以 a 创建主键索引，以 (b, c, d) 创建组合索引，语句 select * from t where c = 0 执行过程中，有以下问题： 上述条件查询并不满足最左匹配原则，为什么查询的时候使用了索引？ 答：联合索引中有查询需要所有数据项，可以使用覆盖索引，但是其并不满足最左匹配，因此 type 是 index，而不是 ref，同时，组合索引中叶子节点信息量更大，主索引通常还包括了版本信息，事务 id，回流指针等等，因此选择组合索引 为这个表增加 e 字段后，上述查询为什么变为全表扫描？ 答：加入 e 字段后，就不能使用覆盖索引了，此时就只能进行全表扫描 04 慢 SQL 查询语句SQL 语句执行流程： 连接器：服务器进行账户检查，权限检查等操作 缓存层 词法语法分析，检查 SQL 正确性 优化器找到最佳物理执行计划 调用存储引擎的相关接口进行查询，并返回结果 InnoDB 存储引擎： 磁盘预读机制：当引擎访问某个数据项的时候，通常其相邻的数据页也会被加载到内存 索引：InnoDB 主索引采用的是聚簇索引，如果没有唯一且非空的键则会隐式创建一个自增的列 慢 SQL 危害：在高并发情况下，慢 SQL 出现后会阻塞大量正常的请求，造成大面积的超时和失败 慢 SQL 原因： 索引创建方面： 索引区分度低 切忌过多创建索引，其会大幅降低更新操作的效率 常用查询，排序，分组字段建索引 主键和外键建索引 索引失效方面： 对索引使用函数 对索引进行运算 对索引使用 &lt;&gt; 、not in 、not exist、!&#x3D; 对索引进行前导模糊查询 隐式转换会导致不走索引 非索引字段的 or 连接 非最左前缀 预防慢 SQL 方案： 使用连接代替子查询 使用覆盖索引 多表关联查询时，小表在前，大表在后 调整 where 子句中的连接顺序 使用联合索引，而非建立多个单独索引 慢 SQL 分析：打开慢 SQL 日志，设置慢 SQL 执行时间阈值，之后使用 explain 命令查看原因 05 分页场景假设存在表 t_record(id, age, name)，id 上存在主索引，age 上存在辅助索引，下列语句 1select * from t_record where age &gt; 10 offset 10000 limit 10; 在第一次执行的时候很慢，在第二次有了缓存之后，才会变快。 对于 MySQL，上述语句会使用 age 上的索引，首先找到满足 age &gt; 10 的第一个数据，然后向后遍历 10000 项数据，并且对每一项数据，都会进行回表操作，即使我们不需要这些数据。这样的话引入了大量的随机 IO，自然速度变慢。 分页性能问题优化： 产品上绕过，只提供上一页和下一页功能不需要回表，并且没有 offset 1select * from t_record where id &gt; last_id limit 10; 使用覆盖索引：不需要进行额外的回表操作 12select * from t_record where id in(select id from t_record where age &gt; 10 offset 10000 limit 10）; 06 MySQL 事务事务特性：ACID InnoDB 保证事务特性： 持久性是通过 redo log （重做日志）来保证的 原子性是通过 undo log（回滚日志） 来保证的 隔离性是通过 MVCC（多版本并发控制） 和锁机制来保证的 并行事务引发的问题：丢失更新，脏读，不可重复读，幻读 事务隔离性级别：未提交读，提交读，可重复读，串行化，快照隔离 InnoDB 保证事务隔离级别： 未提交读：直接读取最新的数据 串行化：加读写锁实现 读提交和可重复读：通过 Read View 实现，读提交在每次读数据前生成一个 Read View，可重复读则在启动事务时生成一个 Read View，在之后都使用该 Read View MVCC 实现：在每个记录后面还增加了两个隐藏列，trx_id 和 roll_pointer，后者指向旧版本记录 Read View 数据结构： 可重复读：遍历记录版本，直到找到 trx_id 小于等于 creator_trx_id 的记录 读提交：遍历记录版本，直到找到 trx_id 小于 max_trx_id，并且其不在 m_ids 列表中的记录 07 幻读的处理在可重复读隔离级别下，普通的查询语句是快照读，其是不会看到别的事务新的插入的数据的，幻读的现象只能在当前读下产生。 InnoDB 为了防止该问题，采用了 next-key 锁，下面的事务 A 会锁住 (2, +inf) 范围的记录，如果该期间有其他事务在这个锁住的范围插入数据就会被阻塞，从而解决了幻读现象。 next-key 锁锁的是索引，而不是数据本身，如果 update 语句的 where 条件没有使用索引列，那么就会全表扫描，不仅加上行锁，还加上了间隙锁，相当于锁住整个表，直到事务结束时释放。因此，在线上千万不要执行没有带索引条件的 update 语句，不然会造成业务停滞。 08 MySQL 锁类型全局锁： 使用方法：flush tables with read lock 和 unlock tables 加锁和解锁，加锁后整个数据库就只处于只读状态，会阻塞其他线程对表的结构和数据的更改 应用场景：全库逻辑备份，通常在不支持 MVCC 的引擎中使用，而对于像 InnoDB 引擎，可以在 mysqldump 时加上 -single-transaction 就能保证数据一致性 缺点：整个数据库只读状态，可能会造成业务停滞 表级锁： 表锁：lock tables t_name read/write，其会限制本线程和其他线程的读写操作，可以通过 unlock tables 或者退出会话释放表锁 元数据锁（MDL）：不需要显式使用 MDL，其根据如下规则加锁： 对一张表进行 CRUD 操作时，加的是 MDL 读锁 对一张表做结构变更操作的时候，加的是 MDL 写锁 MDL 在事务提交后才会释放，另外，MDL 写锁获取的优先级高于读锁，一旦出现 MDL 写锁等待，会阻塞后续该表的所有 CRUD 操作 意向锁：加锁规则如下： 在对某些记录加上共享锁之前，需要先在表级别加上一个意向共享锁 在对某些纪录加上独占锁之前，需要先在表级别加上一个意向独占锁 注意，普通的 select 是不会加锁的，因为其是利用 MVCC 实现，可以通过 lock in share mode 或者 select ... for update 实现，意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁和独占表锁发生冲突，意向锁的目的是快速判断表里是否有记录被加锁 AUTO-INC 锁：该锁在执行完插入语句后就会释放，而不是提交事务时释放。由于其是表级锁，会影响并发插入的性能，因此，MySQL 5.2 后的版本提供了轻量级的锁，其在申请完自增主键后就立即释放锁资源，这样在并发插入的时候，不能保证自增长的值是连续的，这在主从赋值的场景中是不安全的。可以通过设置 binlog_format &#x3D; row 保证一致性 行级锁：InnoDB 才有，MyISAM 并没有行锁 记录锁：锁住一条记录，作用在索引上，存在 S 锁和 X 锁之分 间隙锁：锁定一个范围，间隙锁是兼容的，两个不同的事务可以持有公共的间隙锁，间隙锁本质上用于阻止其他事务在该间隙内插入新记录，而自身事务允许在该间隙内插入数据 Next-Key Lock：记录锁和间隙锁的组合，用左开右闭区间表示，是加锁的基本单位，但是可能会退化，普通的 select 语句是快照读，只有 select .. in share mode 和 select ... for update 才会加锁 插入意向锁：特殊的间隙锁，不同于间隙锁的是，该锁只用于插入操作，一个事务拥有插入意向锁，另外一个事务便不能拥有对应的间隙锁 09 MySQL 加锁规则next-key 锁退化原则：在能使用记录锁或者间隙锁就能避免幻读现象的场景下， next-key lock 就会退化成记录锁或间隙锁 唯一索引等值查询： 查询的记录存在：退化成记录锁 查询的记录不存在：退化成间隙锁 唯一索引范围查询（包含）：首先找到 min 对应的 next-key lock，如果 min 存在，退化成记录锁；最后找到 max 对应的 next-key lock，如果 max 不存在，退化成间隙锁 非唯一索引等值查询：对非唯一索引加锁时，其会同时对主键索引加记录锁 查询的记录存在：先加 next-key lock， 另外一把锁退化成间隙锁 查询的记录不存在：退化成间隙锁 非唯一索引范围查询：next-key lock 不会退化 没有加索引的查询：将会使用全表扫描的方式，此时整个索引上都会加上 next-key 锁，相当于锁住全表，因此，在线上在执行 update、delete、select … for update 等具有加锁性质的语句，一定要检查语句是否走了索引 分析加锁的命令：select * from performance_schema.data_locks\\G LOCK_TYPE：TABLE，RECORD（行级锁） LOCK_MODE： X：next_key 锁 X，REC_NOT_GAP：记录锁 X，GAP：间隙锁 10 update 语句当执行 update 语句的时候，如果没有带上索引，可能会走全表扫描，从而导致全表加上了独占锁，导致业务停滞，通常可以使用以下方法避免该情况： 开启安全更新模式，sql_safe_update 参数设置为 1，此时需要 update 有 where 或者 limit 子句 如果带上索引，但是优化器选择走全表扫描，可以使用 force index 语句 11 MySQL 索引失效索引失效的情况： 对索引使用左或者左右模糊匹配 对索引使用函数：MySQL 8.0 以后可以使用函数索引 对索引进行表达式计算 对索引隐式类型转换：在遇到字符串和数字比较的时候，会先把字符串转为数字，然后再进行比较 联合索引非最左匹配：(a, b, c) 组合索引，where a = 6 and c = 8，MySQL 5.6 之前版本会回表比较，而之后的版本使用索引下推优化 WHERE 子句中的 OR 12 count 性能count 性能：count(*) &#x3D; count(1) &gt; count(主键字段) &gt; count(字段) count 作用：统计符合查询条件的记录中，函数指定的参数不为 NULL 的记录有多少个 count(主键字段)：优先走二级索引，没有的话走主键索引，InnoDB 只会返回对应的主键字段 count(1)：和 count(主键字段) 类似，但是不会读取记录中的任何字段的值 count(*)：会被优化为 count(0)，执行效率和 count(1) 相同 count(字段)：会走全表扫描，如果其 NOT NULL，那么 Server 层就不需要额外的判断 走聚簇索引和全表扫描的区别：全表扫描还需要额外访问叶子节点的非主键字段，效率更低 MyISAM 引擎和 InnoDB 引擎：执行 count(*) 时，其直接返回表的元信息中 row_count 值，但是由于 InnoDB 支持 MVCC，并不能简单统计当前的行数作为业务的返回值，但当带上条件查询时，两者行为类似 如何优化 count(*)： 使用近似值：如 explain 额外表保存计数值 13 MySQL 死锁避免Insert 语句在正常执行的时候不会生成锁结构的，只有在以下情况才会将隐式锁转换为显式锁： 如果记录之间加有间隙锁，为了避免幻读，此时是不能插入记录的 如果 Insert 的记录和已有记录存在唯一键冲突，此时也不能插入记录 在事务未提交前，将会一直保持有对应的共享的记录锁，对于唯一二级索引，下图会造成阻塞： 多个客户端同时对资源加锁，就可能存在死锁现象，MySQL 中死锁避免方法有： 设置事务等待锁的超时时间：innodb_lock_wait_timeout 开启主动死锁检测：innodb_deadlock_detect 14 MySQL 优化思路MySQL 逻辑架构： 客户端层：用于连接处理，授权认证，安全检测等 核心服务层：查询解析，分析，优化，缓存，跨存储引擎的功能在该层实现，如存储过程，触发器 存储引擎：负责 MySQL 中的数据存储和提取 查询语句执行过程： 客户端发送查询请求 检查缓存是否命中，命中直接返回，否则继续执行，可以设置 query_cache_type 为 DEAMND 解析、预处理、再由优化器生成对应的执行计划 根据执行计划，调用存储引擎的 API 来执行查询 将结果返回给客户端，该过程是一个增量且逐步返回的过程 性能优化： Schema 设计和数据类型优化：通常越小的数据类型越快，占用更小的磁盘 对整数类型指定宽度，没有任何作用 大多数情况下不使用枚举类型，因为添加和删除字符串必须使用 alter table schema 的列不要太多 索引： 注意索引失效的情况 前缀索引：如果列字符串很长，可以使用前缀索引 避免多个范围条件查询 覆盖索引 使用索引来排序 删除长期未使用的索引 特定类型优化： 优化 count 查询：直接使用 count(*)，如果不需要精确值，可以使用 explain 优化关联查询：可以使用子查询的方式来让优化器选择索引 优化 LIMIT 分页：使用覆盖索引，或者统计书签信息 优化 UNION：尽量使用 UNION ALL，否则的话需要做唯一性检查，该过程很耗时 15 Buffer PoolInnoDB 存储引擎提供了 buffer pool，用来提高数据库的读写性能 粒度：其以页（16 KB）为单位，通过参数 innodb_buffer_pool_size 调整缓存空间大小 缓存信息：数据页，索引页，undo 页，插入缓存页，锁信息等（不包括 redo log buffer） InnoDB 通过三种链表管理缓存页： Free List：管理空闲页 Flush List：管理脏页 LRU List：管理脏页和干净页，在内存不足时用于淘汰 InnoDB 中的 LRU 优化： 预读失效：分为 young 区域和 old 区域，加入缓冲区的页首先被放在 old 区域，只有后续真正被访问才会真正放入 young 区域 缓存污染：为进入到 young 区域的页增加了一个停留在 old 区域时间的判断，只有后续访问与第一次访问时间大于某个时间间隔，才会将其移动到 young 区域的头部 脏页刷盘时机： redo log 日志满了，主动触发 Buffer pool 空间不足，逐出脏页，需要刷盘 MySQL 认为空闲时，后向线程定期将适量的脏页刷盘 MySQL 正常关闭之前，所有脏页都需要刷盘 16 MySQL 日志系统在执行一条更新语句的时候，会涉及到如下三种日志： undo log：InnoDB 存储引擎生成的日志，实现了事务的原子性，用于事务回滚和支持 MVCC redo log：InnoDB 存储引擎生成的日志，实现了事务的持久性，主要用故意掉电等故障恢复 binlog：Server 层生成的日志，主要用于数据备份和主从复制 redo log 相关问题： redo log 产生动机？ Buffer Pool 为上层应用提供了缓存的功能，脏页并不会立即写到磁盘上，为了防止掉电产生数据不一致的情形，需要先使用 redo log 记录下事务的操作，然后操作内存数据，最后在合适时间点将脏页数据刷到磁盘上，这样就能保证 crash-safe 修改 undo 页面，需要记录对应的 redo log 吗？ 需要，需要首先记录修改 undo 页面的 redo log，再真正修改 undo 页面 redo log 同样需要写到磁盘，数据也要写到磁盘，为什么多此一举？ redo log 采用的是追加写，磁盘操作是顺序写，而写入数据则需要先找到写入位置，对应磁盘操作是随机写，其将 MySQL 的写操作从磁盘的随机写变为顺序写，提高了系统性能 产生的 redo log 是直接写入磁盘吗？ 不是的，首先将操作记录到 redo log buffer 中，后续根据刷盘策略再将其写入到磁盘中 redo log 什么时候刷盘？ 主要有下面几个时机： MySQL 正常关闭时 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘 InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘 每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘（ innodb_flush_log_at_trx_commit &#x3D; 1） innodb_flush_log_at_trx_commit 参数控制什么？ 当设置该参数为 0 时，表示每次事务提交时 ，还是将 redo log 留在 redo log buffer 中 ，该模式下在事务提交时不会主动触发写入磁盘的操作，此时 MySQL 进程崩溃将损失上一秒内的事务数据 当设置该参数为 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘，这样可以保证 MySQL 异常重启之后数据不会丢失 当设置该参数为 2 时，表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache，Page Cache 是专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存，此时 MySQL 进程崩溃不一定会丢失数据，因为操作系统会在合适时机调用 fsync 进行数据落盘 redo log 文件写满了怎么办？ InnoDB 存储引擎有一个重做日志文件组，包含两个文件： ib_logfile0 和 ib_logfile1 。采用循环写的方式，check_point 表示当前要擦除的位置，write_pos 表示下一个 redo log 日志要写入的位置，当 redo log 文件写满了，也就是 write_pos 追上了 check_point，此时 MySQL 不能执行新的更新操作，其会停下来将 buffer pool 中的脏页刷新到磁盘中，以将 check_point 向后推进，使得 MySQL 恢复正常执行 binglog 相关问题： 为什么有了 binlog， 还要有 redo log？ 最初的 MySQL 使用的是 MyISAM 引擎，其只有 binlog，用于归档，InnoDB 以插件形式引入，引入 redo log 是为了 crash-safe 能力 redo log 和 binlog 有什么区别？ 适用对象不同，redo log 只适用于 InnoDB 引擎，而 binlog 所有引擎适用 文件格式不同 binlog：STATEMENT，ROW，MIXED redo log：物理日志，记录的是某个数据页做了什么修改 写入方式不同：redo log 采用循环写，binlog 采用追加写 用途不同：binlog 用于备份恢复，主从复制，redo log 用于掉电等故障恢复 主从复制如何实现？ MySQL 集群的主从复制过程梳理成 3 个阶段： 写入 Binlog：主库写 binlog 日志，提交事务，并更新本地存储数据 同步 Binlog：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志（relay log）中 回放 Binlog：回放 binlog，并更新存储引擎中的数据 MySQL 主从复制还有哪些模型？ 同步复制：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。这种方式在实际项目中，基本上没法用，原因有两个：一是性能很差，因为要复制到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。 异步复制（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。 半同步复制：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险。 什么时候 binlog cache 会写到 binlog 文件？ 在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache。同样地，MySQL 提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率： sync_binlog &#x3D; 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘； sync_binlog &#x3D; 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync； sync_binlog &#x3D;N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync 两阶段提交问题： 为什么需要两阶段提交？ 事务提交后，redo log 和 bin log 都需要持久化到磁盘，但是可能出现半成功的状态，对于命令 UPDATE t_user SET name = &#39;xiaolin&#39; WHERE id = 1： 如果在将 redo log 刷入到磁盘之后， MySQL 突然宕机了，而 binlog 还没有来得及写入。MySQL 重启后，通过 redo log 能将 Buffer Pool 中 id &#x3D; 1 这行数据的 name 字段恢复到新值 xiaolin，但是 binlog 里面没有记录这条更新语句，在主从架构中，binlog 会被复制到从库，由于 binlog 丢失了这条更新语句，从库的这一行 name 字段是旧值 jay，与主库的值不一致性； 如果在将 binlog 刷入到磁盘之后， MySQL 突然宕机了，而 redo log 还没有来得及写入。由于 redo log 还没写，崩溃恢复以后这个事务无效，所以 id &#x3D; 1 这行数据的 name 字段还是旧值 jay，而 binlog 里面记录了这条更新语句，在主从架构中，binlog 会被复制到从库，从库执行了这条更新语句，那么这一行 name 字段是新值 xiaolin，与主库的值不一致性； 两阶段提交是为了避免出现两份日志之间的逻辑不一致的问题。 两阶段提交的过程是怎样的？ MySQL 使用内部 XA 事务完成两阶段提交，将 redo log 写入拆分成两个步骤：prepare 和 commit，中间再穿插写入 binlog，具体如下： prepare 阶段：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 刷新到硬盘； commit 阶段：把 XID 写入到 binlog，然后将 binlog 刷新到磁盘，接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，同样需要写入到磁盘； 异常重启会出现什么现象？ 在 MySQL 重启后会按顺序扫描 redo log 文件，碰到处于 prepare 状态的 redo log，就拿着 redo log 中的 XID 去 binlog 查看是否存在此 XID： 如果 binlog 中没有当前内部 XA 事务的 XID，说明 redolog 完成刷盘，但是 binlog 还没有刷盘，则回滚事务。对应时刻 A 崩溃恢复的情况。 如果 binlog 中有当前内部 XA 事务的 XID，说明 redolog 和 binlog 都已经完成了刷盘，则提交事务。对应时刻 B 崩溃恢复的情况。可以保证主库和备库的一致性。 可以看到，对于处于 prepare 阶段的 redo log，即可以提交事务，也可以回滚事务，这取决于是否能在 binlog 中查找到与 redo log 相同的 XID，如果有就提交事务，如果没有就回滚事务。 事务没提交的时候，redo log 会被持久化到磁盘吗？ 会的，MySQL 后台线程会每隔一秒将 redo log buffer 持久化到磁盘，因此，redo log 可以在事务没提交之前持久化到磁盘，但是 binlog 必须在事务提交之后，才可以持久化到磁盘。 两阶段提交的问题？ 磁盘 IO 次数高：对于“双1”配置，每个事务提交都会进行两次 fsync（刷盘），一次是 redo log 刷盘，另一次是 binlog 刷盘。 锁竞争激烈：两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在「多事务」的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，还需要加一个锁（prepare_commit_mutex）来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。 binlog 组提交实现方式？ MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I&#x2F;O 的次数。引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程： flush 阶段：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）； sync 阶段：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）； commit 阶段：各个事务按顺序做 InnoDB commit 操作，将 redo log 状态设置为 commit 上面的每个阶段都有一个队列，每个阶段有锁进行保护，锁粒度减小，这样就使得多个阶段可以并发执行，从而提升效率。 有 binlog 组提交，那有 redo log 组提交吗？ MySQL 5.6 没有 redo log 组提交，MySQL 5.7 有 redo log 组提交。 在 MySQL 5.6 的组提交逻辑中，每个事务各自执行 prepare 阶段，也就是各自将 redo log 刷盘，这样就没办法对 redo log 进行组提交。 所以在 MySQL 5.7 版本中，做了个改进，在 prepare 阶段不再让事务各自执行 redo log 刷盘操作，而是推迟到组提交的 flush 阶段，也就是说 prepare 阶段融合在了 flush 阶段。 组提交相关参数？ Binlog_group_commit_sync_delay：控制 flush 阶段后 sync 阶段前的等待时间 Binlog_group_commit_sync_no_delay_count：最大组提交中事务的数目 MySQL 磁盘 IO 很高，优化方法有哪些？ 设置组提交的两个参数：binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，延迟 binlog 刷盘的时机，从而减少 binlog 的刷盘次数 将 sync_binlog 设置为大于 1 的值 将 innodb_flush_log_at_trx_commit 设置为 2 17 MySQL 行记录存储结构MySQL 数据文件：在 InnoDB 引擎中，按照数据库名称创建对应的目录，然后在对应目录下面存储对应的表数据，如，对于 my_test 数据库，在 &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;my_test 目录下，存在以下文件： db.opt：存储当前数据库的默认字符集和字符校验规则 t_order.frm：t_order 的表结构会保存在这个文件 t_order.ibd：t_order 的表数据会保存在这个文件 表空间的文件结构： 行（row）：数据库表中的记录都是按行进行存放的 页（page）：记录是按照行来存储的，但是 InnoDB 引擎读写是按照页为单位的 区（extent）：在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区为单位分配，可以保证链表中相邻的页物理位置上也相邻 段（segment）：一般分为数据段、索引段和回滚段 InnoDB 行格式：Redundant，Compact，Dynamic 和 Compressed Redundant：基本不再使用 Compact：紧凑的行格式 Dynamic 和 Compressed 两个都是紧凑的行格式，它们的行格式都和 Compact 差不多，因为都是基于 Compact 改进一点东西 Compact 格式： 记录的额外信息： 变长字段长度列表：用于记录各个变长字段的实际长度，每个长度值 1 到 2 字节 NULL 值列表：记录各个可为 NULL 列的值是否是 NULL，每列使用 1 bit 即可 记录头信息：delete_mask，next_record，record_type 记录的真实数据 row_id：只有在没有声明主键和唯一约束列，才会存在该列 trx_id：标记哪个事务生成的 roll_pointer：记录上个版本的指针，指向的是额外信息和真实数据之间位置 如何处理溢出：MySQL 支持像 TEXT 和 BLOB 数据，这些数据可能单个数据页放不下，因此需要提供溢出页，用来存储额外数据。Compact 格式的行数据是在真实数据处存放部分数据，再通过溢出页指针找到剩余数据，而 Compressed 和 Dynamic 则直接采用溢出指针的方式存储数据，这样，原来的数据页就能存放更多的行数据了 相关问题： MySQL 的 NULL 值是怎么存放的？ 通过 NULL 值列表进行标记，其长度根据可为 NULL 列数目变化 MySQL 怎么知道 varchar(n) 实际占用数据的大小？ 通过变长字段长度列表存储实际占用大小 varchar(n) 中 n 最大取值为多少？ MySQL 规定除了 TEXT、BLOBs 这种大对象类型之外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过 65535 个字节。如果表只有一个 varchar(n) 字段，允许为 NULL，字符集是 ascii，那么最大值是 65535 - 2 - 1 &#x3D; 65532；如果有多个字段的话，要保证所有字段的长度 + 变长字段字节数列表所占用的字节数 + NULL值列表所占用的字节数 &lt;&#x3D; 65535。注意 n 表示字符数目，而不是字节数目，因此对于 UTF8 格式，n 还需要减少 18 LRU 算法优化传统的 LRU 算法存在以下问题： 预读失效：预先读取下一页数据，但是并没有真正使用，导致热数据被逐出 缓存污染：使用了顺序扫描，这些页数据只会被读取一次，导致之前的热数据被逐出 MySQL 优化：InnoDB 提供的 buffer pool，缓存数据页 预读失效：划分为 young 区域和 old 区域，预读数据被放在 old 区域上 缓存污染：在内存页被访问第二次的时候，还需满足停留在 old 区域中的时间大于 1s Linux 优化：再读磁盘时也存在预读机制，将接下来的块读取到 page cache 中 预读失效：实现两个 LRU 链表，即活跃 LRU 链表和非活跃 LRU 链表 缓存污染：在内存页被访问第二次的时候，才将其升级到活跃 LRU 链表中 19 幻读现象幻读：当同一个查询在不同的时间产生不同的结果集时，事务中就会出现所谓的幻象问题。 InnoDB 默认隔离级别是可重复读，但是很大程度上避免了幻读现象（并不是完全解决了）： 针对快照读，通过 MVCC 方式解决 针对当前读，通过 next-key 锁（记录锁 + 间隙锁）方式解决 在 RR 隔离级别下，幻读被彻底解决了吗？ 没有，只是很大程度上避免了幻读，但是没有完全解决幻读，如 MySQL 面试题 能说下 MyISAM 和 InnoDB 的区别吗？ MyISAM 支持全文索引，压缩表和空间函数等，但是不支持事务和行级锁，通常用于大量查询少量插入的场景，MyISAM 的索引和数据是分开的；InnoDB 采用聚簇索引，其支持事务，外键和行级锁，并且通过 MVCC 来支持高并发。 说下 MySQL 的索引有哪些吧，聚簇和非聚簇索引又是什么？ 索引按照数据结构来说主要分为 B+Tree 和哈希索引，聚簇索引是索引和数据存放在一起。 主键索引自增有什么优点？ 在插入数据的时候，可以直接将数据追加到最后一个数据页里面，不涉及数据在数据页中的移动，如果不使用自增主键，在插入的时候可能会造成页分裂等耗时操作。 空闲连接的查看和解决方式？ 通过 show processlist 查看空闲连接，MySQL 规定了空闲连接的默认最大时长（8 小时），用户也可主动在执行 kill connection +&lt;id&gt; 来终结空闲连接。 那你知道什么是覆盖索引和回表吗？ 覆盖索引指的是通过索引就可以取到所需的数据，如果所需数据不能通过覆盖索引，就需要访问聚簇索引获取相应的数据，该行为称为回表。 锁的类型有哪些呢？ 从粒度上，分为全局锁，表级锁，行锁；从加锁方式上，分为乐观锁和悲观锁；从是否共享，分为共享锁和排他锁。 你能说下事务的基本特性和隔离级别吗？ ACID，四种隔离级别分别消除了四种不一致现象。 那ACID靠什么保证的呢？ 原子性通过 undolog 保证，隔离性通过 MVCC 和锁机制实现，持久性通过 redolog 实现，保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。 那你说说什么是幻读，什么是MVCC？ 幻读是指两次读取中读到了新插入的数据行，MVCC 实际上是为每个版本添加创建时间版本号，过期时间版本号，只有满足一定要求的事务才能读到对应的版本。 那你知道什么是 next-key lock 吗？ next-key lock 是 RR 下才有的锁，结合 MVCC 可以解决幻读的问题，其在一定条件下能够退化成记录锁或间隙锁。 MySQL 记录锁+间隙锁可以防止删除操作而导致的幻读吗？ 可以，在 MySQL 的可重复读隔离级别下，针对「当前读」的查询语句会对索引加记录锁+间隙锁，这样可以避免其他事务执行「增、删、改」时导致幻读的现象。 分析以下问题： Q1: select * from t_table where a &gt; 1 and b &#x3D; 2，联合索引（a, b）哪一个字段用到了联合索引的 B+Tree？ Q2：select * from t_table where a &gt;&#x3D; 1 and b &#x3D; 2，联合索引（a, b）哪一个字段用到了联合索引的 B+Tree？ Q3：SELECT * FROM t_table WHERE a BETWEEN 2 AND 8 AND b &#x3D; 2，联合索引（a, b）哪一个字段用到了联合索引的 B+Tree？ Q4：SELECT * FROM t_user WHERE name like ‘j%’ and age &#x3D; 22，联合索引（name, age）哪一个字段用到了联合索引的 B+Tree？ 答案： Q1 只有 a 字段用到了联合索引进行索引查询，而 b 字段并没有使用到联合索引。 Q2 语句 a 和 b 字段都用到了联合索引进行索引查询（a &#x3D; 1 时使用 b 字段索引）。 Q3 语句 a 和 b 字段都用到了联合索引进行索引查询（a &#x3D; 2 或 a &#x3D; 8 时使用 b 字段索引）。 Q4 语句 a 和 b 字段都用到了联合索引进行索引查询（对 name &#x3D; j 的数据，此时使用 age）。 为什么说 MySQL 单表最好不要超过 2000 万？ MySQL 数据页大小大约是 16KB，分为索引页和数据页，索引页大约有 1000 数目，数据页假设有 16 条数据，对应的三层 B+ 树就是 1000 * 1000 * 16 &#x3D; 1600 万，因此 MySQL 数据超过 2000 万可能会造成 B+ 树层数增加，而导致 IO 开销增大。 你们数据量级多大？分库分表怎么做的？ 一般拆分顺序是先垂直后水平，水平分库一般通过哈希取模实现，为提高性能，可以引入一致性哈希。 那分表后的自增 ID 怎么保证唯一性的呢？ 设定不同步长，分布式 ID 如雪花算法，不使用自增主键而是使用其他唯一数据列作为主键。 分表后非 sharding_key 的查询怎么处理呢？ 对实时性要求不高的话，可以建立宽表；数据量不是很大的话，可以多线程扫表，然后再聚合结果。 说说 mysql 主从同步怎么做的吧？ 主服务器写入 binglog 后，会创建 dump 线程推送其到从服务器，从服务器启动 IO 线程读取推送过来的 binglog，记录到 relay log 中继日志中，从服务器开启一个 sql 线程读取 relay log 并且执行，最后从服务器也记录自己的 binlog。主从同步分为全同步复制和半同步复制，前者需要所有从库执行完才返回客户端，后者至少收到一个从库的 ack 即可。 那主从的延迟怎么解决呢？ 并不能解决，但是能尽量优化，如尽量减少或者不使用长事务。 查询缓存的作用？ 查询的时候先查询缓存，但是在 MySQL 8.0 后，该功能被移除。缓存虽然能够提升数据库的查询性能，但是缓存同时也带来了额外的开销，每次查询后都要做一次缓存操作，失效后还要销毁。建议设置query_cache_type 变量为 DEMAND。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.zsstrike.tech/tags/MySQL/"}]},{"title":"《计算机系统》备忘录","slug":"《计算机系统》备忘录","date":"2022-02-28T01:15:19.000Z","updated":"2022-05-16T07:41:46.223Z","comments":true,"path":"2022/02/28/《计算机系统》备忘录/","link":"","permalink":"http://blog.zsstrike.tech/2022/02/28/%E3%80%8A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E3%80%8B%E5%A4%87%E5%BF%98%E5%BD%95/","excerpt":"计算机系统的相关知识，总结于此，以备查询。","text":"计算机系统的相关知识，总结于此，以备查询。 01 内存管理虚拟内存：单片机程序写完后会将其进行烧录，程序里面的地址都是内存的物理地址，这给同时运行多个程序带来了困难，为此，可以在物理地址上层提供虚拟地址，程序看到的都是虚拟地址，而物理地址对其是透明的。为了管理虚拟地址和物理地址之间的关系，一般有内存分段和内存分页方式。 内存分段：程序由不同的逻辑段构成，如代码段，数据段，栈段等。分段下的虚拟地址表示为段选择子和段内偏移量，地址转换过程如下： 内存分段的缺点：内外部碎片严重，内存交换效率低。 内存分页：分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小，称其为页，映射过程： 分页很好地解决了内外部碎片和内存交换的问题，但是也存在以下问题： 简单的分页方式浪费内存：可以使用多级页表的方式解决 多级页表转换过程需要更多的时间：引入 TLB，快速进行转换 段页式内存：先将程序划分为多个有逻辑意义的段，接着再把每个段划分为多个页，此时地址结构就由段号、段内页号和页内偏移三部分组成，该方案能提高内存的利用率。 Linux 内存管理：主要采用的是页式内存管理，但同时也不可避免地涉及了段机制，这是由于 Intel 处理器发展历史导致的。Linux 系统通过为每个段分配 0-4G 范围下的虚拟空间，来屏蔽段机制的存在。用户空间划分为： 02 进程和线程进程：运行中的程序，称之为进程。通常，现代 CPU 都支持多进程并发执行。进程的状态变迁图如下： 挂起状态指的是进程目前在外存中，并没有占用物理内存。 操作系统中，通过进程控制块来描述和管理进程，PCB 里面包含了进程的相关信息，如标识符，优先级，当前状态，打开的文件列表，CPU 各个寄存器的值等。PCB 通常通过链表的方式进行组织，把具有相同状态的进程链在一起，组成各种队列。不同进程并发执行时，还会发生进程的上下文切换。 线程：是进程当中的一条执行流程，是更小的能独立运行的基本单位，相同进程内的线程之间共享地址空间，但是保存自己的寄存器和栈资源。 线程优缺点：相同进程内线程可以共享代码段等资源，并发执行起来上下文切换更快；当进程内的一个线程崩溃时，会导致其所属的所有线程崩溃，其原因是操作系统检测到异常，会杀掉进程，其他线程也就一起被杀掉了，之所以不只杀掉崩溃的线程，是因为崩溃的线程可能会破坏其他线程中的内存，导致错误。 线程的实现： 用户级线程的模型：用户线程的切换也是由线程库函数来完成的，无需用户态与内核态的切换，所以速度特别快；但是如果一个线程发起了系统调用而阻塞，那进程所包含的用户线程都不能执行。 内核线程：在一个进程当中，如果某个内核线程发起系统调用而被阻塞，并不会影响其他内核线程的运行；线程的创建、终止和切换都是通过系统调用的方式来进行，因此对于系统来说，系统开销比较大。 轻量级进程：内核支持的用户线程，一个进程可有一个或多个 LWP，每个 LWP 是跟内核线程一对一映射的，也就是 LWP 都是由一个内核线程支持。模式图如下： 调度：先来先服务调度算法，最短作业优先调度算法，时间片轮转调度算法，最高优先级调度算法，多级反馈队列调度算法。 03 互斥和同步如果不加同步与互斥操作的话，两个线程对 i++ 执行 100 次，最小的结果是 2，最大的结果是 200。i++ 这段代码称为临界区，它是访问共享资源的代码片段，一定不能给多线程同时执行。 同步操作好比操作 A 应在操作 B 之前执行，而互斥就好比操作 A 和操作 B 不能在同一时刻执行。 互斥和同步的实现方式：锁，信号量。 锁操作：忙等待锁（自旋锁）和无忙等待锁。 信号量：通常表示资源的数量，用整型表示，有两个原子操作：P 操作和 V 操作。将信号量设置为 1 可实现临界区互斥，将信号量设置为 0 可实现线程间的同步。 生产者-消费者问题：任何时刻只能有一个线程操作缓冲区，需要互斥；缓冲区空时，消费者必须等待生产者生成数据；缓冲区满时，生产者必须等待消费者取出数据。说明生产者和消费者需要同步。 哲学家就餐问题：可能会发生死锁的现象，解决方法有： 每次只能一个哲学家就餐，通过 mutex 实现 偶数编号的哲学家先拿左边的叉子后拿右边的叉子，奇数编号的哲学家先拿右边的叉子后拿左边的叉子 一个哲学家只有在两个邻居都没有进餐时，才可以进入进餐状态 读者-写者问题：读者优先方案，写者优先方案，多读者单写者方案（注意写者开始写的时候需要以前的读者全部读取完毕） 04 进程间通信每个进程的用户空间都是独立的，一般而言不能互相访问，但是内核空间是每个进程都共享的，因此，进程之间通信需要通过内核来实现。Linux 中提供的通信方式有： 管道：管道传输数据是单向，先进先出的。对于匿名管道，它的通信范围是存在父子关系的进程，对于命名管道，它可以在不相关的进程间也能相互通信。管道通信的方式效率低下，不适合进程间频繁地交换数据。 消息队列：消息队列是保存在内核中的消息链表，A 进程要给 B 进程发送消息，A 进程把数据放在对应的消息队列后就可以正常返回了，B 进程需要的时候再去读取数据就可以了。但是消息队列方式存在通信不及时和消息大小限制的问题，另外，消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销。 共享内存：不存在内核态和用户态数据拷贝的开销，提高了通信的速度。 信号量：共享内存如果存在多个进程一起写的情况下，可能会产生数据竞争等问题，通过将信号量初始化为 1 或者 0，就可以实现互斥或同步的功能。 信号：对于异常情况下的工作模式，就需要用信号的方式来通知进程。这是进程间通信机制中唯一的异步通信机制，用户进程对信号处理方式：执行默认操作，捕捉信号，忽略信号。 Socket：跨网络与不同主机上的进程之间通信。 05 文件系统Linux 文件系统会为每个文件分配两个数据结构： 索引节点（inode）：用来记录文件的元信息，如文件大小，修改时间、数据在磁盘的位置等，其占用磁盘空间 目录项（dentry）：用来记录文件的名字、索引节点指针以及与其他目录项的层级关联关系，其被换存在内存中 目录和目录项：目录也是一个文件，只不过该文件存储的是目录数据，目录项是内核的数据结构 逻辑块：多个扇区组成，是文件系统读写的最小单位 虚拟文件系统：文件系统多种多样，为了屏蔽不同文件系统，给上层用户一个统一的接口，操作系统加入了虚拟文件系统，其定义了一组所有文件系统都支持的数据结构和标准接口，这样程序员不需要了解文件系统的工作原理，只需要了解 VFS 提供的统一接口即可。 文件的存储方式： 连续空间存放：读写效率高，但是磁盘空间碎片和文件长度不易扩展 非连续空间存放方式： 链表方式：无外部碎片，动态增长方便，但是查找效率低下 索引方式：可以随机访问，易于文件增长，但是索引表会增加对空间的消耗 目录的存储：目录文件的块里面保存的是目录里面一项一项的文件信息，简单的方式就是表，如果里面的文件太多，则会使用哈希方式优化 空闲空间管理：空闲表，空闲链表，空闲位图（Linux 使用） 软链接和硬链接：硬链接是多个目录项中的索引节点指向同一个索引节点，不可用于跨文件系统；软链接相当于重新创建一个文件，这个文件有独立的 inode，但是这个文件的内容是另外一个文件的路径 文件 IO： 缓冲与非缓冲 I&#x2F;O：根据是否利用了标准库缓冲划分，缓冲 IO 可以减少系统调用次数 直接与非直接 I&#x2F;O：根据是否利用操作系统的缓存划分，非直接 IO 需要将用户数据拷贝到内核缓存中 阻塞与非阻塞 I&#x2F;O：阻塞 IO 等待的是内核数据准备好和数据从内核态拷贝到用户态这两个过程，非阻塞 IO 则需要在最后进行同步过程。非阻塞 IO 需要应用程序进行轮询，该实现方式低下，可以使用 IO 多路复用技术，如 select，poll 等，它是通过 I&#x2F;O 事件分发，当内核数据准备好时，再以事件通知应用程序进行操作，多路复用过程： 同步与异步 I&#x2F;O：阻塞 I&#x2F;O、非阻塞 I&#x2F;O，还是基于非阻塞 I&#x2F;O 的多路复用都是同步调用，异步 I&#x2F;O 是内核数据准备好和数据从内核态拷贝到用户态这两个过程都不用等待 06 输入输出设备控制器：CPU 不直接和相应的设备进行交互，而是通过设备控制器来控制对应的设备的。控制器一般存在三类寄存器，分别是状态寄存器，命令寄存器和数据寄存器。另外，设备也可以分为块设备和字符设备，字符设备不可寻址，鼠标是常见的字符设备。CPU 和控制器的寄存器进行通信的方式有：端口IO 和 共享内存。 IO 控制方式：当 CPU 给设备发送了一个指令，让设备控制器去读设备的数据，CPU 通过以下方式感知到读取数据完毕： 轮询控制器的寄存器状态位：浪费 CPU 资源 使用中断：对于频繁读写数据的磁盘，并不友好 使用 DMA：使得设备在 CPU 不参与的情况下，能够自行完成把设备 I&#x2F;O 数据放入到内存 设备驱动程序：虽然设备控制器屏蔽了设备的众多细节，但是不同控制器其寄存器和缓冲区也是不同的，为了屏蔽设备控制器的差异，引入了设备驱动程序。通常，设备驱动程序初始化的时候，要先注册一个该设备的中断处理函数。 通用块层：对于块设备，为了减少不同块设备的差异带来的影响，Linux 通过一个统一的通用块层，来管理不同的块设备。其为文件系统和应用程序，提供访问块设备的标准接口；还会给文件系统和应用程序发来的 I&#x2F;O 请求排队，进行 I&#x2F;O 调度。 存储系统 IO 软件分层： 键盘敲入字母到显示屏显示时，该期间： 当用户输入了键盘字符，键盘控制器就会产生扫描码数据，将其缓存，并发送中断请求 CPU 收到中断请求时，会保存被中断进程的 CPU 上下文，然后调用键盘的中断处理程序 中断处理函数的功能就是将缓存的数据搬运到显示设备的读缓冲区队列中，最终显示设备驱动程序定时将其显示到屏幕上 07 调度算法进程调度：当 CPU 空闲时，操作系统就会选择内存中的某个就绪状态的进程，并为其分配 CPU 资源，调度方式分为抢占式调度和非抢占式调度，常见的调度方式有： 先来先服务调度算法 最短作业优先调度算法 高响应比优先调度算法：优先级计算考虑到了等待时间和要求服务时间 时间片轮转调度算法 最高优先级调度算法 多级反馈队列调度算法：抢占式的，优先级越高的进程分配到的时间片越少 内存页面置换算法：当产生缺页错误并且当前的内存没有空闲页的时候，此时需要选择一个页面替换出去，常见置换算法： 最佳页面置换算法（OPT）：理论存在，作为基准测试 先进先出置换算法（FIFO） 最近最久未使用的置换算法（LRU） Second-Chance LRU：每个页面连续两次被选择逐出时才会真正被逐出 时钟页面置换算法（LOCK）：只有当需要替换的时候，才会移动指针，并且只有当访问位为 0 时淘汰 最不常用置换算法（LFU） 磁盘调度算法：磁盘访问中，耗时成分主要有寻道延迟，旋转延迟和传输延迟。磁盘调度算法则主要为了减少寻道延迟，相关算法有： 先来先服务算法 最短寻道时间优先算法：可能会导致饥饿现象 扫描算法：磁头在一个方向上移动，直到磁头到达该方向上的最后的磁道才调换方向 循环扫描算法：磁头只能朝某个方向移动，返回时直接复位磁头，返回时不处理请求 LOOK 与 C-LOOK 算法：对扫描算法和循环扫描算法的优化，当磁头在移动到最远的请求位置，然后立刻向反方向移动 08 锁的应用场景互斥锁和自旋锁：互斥锁加锁失败后，线程会释放 CPU，给其他线程；自旋锁加锁失败后，线程会忙等待，直到它拿到锁。互斥锁存在两次上下文切换的成本，因此如果被锁住的代码执行时间很短，就不应该用互斥锁，而应该选用自旋锁，否则使用互斥锁。需要注意的是，在单核 CPU 上，需要抢占式的调度器，否则自旋锁将会一直循环等待下去。互斥锁加锁过程： 读写锁：适用于能明确区分读操作和写操作的场景，读写锁在读多写少的场景，能发挥出优势。不管是读优先锁还是写优先锁，都存在饥饿的问题。为此，可以采用公平读写锁，比较简单的一种方式是：用队列把获取锁的线程排队，不管是写线程还是读线程都按照先进先出的原则加锁即可，这样读线程仍然可以并发，也不会出现饥饿的现象。 乐观锁和悲观锁：悲观锁做事比较悲观，它认为多线程同时修改共享资源的概率比较高，于是很容易出现冲突，所以访问共享资源前，先要上锁。乐观锁做事比较乐观，它假定冲突的概率很低，它的工作方式是：先修改完共享资源，再验证这段时间内有没有发生冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作。像在线文档便是采用乐观锁。 09 零拷贝DMA 技术：在没有 DMA 技术之前，数据的读取过程需要 CPU 进行数据的搬运，而在这个过程中，CPU 是不能做其他的事情的，有了 DMA 之后，数据搬运的工作就交给了 DMA 控制器， 其过程如下： 传统的文件传输：如果服务端需要提供文件传输的功能，一个简单的想法是先读取磁盘上的文件，在通过网络协议发送给客户端，整个过程如下： 综上，一共发生了 4 次用户态和内核态的切换，还发生了 4 次数据拷贝，因此，优化方案从这两点出发： mmap + write：mmap 系统调用函数会直接把内核缓冲区里的数据映射到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作 sendfile：其可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态；如果网卡支持 SG-DMA，可以进一步减少拷贝的次数： 使用零拷贝的项目：Kafka 和 Nginx，注意，使用零拷贝技术是不允许进程对文件内容作进一步的加工的，比如压缩数据再发送。 PageCache：文件传输过程中，第一步都是先需要先把磁盘文件数据拷贝内核缓冲区里，这个内核缓冲区实际上是磁盘高速缓存，其用来缓存最近被访问的数据，当空间不足的时候淘汰最久未被访问的缓存。PageCache 的主要优点是缓存最近被访问的数据和预读功能。但是，当传输大文件的时候，可能会造成缓存污染。在高并发的场景下，针对大文件的传输的方式，应该使用异步 I&#x2F;O + 直接 I&#x2F;O 来替代零拷贝技术。 10 存储器的层次结构存储器通常可以分为以下几个级别： 寄存器：最靠近 CPU 的存储单元，一般 4 个字节或者 8 个字节 CPU Cache：使用 SRAM，通常分为三层，L1 Cache + L2 Cache + L3 Cache，L1 Cache 指令和数据是分开的，L3 Cache 通常是多个 CPU 核心共用的 内存：使用 DRAM，现在也存在 NVM SSD 和 HDD 硬盘：SSD 通过 ROW + COL Addr 获取相应数据，HDD 通过磁头找到起始地址 CPU Cache 组织： 11 CPU 缓存CPU Cache 中的数据是从内存中读取过来的，它是以一小块的数据为单位进行读取和写入的，这样的一个一小块的数据，称之为 Cache Line，通常在 Linux 中，Cache Line 一次载入的数据大小是 64 字节。Cache Line 中除了实际的数据外，还存在对应的 tag 和 valid 标记位，对于直接映射方式： 除了直接映射方式外，还存在全相联 Cache 和组相联 Cache 两种方式 提高数据缓存的命中率：二维数组遍历 arr[i, j] 遍历 提高指令缓存的命中率：涉及到分支预测器，如可以先排序再判断；对于多核 CPU，如果一个进程在不同核心来回切换，各个核心的缓存命中率就会受到影响，可以将线程绑定在某一个 CPU 核心上，Linux 上提供了 sched_setaffinity 方法来实现该功能 12 CPU 缓存一致性CPU 缓存写入方式：CPU 缓存写入后，如果不进行其他操作，就会导致缓存和内存中的数据不一致 写直达：把数据同时写入内存和 Cache 中，每次都需要写内存，性能浪费 写回：修改对应的 Cache Line 并且设置 dirty 位 缓存一致性问题：现代 CPU 通常是多核的，由于 L1，L2 都是独占的，某个 CPU 修改数据后，其他的 CPU 可能并不知道该情况，因此造成了数据不一致，解决该问题的方式： 写传播：某个 CPU 更新 Cache 的时候，必须要传播到其他核心的 Cache，实现方式有总线嗅探，该方式会加重总线负担，并且不能保证事务的串行化 事务的串行化：一组 CPU 对数据的操作顺序，必须在其他核心看起来顺序是一样的，实现方式有 MESI MESI 协议：分别是首字母 Modified，Exclusive，Shared 和 Invalidated，下面是协议转换图： 13 伪共享伪共享：假设存在一个双核心的 CPU，这两个 CPU 核心并行运行着两个不同的线程，并且他们分别访问 A 和 B 变量，如下图： 如果这两个核心交替访问且修改 A，B，那么根据缓存一致性协议 MESI，每个核心访问修改前都需要重新从内存中加载最新的 Cache Line 到高速缓存中，造成性能浪费。 避免伪共享的方法：对于多个线程共享的热点数据，应该避免这些数据刚好在同一个 Cache Line 中。在 Linux 结构体中，可以通过宏 __cacheline_aligned_in_smp 来对齐，使得数据不在同一个 Cache Line 中。如果是在应用层，则可以使用手动填充的方式，如在两个热点数据中间定义一些无用数据用来 padding。 14 Linux 内核任务调度Linux 进程和线程：在 Linux 内核中，进程和线程都是使用 task_struct 结构体来保存的，区别在于线程会共享一部分进程已创建的资源，它们统称为任务。 Linux 系统中，主要有以下调度： Deadline 调度器：按照 deadline 进行调度 Realtime 调度器：相同优先级使用 FIFO 或者 RR 调度，高优先级可以抢占 Fair 调度器：普通任务和后台任务的调度策略 完全公平调度（CFS）：普通任务的调度算法，其会根据每个程序的 vruntime 来决定下一次选择运行的程序，即在调度的时候，选择 vruntime 少的任务，vruntime 参数根据已经运行的时间和优先级计算得到。 CPU 任务队列：一个系统运行的任务通常多余 CPU 核数，为此需要排队，为了效率，cfs 采用红黑树实现。在选择任务时，总是先从 dl_rq 里选择任务，然后从 rt_rq 里选择任务，最后从 cfs_rq 里选择任务。 优先级调整： 调整任务的优先级：使用 nice 命令，priority(new) &#x3D; priority(old) + nice 调整任务所在队列优先级：使用 chrt 命令 15 计算机数据表示十进制和二进制转换方法：除 2 取余法，乘 2 取整法 负数表示：反码和补码，采用补码方便计算机运算，并且不存在 0 的多个不同表示方法 带小数的存储：采用科学表示法，现在一般采用 IEEE 制定的标准，即 符号位：1 为负数，0 为正数 指数位：无符号的 8 位数，IEEE 规定单精度浮点的指数取值范围是 -127 ~ +128，需要偏移 127 尾数：1 后面的小数部分 由于计算机存储小数存在误差，因此： 在做浮点数加法的时候，需要注意误差，如 0.1 + 0.2 != 0.3 在做浮点数判断的时候，通常采用 fabs(num - target) &lt; 1e-5 16 Linux 收发网络包过程网络模型： OSI 网络模型：应用层、表示层、会话层、传输层、网络层、数据链路层以及物理层 TCP&#x2F;IP 网络模型：应用层、传输层、网络层和网络接口层 五层网络模型：应用层、传输层、网络层，数据链路层和物理层 Linux 网络数据的封包： 数据包名称：在应用层叫 data，在 TCP 层称为 segment，在 IP 层叫 packet，在数据链路层称为 frame Linux 接收网络包流程： 网卡接收到网络包后，通过 DMA 技术，将网络包放入 Ring Buffer 中，该缓冲区在内核内存中的网卡驱动里 网卡发起硬件中断，从而处理相应的网卡硬件中断响应函数，中断函数处理完需要暂时屏蔽中断，然后唤醒软中断来轮询（poll）处理数据，直到没有新数据时才恢复中断，这样一次中断处理多个网络包，提高了网卡的性能 软中断响应函数会从 Ring Buffer 中拷贝数据到内核 struct sk_buff 缓冲区中，从而可以作为一个网络包交给网络协议栈进行逐层处理 首先去掉帧头和帧尾，交给网络层，网络层如果发现目标 IP 不是本机，则转发其到下一个节点，否则将其交给上层处理 传输层根据四元组找到相应的 Socket，并且将数据拷贝到 Socket 的接收缓冲区 应用程序调用 Socket 接口，从内核的 Socket 接收缓冲区读取新到来的数据到应用层 17 Linux 网络性能监控网络配置信息：ifconfig 和 ip，可以查看网口的连接状态，MTU 大小，网口的 IP 地址，子网掩码等 Socket 信息：netstat 和 ss，接收队列（Recv-Q）和发送队列（Send-Q）比较特殊，在不同的 socket 状态下表示不同的信息： socket 处于 Established 时：Recv-Q 表示 socket 缓冲区中还没有被应用程序读取的字节数，Send-Q 表示 socket 缓冲区中还没有被远端主机确认的字节数 socket 处于 Listen 时：Recv-Q 表示全连接队列的长度，Send-Q 表示全连接队列的最大长度 网络吞吐率和 PPS：使用 sar 命令，查询带宽可以使用 ethtool 命令 连通性和延时：使用 ping，基于 ICMP 协议 18 软中断中断：是系统用来响应硬件设备请求的一种机制，操作系统收到硬件的中断请求，会打断正在执行的进程，然后调用内核中的中断处理程序来响应请求。中断处理程序在响应中断的时候，还可能会临时关闭中断，也就是让系统中其他的中断暂时无法执行，所以中断处理程序要短且快 软中断：Linux 系统为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是上半部和下半部分 上半部用来快速处理中断，一般会暂时关闭中断请求，也就是硬中断 下半部用来延迟处理上半部未完成的，比较耗时的工作，一般以内核线程的方式运行，也就是软中断 每个 CPU 都有一个软中断内核线程，可以通过 top 查看当前软中断的 CPU 使用率 19 操作系统内核内核：应用连接硬件设备的桥梁，屏蔽了硬件的细节，通常具有的功能：进程管理，内存管理，硬件设备管理，提供系统调用。内核具有很高的权限，操作系统通常将内存分为内核空间和用户空间，实现权限控制。 内核架构： 宏内核，包含多个模块，整个内核像一个完整的程序，如 Linux 内核 微内核，有一个最小版本的内核，一些模块和服务则由用户态管理 混合内核，内核中抽象出了微内核的概念，也就是内核中会有一个小型的内核，如 Windows 内核 20 I&#x2F;O 多路复用基本的 Socket 模型：想要实现跨主机通信，我们需要使用 Socket 编程，通常，每个 Socket 都绑定一个 IP 地址和端口，用以区分主机和主机上的程序。在绑定后，可以通过 listen 函数进行监听，客户端此时可通过 connect 函数发起连接，连接过程中，内核实际上会为每个 Socket 维护两个队列，半连接队列和全连接队列。当 TCP 全连接队列不为空后，服务端的 accept 函数，就会从内核中的 TCP 全连接队列里拿出一个已经完成连接的 Socket 返回应用程序，后续数据传输都用这个 Socket。注意监听的 Socket 和真正数据传输的 Socket 是两个不同的 Socket。在 Socket 文件索引节点指向了 Socket 结构，里面存在两个队列，保存的数据结构是 struct sk_buff，通过移动指针，实现包的解包和封包： 上面实现的 Socket 调用流程是最简单的，基本只能实现一对一的通信，为了提高性能，可以： 多进程模型：为每个客户端分配一个进程来处理请求，通过 fork 可以实现进程复制，但是该方式需要父进程管理其子进程，可通过 wait 和 waitpid 实现，但是，进程上下文切换开销比较大 多线程模型：优化进程上下文开销很大的措施，同时采用线程池来减少线程创建和销毁的开销，但是线程池毕竟是有限的，很难支持 C10K 问题 IO 多路复用：为每个请求分配一个进程&#x2F;线程的方式是不合适的，可以让一个进程来维护多个 Socket，也就是 IO 多路复用技术。一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，该方式也称作时分多路复用。常见的多路复用方式有： select：将已经连接的 socket 放到一个文件描述符集合，通过 select 调用将该集合拷贝到内核中，让内核通过遍历来检查可读或者可写的 socket，并且设置相应的标记位，接着将文件描述符集合拷贝到用户态里面，然后用户态还需要再次遍历找到可读写的 socket，然后进行相应处理。select 使用 BitsMap 表示文件描述符集合，其支持的个数是有限的 poll：和 select 基本相同，但是其使用链表式动态数组形式来存储文件描述符集合，突破了个数限制，但是仍然需要两次遍历和复制 epoll：首先，使用红黑树来跟踪进程所有待检测的文件描述符，提高效率；其次，使用事件驱动的机制，内核里维护了一个链表来记录就绪事件，当用户调用 epoll_wait 时，只会返回就绪事件对应的文件描述符。epoll 支持边缘触发和水平触发，边缘触发模式一般和非阻塞 I&#x2F;O 搭配使用，程序会一直执行 I&#x2F;O 操作，直到系统调用（如 read 和 write）返回错误，并且错误类型为 EAGAIN 或 EWOULDBLOCK。边缘触发效率较高，因为其减少了 epoll_wait 系统调用的次数。 21 死锁死锁发生条件：互斥，持有并等待，不可剥夺，环路等待 死锁避免：资源分配有序方式，超时放弃 Java 中死锁排查工具：jps，jstack，jconsole 22 Reactor 和 Proactor为了让服务器能够最大限度的服务客户端，有以下方式： 为每个连接创建一个进程 为每个连接创建一个线程，减少上下文切换开销 使用线程池，减少线程创建和销毁的开销 使用 IO 多路复用，减少线程在 read 操作时的阻塞时间 使用面向对象编程思想，封装 IO 多路复用，产生 Reactor 框架 异步网络模式编程 Proactor Reactor：I&#x2F;O 多路复用监听事件，收到事件后，根据事件类型分配给某个进程 &#x2F; 线程，其类型有： 单 Reactor 单进程：由于只存在单个进程，无法利用多核 CPU 性能，同时如果业务比较耗时，则会造成响应的延迟 单 Reactor 多线程：此时 Reactor 很可能成为性能瓶颈 多 Reactor 多线程：充分利用了 CPU 资源，并且 Reactor 不会成为瓶颈 Proactor：Reactor 使用的是非阻塞同步网络模式，而 Proactor 是异步网络模式，前者感知的是就绪可读写事件，后者感知的是已经完成的读写事件 应用：在 Linux 下，异步 IO 是不完善的，aio 系列函数并不是操作系统级别支持的，因此，基于 Linux 的高性能网络框架使用的是 Reactor 方案，而在 Windows 中，实现了一套完整的支持 socket 的异步编程接口，因此在 Windows 里实现高性能网络程序可以使用效率更高的 Proactor 方案。 23 高性能的单机管理主机的心跳服务需求：后台服务器通常以集群的方式对外提供服务，如果集群中的某台服务器宕机了，需要感知到该主机，并作容灾处理操作，提高系统的稳定性。感知方式可以通过心跳服务实现，即要求每台主机都要向一台主机上报心跳包，这样我们就能在这台主机上看到每台主机的在线情况，但是，当集群数量过大时，心跳服务并不容易实现。 宕机判断算法的设计：超时的主机则已经宕机，为了减少遍历，可以使用哈希链表实现，并且插入按照时间有序，这样在判断超时时，只需要看队头心跳包的信息。 高并发架构设计： 使用多线程：单线程不能充分利用 CPU 资源，多进程通信起来复杂 使用多路复用技术 epoll，其性能比 select&#x2F;poll 高 负载均衡：集群中的主机心跳包只会落到固定的线程中，可以实现无锁编程 线程绑定 CPU，提高核心的缓存命中率，Linux 中通过 sched_setaffinity 实现 内存分配器：Linux 中默认内存分配器是 PtMalloc2，它有一个缺点在申请小内存和多线程的情况下，申请内存的效率并不高，Google 开发的 TCMalloc 可以解决该问题 传输层协议选择：如果心跳包大小小于 MTU，可以选择 UDP，否则选择 TCP，TCP 还可以进行参数优化 24 一致性哈希负载均衡问题：最简单的方式，引入中间的负载均衡层，让它将外界的请求轮流转发给集群内部的节点上，考虑到每个节点的配置不同，可以为节点分配不同的权重，权重高的节点会被分配到更多的请求，这种方式称做是加权轮询。 哈希算法：加权轮询是无法应对分布式系统的，因为在分布式系统中，不同节点存储的数据是不同的。考虑一个 KV 系统，某个 key 应在哪些节点上获取，应该是确定的。哈希算法就能满足分布式系统的负载均衡需求。但是问题在于，如果集群规模扩容或者缩容时，必须迁移改变了映射关系的数据，解决该问题就需要进行数据迁移，最坏情况下是所有数据都需要迁移。 一致性哈希算法：该算法主要是为了解决分布式系统扩容或者缩容时，发生过多数据迁移的问题。一致性哈希是指将存储节点和数据都映射到一个首尾相连的哈希环上。这样，对于数据映射的结果，结果值往顺时针的方向的找到第一个节点，就是存储该数据的节点。 该方式在增大一个节点或者减少一个节点的时候，仅仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响，但是一致性哈希算法并不能保证节点能够在哈希环上分布均匀，有可能会有大量的请求集中在一个节点上。这样可能会造成雪崩式的连锁反应。 引入虚拟节点：一致性哈希算法存在节点在哈希环上分布不均的问题，这个时候可以引入虚拟节点，不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有两层映射关系。节点数量多了后，节点在哈希环上的分布就相对均匀了，同时稳定性也会更高。","categories":[],"tags":[{"name":"计算机系统","slug":"计算机系统","permalink":"http://blog.zsstrike.tech/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"}]},{"title":"《计算机网络》备忘录","slug":"《计算机网络》备忘录","date":"2022-02-12T12:35:53.000Z","updated":"2022-05-16T07:41:46.255Z","comments":true,"path":"2022/02/12/《计算机网络》备忘录/","link":"","permalink":"http://blog.zsstrike.tech/2022/02/12/%E3%80%8A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E3%80%8B%E5%A4%87%E5%BF%98%E5%BD%95/","excerpt":"计算机网络相关知识，用作备忘录，以备查阅。","text":"计算机网络相关知识，用作备忘录，以备查阅。 01 网络协议的分层结构OSI 分层模型： 自上而下分为应用层，表示层，会话层，传输层，网络层，数据链路层，物理层七层 会话层，表示层，应用层称为资源子网(用来数据处理)，物理层，数据链路层，网络层称为通信子网(实现数据通信)，传输层是通信子网和资源子网的通信接口 五层模型： 应用层：进行应用数据处理，封装后交给传输层 传输层：对应用层数据进行封装，如 TCP 和 UDP 协议 网络层：对传输层数据封装，如 IP 协议，其会提供寻址能力 数据链路层：每台设备有唯一的 MAC 地址（通过 ARP 协议），为网络层提供链路级别传输的服务 物理层：提供二进制传输的服务 网络包封装过程： 02 数据在网络中的封装一个常见的问题：当在浏览器中输入网址后，到网页显示，其间发生的过程？ 浏览器对 URL 进行解析 查询 URL 中的服务器对应的 IP 地址，客户端对本地 DNS 服务器采用递归查询，本地 DNS 服务器对 DNS 根服务器进行迭代查询 浏览器通过调用 Socket 库，来委托操作系统的 TCP&#x2F;IP 协议栈进行相应的数据操作 TCP 封装：通过 MSS 分割应用层数据，添加对应包头，主要包含源端口号，目的端口号，序号，确认序列，状态位（SYN，ACK，RST，FIN），在传输数据之前，需要先进行三次握手 IP 封装：主要包含源 IP 地址和目标 IP 地址，如果服务器存在多个网卡，需要通过网络和子网掩码来确认对应的 IP MAC 封装：两点间的传输，主要包含发送方 MAC 地址和接收方目标 MAC 地址，协议类型主要有 IP 协议和 ARP 协议，ARP 协议会在以太网中用广播的方式来询问 IP 地址对应的 MAC 地址 网卡：出口，将数字信息转换为电信号，网卡驱动从 IP 模块获取到包之后，会将其复制到网卡内的缓存区中，接着会其开头加上报头和起始帧分界符，在末尾加上用于检测错误的帧校验序列 交换机：二层网络设备，其端口不包含 MAC 地址，根据 MAC 地址表查找 MAC 地址，然后将信号发送到相应的端口 路由器：三层网络设备，路由器的各个端口具有 MAC 地址和 IP 地址，每次传输都需要修改 MAC 地址，但是源 IP 和目标 IP 始终是不会变动 请求达到服务器端：解包，获取应用层数据，返回相应的响应报文 响应到达客户端：解包出来响应的 html 数据交给浏览器进行渲染 客户端断开连接：通过 TCP 四次握手（亦可通过 keep-alive 保持连接） 网络包报文图解： 03 ICMP 协议及其应用ICMP 协议： 功能：确认 IP 包是否成功送达目标地址、报告发送过程中 IP 包被废弃的原因和改善网络设置 报文格式：被封装在 IP 包中，主要包含类型和代码，主要类型有 查询报文类型：回送应答和回送请求 差错报文类型：目标不可达，原点抑制消息，重定向消息，超时消息（TTL） ping 命令的过程： 构建回送请求数据包，此时类型为 8，序号为 1，同时插入发送时间 系统根据回送请求数据包构建 IP 包，修改协议为 1，表示 ICMP 将 IP 包交给下层，将其发送到目标主机 目标主机获取到回送请求数据包，构建对应的回送响应数据包，此时类型为 0，委托 IP 层发送给源主机 traceroute 命令实现（通过 UDP 或者 ICMP）： 故意设置特殊的 TTL，来追踪去往目的地时沿途经过的路由器 故意设置不分片，从而确定路径的 MTU ICMP 报文格式： 04 HTTP 协议的变迁HTTP 状态码： 1xx：表示目前是协议的中间状态 2xx：成功，如 200，204，206 3xx：重定向，如 301，302，304 4xx：客户端错误，如 400，403，404 5xx：服务器错误，如 500，501，502，503 HTTP 首部常见字段：Host，Content-Length，Connection，Content-Type，Content-Encoding HTTP 特性： 优点：报文格式简单，灵活和易于扩展，应用广泛和跨平台 不足：无状态双刃剑，明文传输不安全 HTTPS：在原来的 TCP 和 HTTP 层之间加入了 SSL&#x2F;TLS 层，保证信息的加密传输 混合加密保证了信息的机密性 摘要算法用来实现完整性 通过数字证书的方式保证服务器公钥的身份，解决冒充的风险 HTTP 协议的演变： HTTP&#x2F;1.1：增加了长连接，支持管道网络传输，但是可能会存在队头阻塞问题 HTTP&#x2F;2：基于 HTTPS，保障安全性，头部压缩，二进制格式，支持数据流，多路复用，服务器推送 HTTP&#x2F;3：在 HTTP&#x2F;2.0 多路复用时，如果某个请求发生丢包，会触发 TCP 重传机制，而阻塞其他请求，HTTP&#x2F;3 基于 UDP，上层的 QUIC 协议可以实现类似 TCP 的可靠性传输，同时 QUIC 合并了 TCP 三次握手和 TLS 四次握手 HTTP 各种协议栈： 05 TLS 协议TLS 握手过程： Client Hello：发送支持的密码套件列表，以及生成的随机数等 Server Hello 消息给出随机数和选择的密码套件，Server Certificate 消息给出服务器的数字证书，Server Hello Done 结束 客户端进行证书验证，可能涉及到证书链，验证证书通过检查证书的签名 Change Cipher Key Exchange 消息加密 pre-master，将其发送到服务器；Change Cipher Spec 消息用于告诉服务端开始使用加密方式发送消息；Finishd 消息会把之前的消息做个摘要，防止篡改 服务器发送 Change Cipher Spec 和 Encrypted Handshake Message 消息给客户端 RSA 算法：用于进行密钥协商，但是不支持前向保密，一旦服务端的私钥泄漏了，过去被第三方截获的所有 TLS 通讯密文都会被破解，可以改用 ECDHE 密钥协商算法。 HTTPS 建立连接的过程： 06 HTTPS 优化HTTPS 性能损耗： TLS 握手过程最长需要 2 RTT 后续的应用数据需要使用对称加密密钥进行加密和解密 HTTPS 优化方式： 硬件优化：选择支持 AES-NI 特性的 CPU，否则可以将对称加密算法改为 ChaCha20 软件升级：升级 Linux 内核，更新 OpenSSL 协议优化： TLS 1.2 可以使用 ECDHE 密钥交换算法，客户端可以在 TLS 协议的第 3 次握手后，第 4 次握手前，可以发送加密的应用数据，减少了 1 个 RTT TLS 1.3 把 Hello 和公钥交换这两个消息合并成了一个消息，于是这样就减少到只需 1 RTT 就能完成 TLS 握手 证书优化： 选择椭圆曲线（ECDSA）证书，相同安全强度下，其密钥长度比 RSA 短的多 证书验证协议优化：CRL，OCSP（CA 服务器成为瓶颈），OCSP Stapling 会话复用：不仅不具备前向安全，而且有重放攻击的风险，设置合理的过期时间 Session ID：双方缓存会话密钥，并用唯一的 Session ID 标识，服务器的内存压力增大 Session Ticket：服务器会加密会话密钥作为 Ticket 发给客户端，交给客户端缓存该 Ticket Pre-shared Key：TLS 1.3 使用，原理和 Ticket 类似，只不过在重连时，客户端会把 Ticket 和 HTTP 请求一同发送给服务端 ECDHE 交换算法： 07 密钥交换算法HTTPS 中常见的密钥交换算法有 RSA 和 ECDHE 算法，由于前者不支持前向安全性，ECDHE 被广泛使用。 ECDHE 算法发展： DH 算法：基于离散对数 DHE 算法：让双方的私钥在每次密钥交换通信时，都是随机生成的、临时的 ECDHE 算法：在 DHE 算法的基础上利用了 ECC 椭圆曲线，用更少的计算量计算出公钥和会话密钥 DH 算法过程： 08 HTTP&#x2F;1.1 优化HTTP&#x2F;1.1 优化方式： 减少 TCP 握手时间：使用 KeepAlive 将短连接修改为长连接，注意其不同于 TCP 的 keepalive 避免发送 HTTP 请求：使用缓存 减少 HTTP 请求次数：减少重定向请求次数，合并多个请求，懒加载 减少 HTTP 响应的数据大小：无损压缩和有损压缩 客户端缓存过程： 09 HTTP&#x2F;2 带来的优化HTTP&#x2F;1.1 面临的问题： HTTP 头部巨大并且存在重复 并发连接有限，如 Chrome 浏览器最大并发数是 6 HTTP&#x2F;1.x 协议导致的队头阻塞问题 不支持服务器推送 HTTP&#x2F;2 向下兼容 HTTP&#x2F;1.1，其特性： 头部压缩 HPACK 算法：由静态字典，动态字典和 Huffman 编码构成 二进制帧：对于一条 HTTP 响应，划分成了两个帧（HEADER + DATA）来传输 并发传输：多个 Stream 复用一条 TCP 连接，达到并发的效果，不同 Stream 的帧是可以乱序发送的，同一 Stream 内部的帧必须是严格有序的，还可以设置优先级 服务器主动推送：如在客户端请求 index.html 的时候主动推送 index.js 和 index.css TCP 协议导致的队头阻塞问题：TCP 需要等待前面的字节到达内核后才能将其传输到应用层 Stream 示意图： 10 HTTP&#x2F;3 带来的优化HTTP&#x2F;2 缺点： TCP 队头阻塞 TCP 和 TLS 的握手时延需要 3 RTT 网络迁移后需要重新进行连接 HTTP&#x2F;3 使用 UDP 传输协议，在应用层使用 QUIC 保证其可靠： 无 TCP 产生的队头阻塞，如果 QUIC 连接中某个数据包丢失了，其只会阻塞该流，其他流不受影响 更快的连接建立：QUIC 可以在 1 个 RTT 内同时完成建立连接与密钥协商，在第二次连接时，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果 只需要通过连接 ID 来标记通信的两个端点，不涉及 IP 地址，切换网络后（弱网环境）不需要重新连接 HTTP&#x2F;3 的 QPACK 通过两个特殊的单向流来同步双方的动态表，解决 HTTP&#x2F;2 的 HPACK 队头阻塞问题 QUIC 使用 UDP 传输，大部分路由器在网络繁忙的时候，会丢 UDP 包 HTTP 不同协议的连接示意图： 11 TCP 三次握手和四次挥手TCP 基本认识： TCP 是面向连接的、可靠的、基于字节流的传输层通信协议 头部格式包含：序列号，确认应答号，控制位 最大 TCP 连接数：客户端 IP 数 × 客户端端口数，但实际受限于文件描述符和内存大小 TCP 和 UDP 区别：连接，服务对象，可靠性，拥塞控制和流量控制，首部开销，应用场景 TCP 三次握手： 第三次握手是可以携带数据的，前两次握手是不可以携带数据的 为什么是三次握手： 三次握手才可以阻止历史重复连接的初始化（防止服务器一收到 SYN 包直接进入 ESTABLISH 状态，消耗资源） 三次握手才可以同步双方的初始序列号 三次握手才可以避免资源浪费 IP 层也会进行分片，为啥 TCP 层还需要 MSS：如果只交给 IP 层分片，发生丢包后，TCP 会重传整个 TCP 报文，而如果 TCP 采用 MSS，则只需要传送 MSS 单位的数据即可 SYN 攻击：通过伪造 SYN 报文，占满服务器的 SYN 接收队列，使得服务器不能正常工作 TCP 四次挥手： 需要四次挥手：客户端发送 FIN 报文表示客户端不再发送数据给服务器，服务器接收到 FIN 报文后，可能还需要发送响应数据给客户端，因此服务器 ACK 和 FIN 报文不会一起发送给客户端 为什么需要 TIME_WAIT 状态： 防止 TCP 重连后接收到旧的数据包 保证最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭 为什么 TIME_WAIT 需要 2MSL：保证最后的 ACK 可能丢失时，依旧能够收到服务器的 FIN 报文 TIME_WAIT 过高的危害：消耗内存资源和端口资源 TCP 三次握手过程图： TCP 四次挥手过程图： 12 TCP 相关机制TCP 重传机制： 超时重传：超时重传时间 RTO 的值应该略大于报文往返 RTT 的值，发生重传时，会加倍 RTO 快速重传：收到三次相同 ACK，触发重传 SACK：在 TCP 头部选项中加入 SACK，告知发送方哪些数据已经被接收 D-SACK：使用了 SACK 来告诉发送方有哪些数据被重复接收了，让发送方知道是丢数据包还是 ACK 包 滑动窗口： 改善 TCP 每发送一个数据包就进行一次确认应答的性能 接收窗口和发送窗口构成，接收窗口的大小是约等于发送窗口的大小 流量控制： 通过滑动窗口，可以让发送方根据接收方的实际接收能力控制发送的数据量 如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器，如果超时，就会发送窗口探测报文 糊涂窗口综合症：通过 TCP 发送少量数据给接收方，可以 让接收方不通告小窗口给发送方 让发送方避免发送小数据 拥塞控制： 防止网络出现拥堵时，发送方还继续发送大量数据包，导致网络更加拥堵 引入拥塞窗口后，发送窗口的值为 swnd &#x3D; min(cwnd, rwnd)，只要网络没有拥塞，cwnd 就会增大，反之，cwnd 就会减少 一旦发生了重传，就会认为网络出现了拥塞，拥塞算法如下 慢启动：cwnd 呈现指数型增长 拥塞避免：一旦到达慢启动门限，cwnd 呈现线性增长 拥塞发生： 超时重传：ssthresh &#x3D; cwnd &#x2F; 2，cwnd &#x3D; 1，也就是进入慢启动 快速重传 + 快速恢复：ssthresh &#x3D; cwnd &#x2F; 2，cwnd &#x3D; ssthresh + 3，进入拥塞避免 发送窗口和接收窗口： 13 IP 层IP 层工作在第三层，实现两个主机间的通信，而 MAC 作用是实现两个直连设备之间的通信 IP 地址的分类： 传统分类：A，B，C，D，E，但存在同一网络下没有地址层次，不能很好的与现实网络匹配等问题 CIDR：表示为 a.b.c.d/x ，前 x 位表示网络号，后 32 - x 位表示主机号，类似子网掩码，还可以对其进行子网划分 IP 分片和重组：当 IP 数据包超过 MTU 大小时，IP 数据包就会被分片，但是由于分片后的 IP 数据包只能在目标主机进行重组，一旦丢失某个分片数据，则需要重传整个 IP 数据包，为此引入了 TCP 的 MSS IPv4 和 IPv6 区别： 地址位数及其表示方法 IPv6 即使没有 DHCP 服务器也可以自动分配 IP 地址 IPv6 包头固定长度为 40 字节，提高了传输性能 IPv6 能够防止线路窃听，提高了安全性 IP 相关协议：DNS，ARP，RARP，DHCP，NAT，NAPT，ICMP 14 TCP 抓包分析抓包工具：tcpdump 和 wireshark 抓包到的 TCP 挥手是三次：当服务器受到客户端的 FIN 时，如果之后没有数据发送给客户端，就会合并 ACK 和 FIN 报文 TCP 三次握手异常： 第一次的 SYN 包丢失：RTO 指数增长，tcp_syn_retries 第二次的 SYN + ACK 包丢失：RTO 指数增长，tcp_syn_retries，tcp_synack_retries 第三次的 ACK 丢包：RTO 指数增长，tcp_synack_retries，tcp_retries2 TCP 四次挥手异常： 第一次的 FIN 包丢失：RTO 指数增长，tcp_orphan_retries 第三次的 FIN 包丢失：RTO 指数增长，tcp_orphan_retries 第一次或者第四次 ACK 包丢失：不会重传 ACK 包，而是重传 FIN 包 TCP Fast Open：第一次 TCP 连接时给客户端发送 Cookie，接下来的连接就可以直接带上 Cookie 和对应的 HTTP 请求，达到 1 RTT 的效果 TCP 延迟确认和 Nagle 算法：用于增加整个报文的有效负载 延迟确认：没有携带数据的 ACK，其有效负载也是很低的，通过等待一定时间再发送 ACK 给客户端 Nagle 算法：没有已发送未确认报文时，立刻发送数据；存在未确认报文时，直到没有已发送未确认报文或数据长度达到 MSS 大小时，再发送数据 不能同时使用延迟确认和 Nagle 算法，其会造成额外的时延 TCP Fast Open： 15 TCP 半连接队列和全连接队列当服务端受到客户端的 SYN 请求后，内核会把该连接存储到半连接队列；当服务端受到客户端的 ACK 响应后，内核会把连接从半连接队列移除，然后创建新的完全的连接，将其放入全连接队列中 TCP 全连接队列溢出： 后续请求报文或者被丢弃，或者被发送 RST 报文，由 tcp_abort_on_overflow 控制 增大全连接队列：全连接队列最大值 min(somaxconn, backlog) TCP 半连接队列溢出： 半连接队列最大值 max_qlen_log &#x3D; min(min(somaxconn, backlog), max_syn_backlog) × 2 当开启了 tcp_syncookies 功能后可以在不使用 SYN 半连接队列的情况下建立连接 防御 syn 攻击：开启 tcp_syncookies 功能，增大半连接队列，减少 SYN + ACK 重传次数 TCP 连接队列示意图： 16 TCP 调优TCP 三次握手： 调整 SYN 报文的重传次数：tcp_syn_retries 调整 SYN 半连接队列长度：tcp_max_syn_backlog，somaxconn，backlog 调整 SYN + ACK 报文的重传次数：tcp_synack_retries 调整 accept 队列长度：somaxconn，backlog 绕过三次握手：tcp_fastopen TCP 四次挥手： 调整 FIN 报文重传次数：tcp_orphan_retries 调整 FIN_WAIT2 状态的时间：tcp_fin_timeout 调整孤儿连接上限：tcp_max_orphans 复用 time_wait 状态的连接：tcp_tw_reuse TCP 传输优化： 扩大窗口：tcp_window_scaling 调整发送缓冲区范围：tcp_wmem 调整接收缓冲区范围：tcp_rmem 调整内存范围：tcp_mem 17 TCP 和 UDPTCP 是面向字节流的协议，而 UDP 是面向报文的协议： 面向报文指的是每个 UDP 报文就是一个用户消息的边界，在收到 UDP 报文后，可以读取并解析，因此需要应用层控制报文的大小 面向字节流则是指操作系统可能会对用户的消息进行拆分，使其满足 MSS 等限制，这样可能会造成粘包问题，可以以下方式解决： 固定长度的消息 特殊字符作为边界 自定义消息结构 18 TCP 和 HTTP 的 keepalive 机制HTTP 的 Keep-Alive 也叫 HTTP 长连接，其目的为减少 HTTP 短连接中每次请求都需要 TCP 三次握手和四次挥手的开销，同时也使得 HTTP&#x2F;1.1 流水线技术得以实现 TCP 的 Keepalive 也叫 TCP 保活机制，当 TCP 层长时间没有数据交互时，内核为了确保该连接是否还有效，就会发送探测报文，来检测对方是否还在线，然后来决定是否要关闭该连接 19 TCP 异常假设：客户端和服务端通过 TCP 连接，并且没有开启 TCP 的 keepalive，在下列情况下： 一直没有数据交互下： 主机崩溃（掉电）：服务器对此是无法感知的，服务端的 TCP 连接将会一直处于 ESTABLISHED 连接状态，直到重启 进程崩溃（kill）：操作系统会自动发送 FIN 报文，进行四次挥手 存在数据交互下，客户端主机崩溃： 迅速重启：在收到服务器重传的报文时，会回复 RST 报文，重置 TCP 连接 不重启：服务器端一直进行报文重传，由 tcp_retries2 控制 如果客户端在 TIME_WAIT 状态下，收到来自服务端的数据包时，客户端的内核会发送该数据包的 ACK 确认报文（防止服务端重传），然后直接丢掉该数据包，具体过程如下： 如果客户端在 FIN_WAIT_2 状态下，FIN 报文和服务器响应的数据乱序了，也就是如果客户端收到乱序的 FIN 报文，那么就被会加入到乱序队列，并不会进入到 TIME_WAIT 状态，等 FIN 之前的数据都被接收后自动进入 TIME_WAIT 状态： 20 常用的网络命令远程登陆和传输：ssh，scp 查看本地网络状态：ifconfig，netstat 网络测试：ping，telnet DNS 查询：host，dig HTTP 请求：curl 21 信息风险与防范措施HTTP 明文传输，并且没有身份认证，所以存在以下风险： 窃听风险：中间人可以截获报文查看其中信息，通过对称加密防范 篡改风险：中间人可以截获报文并且修改后发送给收件人，可以用哈希函数来计算出内容的哈希值 冒充风险：虽然使用哈希可以保证报文不被篡改，但是不能防止中间人重新构建一个报文发送给收件人的情况。为此可以采用非对称加密方法，通过私钥加密，公钥解密的方式，来确认消息的身份，注意此时加密的是内容的哈希值。通过 CA 保证信任链的真实可靠，防止中间人伪造公私钥 数字证书签发和验证流程： 22 TLS 协议TLS 1.2 和 TLS 1.3 协议的区别： TLS1.2 需要四次握手，握手完成后才能发送数据；TLS 1.3 只需要两次握手过程 TLS 1.3 会话恢复时握手过程只需要 0 RTT，通过 pre_shared_key 实现 通常，不管 TLS 握手次数如何，都得先经过 TCP 三次握手后才能进行；但是如果在 TCP Fast Open + TLS 1.3 情况下，在第二次以后的通信过程中，TLS 和 TCP 的握手过程是可以同时进行的 TLS 会话恢复： 23 CDNCDN 用于解决远距离导致的网络访问延迟问题，通常用于加速一些静态资源，CDN 加速策略分为： 推模式：当用户就近访问的 CDN 节点没有缓存请求的数据时，CDN 会主动从源服务器下载数据，并更新到这个 CDN 节点的缓存中 拉模式：也称为 CDN 预热，通过 CDN 服务提供的 API 接口，把需要预热的资源地址和需要预热的区域等信息提交上去 CDN 通过 GSLB 服务器找到最近的 CDN 服务器： 24 第三方 TCP 工具断开连接的实现如果一个已经建立的 TCP 连接，客户端中途宕机了，而服务端此时也没有数据要发送，一直处于 establish 状态，客户端恢复后，向服务端建立连接，此时服务端会怎么处理： 客户端的 SYN 报文里的端口号与历史连接不相同：服务器重新建立新连接，原来的 TCP 连接： 如果服务端发送了数据包给客户端，此时客户的内核就会回 RST 报文，服务端收到后就会释放连接 如果服务端一直没有发送数据包给客户端，在超过一段时间后， TCP 保活机制就会启动 客户端的 SYN 报文里的端口号与历史连接相同：由于序列号不同，服务器会回复一个携带了正确序列号和确认号的 ACK 报文，客户端收到该报文后，发现不是自己期待的，就会发回 RST 报文，此时，服务端释放该连接 可以使用命令 killcx &lt;IP地址&gt;:&lt;端口号&gt; 来关闭某个特定的 TCP 连接，其利用 Challenge ACK 中的有效信息，构造出两个 RST 报文，用于终止双方的 TCP 连接： 25 序列化和反序列化序列化和反序列化：在应用程序中，对象如果想要通过网络传输到另一端上或者保存在文件中，就需要将其序列化，而如果想要将序列化的对象读取，则需要反序列化 常见的序列化： JDK 原生序列化：缺陷是不能跨语言 JSON：进行序列化的额外空间开销比较大 ProtoBuf：由 Google 出品的，是一种轻便、高效的结构化数据存储格式，可以用于结构化数据序列化，并且支持跨平台，使用前需要定义 IDL（Interface description language）文件 26 SYN 报文何时被丢弃SYN 报文丢失的两种场景： 开启 tcp_tw_recycle 参数，并且在 NAT 环境下，造成 SYN 报文被丢弃 accpet 队列满了，造成 SYN 报文被丢弃 对于主动关闭方，最后一个状态是 TIME_WAIT，其作用是： 防止具有相同四元组的旧数据包被收到，也就是防止历史连接中的数据，被后面的连接接受 保证被动关闭连接的一方能被正确的关闭，即保证最后的 ACK 能让被动关闭方接收 Linux 提供了两个参数快速回收处于 TIME_WAIT 的状态： net.ipv4.tcp_tw_reuse：客户端在调用 connect() 函数时，内核会随机找一个 time_wait 状态超过 1 秒的连接给新的连接复用，所以该选项只适用于连接发起方 net.ipv4.tcp_tw_recycle：允许处于 TIME_WAIT 状态的连接被快速回收 对于服务器来说，如果同时开启了 recycle 和 timestamps 选项，则会开启 per-host 的 PAWS 机制。 PAWS 机制：开启了 tcp_timestamps 之后，该机制自动启动，它的作用是防止 TCP 包中的序列号发生绕回，因为 seq 位数 32，可能存在一次连接中，两次 seq 是相同值。PAWS 机制如果发现收到的数据包中时间戳不是递增的，则表示该数据包是过期的，就会直接丢弃这个数据包 per-host 的 PAWS 机制：per-host 是对对端 IP 做 PAWS 检查，而非对IP + 端口四元组做 PAWS 检查 SYN 包丢失场景一：当客户端 A 通过 NAT 网关和服务器建立 TCP 连接，然后服务器主动关闭并且快速回收 TIME-WAIT 状态的连接后，客户端 B 也通过 NAT 网关和服务器建立 TCP 连接，注意客户端 A 和 客户端 B 因为经过相同的 NAT 网关，所以是用相同的 IP 地址与服务端建立 TCP 连接，如果客户端 B 的 timestamp 比 客户端 A 的 timestamp 小，那么由于服务端的 per-host 的 PAWS 机制的作用，服务端就会丢弃客户端主机 B 发来的 SYN 包。 SYN 包丢失场景二：在服务端并发处理大量请求时，如果 TCP accpet 队列过小，或者应用程序调用 accept() 不及时，就会造成 accpet 队列满了 ，这时后续的连接就会被丢弃，这样就会出现服务端请求数量上不去的现象。 Socket 连接过程： 27 TIME_WAIT 状态的作用TIME_WAIT 状态的作用： 防止历史连接中的数据，被后面相同四元组的连接错误的接收 保证「被动关闭连接」的一方，能被正确的关闭 tcp_tw_reuse 参数用于客户端 在调用 connect() 函数时，内核会随机找一个 TIME_WAIT 状态超过 1 秒的连接给新的连接复用，所以该选项只适用于连接发起方。 为了使得上述参数起作用，需要开启 tcp_timestamps 参数，起作用有： 便于精确计算 RTT 能防止序列号回绕（PAWS） 为什么 tcp_tw_reuse 默认是关闭的： 开启 tcp_tw_reuse 后，对于 RST 报文，即使其时间戳过期了，只要 RST 报文的序列号在对方的接收窗口内，也是能被接受的 RFC：建议 RST 段不携带时间戳，并且无论其时间戳如何，RST 段都是可接受的。老的重复的 RST 段应该是极不可能的，并且它们的清除功能应优先于时间戳 如果第四次挥手的 ACK 报文丢失了，有可能会导致被动关闭连接的一方不能被正常的关闭 tcp_tw_reuse 的作用是让客户端快速复用处于 TIME_WAIT 状态的端口，相当于跳过了 TIME_WAIT 状态，这可能会出现这样的两个问题： 历史 RST 报文可能会终止后面相同四元组的连接，因为 PAWS 即使发现 RST 是过期的，也不会丢弃 如果第四次挥手的 ACK 报文丢失了，有可能被动关闭连接的一方不能被正常的关闭 28 DNS 解析客户端对本地 DNS 服务器采用递归查询，本地 DNS 服务器对 DNS 根服务器进行迭代查询。 IPv4 的根域名服务器一共有 13 个，其是由 UDP 包大小决定的，RFC 规定，DNS 报文要求被控制在 512 字节之内。 DNS 主根域名服务器在美国，如果美国终止 cn 后缀的解析，并不会导致中国国内的网络瘫痪，因为中国已经有很多台镜像的根域名服务器，可以让其继续支持 cn 后缀的解析，但是国外用户可能就无法访问 cn 后缀的网站了。 29 异常情况下的 TCP 连接在 Linux 中，TCP 连接信息会放在一个 struct socket 结构体中，拔掉网线并不会影响 TCP 的连接状态。如果之后进行下面不同操作： 拔掉网线，有数据传输：客户端会触发超时重传，如果一直没有将网线插回去，达到重传次数上限或者最大超时时间时，就会停止重传，内核断开 TCP 连接 拔掉网线，无数据传输：如果没有开启 TCP keepalive 机制，该连接一直保存；如果开启了 TCP keepalive 机制，等待一段时间后，就会发送 TCP 保活报文到服务端，如果一直没有网络，最终达到最大保活探测次数后，TCP 会报告该连接已经死亡 客户端宕机：跟拔掉网线是一样无法被服务端的感知的，所以如果在没有数据传输，并且没有开启 TCP keepalive 机制时，服务端的 TCP 连接将会一直处于 ESTABLISHED 连接状态，直到服务端重启进程 客户端杀死进程：杀死客户端的进程后，客户端的内核就会向服务端发送 FIN 报文，与服务端四次挥手","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://blog.zsstrike.tech/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"MySQL实战45讲","slug":"MySQL实战45讲","date":"2021-09-19T07:56:04.000Z","updated":"2022-12-06T08:20:54.603Z","comments":true,"path":"2021/09/19/MySQL实战45讲/","link":"","permalink":"http://blog.zsstrike.tech/2021/09/19/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/","excerpt":"本文主要是对 MySQL实战 45 讲中内容的一些总结，以备查阅。","text":"本文主要是对 MySQL实战 45 讲中内容的一些总结，以备查阅。 01 基础架构：SQL 查询语句如何执行 MySQL 基本架构图如上，大体上分为 Server 层和存储引擎层。前者涵盖了 MySQL 的大多数核心服务，如内置函数，跨存储引擎的功能，如存储过程，触发器，视图等 ；后者则是负责数据的存储和提取，其架构模式是插件式的，如 InnoDB，MyISAM，Memory 等，其中 InnoDB 从 MySQL 5.5 之后成为默认存储引擎。 考虑这样的一条语句：select * from T where ID=10 ，了解一下每个组件的功能。 连接器：用户在发送 SQL 语句前需要和 Server 端连接，TCP 连接之后，连接器对用户身份进行认证。在连接的时候尽量选择长连接，可能会发现 MySQL 的内存占用涨得很快，这是因为执行过程中的临时内存是管理在连接对象里面的。为了解决该问题，可以定期断开连接或者主动执行重置连接 mysql-reset_connection 查询缓存：MySQL 拿到一个查询请求后，先去查询缓存里面看看是不是已经执行过该语句，如果存在缓存则直接返回。不建议使用查询缓存，只要对一个表存在更新，这个表上的查询缓存就会被清空，性能提升不大，另外，从 MySQL 8.0 版本开始，不存在查询缓存组件 分析器：首先进行词法分析，转换为单词流，之后再进行语法分析，判断语句是否满足 SQL 语法 优化器：将上一步的语法树进行语义分析，决定如使用哪个索引或者如何进行 join 操作，生成执行计划 执行器：开始执行之前，判断一下用户对该表有没有查询权限，即进行权限验证。假设 ID 字段无索引，那么执行流程如下： 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端 对于有索引的表，逻辑差不多相同，只不过会按照索引来进行遍历。另外，索引下推便是让执行器进行条件判断，减少回表次数。 02 日志系统：SQL 更新语句如何执行更新语句同样也需要经过连接器，分析器，优化器和执行器，不过不同于查询语句，其还设计到两个重要的日志模块：redo log（重做日志） 和 binlog（归档日志）。 redo log：没有 redo log 的时候，每次更新一条数据都需要至少进行一次 IO 访问，降低了系统的性能，为此，可以采用redo log。当需要更新一条记录的时候，InnoDB 引擎会把记录写到 redo log 中，并且更新内存，这样就算更新完成了，之后，其会在适当的时候，将这个操作记录更新到磁盘里面。InnoDB 中的 redo log 可以组织为一个循环文件，写到结尾之后又从开头的地方写，如下图： write_pos 是当前写指针的位置，checkpoint 是当前需要擦除的位置，注意擦除前需要将记录更新到数据文件。有了 redo log ，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，该能力称为 carsh-safe。redo log 也是将数据写到磁盘文件上，但是使用了顺序写和组提交，这是主要的优化部分。更多可以参考该链接。 binlog：redo log 和 binlog 不同点如下： redo log 是 InnoDB 独占的，binlog 是 Server 层实现，所有引擎都可以使用 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录语句的原始逻辑，比如“给 ID&#x3D;2 这一行的某字段加 1”（binglog 有 statement 模式和 row 模式） redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的 redo log 用于保证事务的一致性和提升更新操作的效率 ；binlog 主要用于备份和恢复数据使用 考虑语句 update T set c=c+1 where ID=2，其在执行器和 InnoDB 引擎内部流程： 执行器先找引擎取ID&#x3D;2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID&#x3D;2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务 执行器生成这个操作的 binlog，并把 binlog 写入磁盘 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成 两阶段提交：InnoDB 将 redo log 的分为 prepare 和 commit 状态，称为两阶段提交，主要目的是为了保证两份日志之间的逻辑一致。如果不采用两阶段提交，那么不管是先写redo log后写binlog，还是先写binlog后写redo log，只要在某个 log 写完之后发生重启，两种日志的恢复出来的状态并不一致。 有了两阶段提交的话，如果在第 4 步发生重启，那么由于 redo log 处于 prepare 状态，并且 binlog 失败，那么事务本身会回滚；如果在第 5 步发生重启，由于 redo log 已经处于 prepare 状态，并且存在相应的 binlog，那么重启后会自动 commit。 03 事务隔离在 MySQL 中，事务支持是在存储引擎层实现的，因此，并不是所有的引擎都支持事务处理，如 MyISAM，这是其被 InnoDB 取代的原因之一。 隔离性与隔离级别当多个事务并发进行处理的时候，可能会出现脏读，不可重复读，幻象问题，为了解决这些问题，引入了对应的隔离级别。SQL92 中定义了四种隔离级别：未提交读，提交读，可重复读和串行化。 未提交读：v1 &#x3D; 2，v2 &#x3D; 2， v3 &#x3D; 2 提交读：v1 &#x3D; 1，v2 &#x3D; 2， v3 &#x3D; 2 可重复读：v1 &#x3D;1，v2 &#x3D; 1， v3 &#x3D; 2 串行化：v1 &#x3D; 1，v2 &#x3D; 1， v3 &#x3D; 2（防止幻象） 事务隔离的实现在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。 在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 回滚日志不能一直保存，当系统中没有比这个回滚日志更早的 read-view 的时候就会被清理掉。基于该事实，一般建议不要使用长事务。长事务意味着系统里面会存在很老的事务视图，可能会占用很大的空间，另外，长事务也会占用锁资源，可能会拖垮整个库。 事务启动方式MySQL的事务启动方式有以下几种： 显式启动事务语句， begin 或 start transaction。配套的提交语句是commit，回滚语句是rollback。 set autocommit&#x3D;0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个select语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行commit 或 rollback 语句，或者断开连接。 建议使用 set autocommit=1，显式处理事务流程，防止长事务的产生。 04 深入浅出索引（上）索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。 索引的常见模型 哈希表：适用于等值查询，不适用于范围查询 有序数组：在等值查询和范围查询场景中的性能就都非常优秀，但是只适用于静态存储引擎 搜索树：搜索效率高，读写性能比较平衡，其中二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。为了减少访问磁盘的次数，应该尽量控制索引树的高度，因此使用 N 叉树，其中 N 的大小取决于数据块的大小。N叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。 InnoDB 的索引模型在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。 从图中不难看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引。 主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）。 非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引（secondary index）。 基于非主键索引的查询需要多扫描一棵索引树（回表操作），在应用中应该尽量使用主键查询。 索引维护InnoDB 的索引结构是 B+ 树，如果新插入的数据所在的数据页已经满了，则需要进行页分裂操作。除了性能外，页分裂操作还影响数据页的利用率。当然，也存在页合并操作。 索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。需要注意的是，重建聚簇索引会将整个表重建，意味着二级索引也会被重建。 使用自增主键可以保证递增插入，不会触发页分裂操作，而且在某些情况下，使用自增主键比使用其他唯一主键节省空间，因为主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 当然，如果在典型的 KV 场景下（有且仅有一个唯一索引），可以直接使用业务字段作为主键。 05 深入浅出索引（下）当在二级索引 k 上查找 select * from T where k between 3 and 5，由于叶子节点记录的是主键，还需要回表进行数据的获取。 覆盖索引如果执行 select ID from T where k between 3 and 5，由于需要的数据在二级索引叶子节点上，此时不需要回表，称为覆盖索引。覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。具体来讲，便是创建联合索引。 最左前缀原则不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。 索引下推假设存在 （name，age）的联合索引，需要执行 select * from tuser where name like &#39;张%&#39; and age=10 and ismale=1，首先根据前缀索引原则，找到 ‘张’ 开始记录，然后依次判断其他条件是否满足： 在MySQL 5.6之前，只能从ID3开始一个个回表。到主键索引上找出数据行，再对比字段值 而MySQL 5.6 引入的索引下推优化， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数 下图是未执行索引下推的示意图： 下图是执行了索引下推的示意图： 问题已经存在（a，b）联合主键，并且存在 c 二级索引，那么对于以下查询需求，（c，a）和（c，b）联合索引是否必要？ 12select * from geek where c=N order by a limit 1;select * from geek where c=N order by b limit 1; 答案是（c，a）索引没有存在的必要，找到 c = N 的记录后可以直接回表，是按照 a 排序的，符合最左前缀，但是（c，b）需要存在。 06 全局锁和表锁数据库锁设计的初衷是处理并发问题，并发时需要合理控制资源的访问规则，而锁就是用来实现这些访问规则的重要数据结构。 全局锁全局锁就是对整个数据库实例加锁，MySQL 提供加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)，该命令会让整个库处于只读状态。 全库逻辑备份方案： FTWRL：整个数据库只读，会降低系统性能甚至拖垮数据库 mysqldump：使用参数 --single-transaction，导数据之前就会启动一个事务，来确保拿到一致性视图（可重复读级别下开启事务），需要引擎支持 RR 隔离级别 全库只读另外一个命令set global readonly=true，但是不推荐使用，主要有 在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库 在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高 表级锁MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。 表锁的语法是 lock tables … read&#x2F;write，同 FTWRL 一样，在客户端断开的时候自动释放。 MDL 则不需要显式使用，在访问一个表的时候会被自动加上，保证读写的正确性。MySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。事务中的MDL锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。 如何安全地给小表加字段？首先要解决长事务，如果要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，此时可以使用 DDL NOWAIT&#x2F;WAIT n 语法，等待一段时间，如果还是没有获取到，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程。 问题备份一般都会在备库上执行，你在用 --single-transaction 方法做逻辑备份的过程中，如果主库上的一个小表做了一个 DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？ 123456789101112Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;Q2:START TRANSACTION WITH CONSISTENT SNAPSHOT；/* other tables */Q3:SAVEPOINT sp;/* 时刻 1 */Q4:show create table `t1`;/* 时刻 2 */Q5:SELECT * FROM `t1`;/* 时刻 3 */Q6:ROLLBACK TO SAVEPOINT sp; /* release MDL *//* 时刻 4 *//* other tables */ 参考答案如下： 如果在 Q4 语句执行之前到达，现象：没有影响，备份拿到的是 DDL 后的表结构。 如果在”时刻 2”到达，则表结构被改过，Q5 执行的时候，报 Table definition has changed, please retry transaction，现象：mysqldump 终止。 如果在“时刻2”和“时刻3”之间到达，mysqldump 占着 t1 的 MDL 读锁，binlog 被阻塞，现象：主从延迟，直到 Q6 执行完成。 从“时刻4”开始，mysqldump 释放了 MDL 读锁，现象：没有影响，备份拿到的是 DDL 前的表结构。 07 行锁功过：怎么减少行锁对性能的影响MySQL 的行锁是在引擎层由各个引擎自己实现的，InnoDB 引擎支持行锁，但是 MyISAM 引擎不支持行锁，这也是 MyISAM 被 InnoDB 替代的重要原因之一。 两阶段锁下图展示两个事务的语句执行流程： 事务 B 的 update 语句会被阻塞，直到事务 A 执行 commit 之后，事务 B 才能继续执行。也就是说，在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 根据该协议，为了提高并发速度，如果多个事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。 另外，innodb 行级锁是通过锁索引记录实现的，如果 update 的列没建索引，innodb 内部是全表根据主键索引逐行扫描，逐行加锁，事务提交后释放锁。 死锁和死锁检测当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。出现死锁以后，有两种策略： 直接进入等待，直到超时：innodb_lock_wait_timeout 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行：innodb_deadlock_detect 正常情况下我们还是要采用第二种策略，但是在很多事务存在的情况下，死锁检测有很大的负担。为了解决由这种热点行更新导致的性能问题，一般有两种思路： 一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉 另一个思路是控制并发度，如控制每个客户端的并发线程数量等 08 事务到底是隔离的还是不隔离的假设初始时表中存在 &lt;id, k&gt;(1, 1), (2, 2) 数据，现在存在如下执行序列： 上图中的执行结果是事务 B 查到的 k 的值是 3，而事务 A 查到的 k 的值是 1。 在 MySQL 中，有两种视图的概念： view：即用查询语句定义的虚拟表 consistent read view：InnoDB 在实现 MVCC 时用到的一致性读视图，用于支持 RC 和 RR 隔离级别的实现 快照在 MVCC 里怎么工作的在可重复读隔离级别下，事务在启动的时候就创建了一个整库快照。快照通过每个事务ID实现，它是在事务开始的时候向InnoDB的事务系统申请的，并且按申请顺序严格递增的。每行数据也是有多个版本的，每个版本有自己的 row trx id。 实际上，上图中的三个虚线箭头就是 undo log，而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。另外，InnoDB 会为每个事务构造一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。判断某个事务是否可见便是根据该数组和高低水位控制的。另外，事务总是能够看到自己修改过的值。 更新逻辑更新逻辑和查询逻辑不同，查询遵循一致性读，但是更新只会读最新已提交值（前提是已经获取到了行锁），否则的话可能会造成丢失更新问题。这就是为什么 B 事务查询到的 k 值是 3。select 语句加上 lock in share mode 或 for update 也会读到 3 ，分别加了 S 锁和 X 锁。 如果存在以下执行语句： 那么这时两阶段锁协议就派上用场了，事务 C 在没有提交前，事务 B 会进行锁等待。 RC 和 RRRC 和 RR 的逻辑类似，主要的区别是： 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图 问题假设某用户在 RR 级别下，在客户端执行： 12345insert into t(id, c) values(1,1),(2,2),(3,3),(4,4);begin;update t set c=0 where id=c;select * from t;commit; 第 4 行发现表中数据没有变化，这是为什么？ 解析：RR 下，用另外一个事务在 update 执行之前，先把所有 c 值修改，应该就可以，比如先执行 update t set c = id + 1。 09 普通索引和唯一索引，应该怎么选择假设存在 t(ID, k) 表，其索引结构如下： 对于查询过程，若 k 是 普通索引：查找到满足条件的第一个记录 (5, 500) 后，需要查找下一个记录，直到碰到第一个不满足 k&#x3D;5 条件的记录 唯一索引来说：由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索 两者性能差别不大（数据库按照页为单位读取数据）。 对于插入过程 (4, 400)，若 k 是 普通索引：如果记录要更新的目标页在内存中，找到 3 和 5 之间的位置，插入这个值，语句执行结束；否则，将更新记录在 change buffer，语句结束 唯一索引：如果记录要更新的目标页在内存中，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束；否则，将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束 change buffer 相当于将插入操作推迟，减少或延迟了随机 IO 访问，并且其会在相应页因为查询操作读入内存时，执行 merge 操作，将修改后的数据写入到数据页中（也会更新 redo log）。注意其也会被写入到磁盘上。 由于唯一索引需要判断是否存在冲突，需要查看相关数据页，这时数据页已经在内存中了，没必要写到 change buffer 中，因此 change buffer 只用于唯一索引，并且为了提高效率，可以只用于写多读少的负载下。 假设想要插入 (id1, k1), (id2, k2)，k1 所在的数据页在内存中，k2 所在的数据页不在内存中，下图用于展示 change buffer 和 redo log 之间的关系： 上述过程实际上只涉及一次 IO 操作，即 redo log 的持久化，并且还是顺序写入。假设现在需要查询 k1, k2： 问题change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢？change buffer 丢失可不是小事儿，再从磁盘读入数据可就没有了 merge 过程，就等于是数据丢失了。会不会出现这种情况呢？ 解析：会导致 change buffer 丢失，会导致本次未完成的操作数据丢失，但不会导致已完成操作的数据丢失。change buffer 中分两部分，一部分是本次写入未写完的，一部分是已经写入完成的。针对未写完的，此部分操作，还未写入 redo log，因此事务还未提交，所以没影响；针对已经写完成的，可以通过 redo log 来进行恢复。综上，不会对数据库造成影响。 10 MySQL为什么有时候会选错索引假设存在表 t(id, a, b)，三个字段上都有索引。使用 idata 过程插入 100000 行数据，并且数据按照整数递增，即 (1, 1, 1) 到 (100000, 100000, 100000) 。如果存在以下操作序列： 上面的 explain 语句将会展示该语句不会使用索引 a，造成上面现象的原因是优化器的错误分析。 优化器会根据扫描行数，使用临时表，是否排序等因素综合判断，以选择最合适的索引。扫描行数是根据统计值区分度基数（cardinality）来估算的，该基数越大，索引的效果越好。为了性能，该基数采样统计生成的。 回到以上操作序列，优化器预估的语句扫描时间： 上文中，尽管 rows 数据显示使用 a 索引会更好，但是优化器会将回表的时间开销也算进去，最终选择了 id 索引。 之所以索引 a 上数据是 37116，而非 10000，造成上述现象的原因：由于 session A 开启了事务并没有提交，之前插入的 10 万行数据是不能删除的，这样一来，之前的数据每一行都有两个版本，旧版本是delete之前的数据，新版本是标记为deleted的数据，这样，索引 a 上的数据就有两份。但是，对于主索引，主键是直接按照表的行数来估计的，而表的行数通过 show table status 语句实现，所以仍然是 100000 左右。 上文中优化器选错索引的原因在于没能准确地判断出扫描行数，可以使用 analyze table t 来重新统计索引信息： 但是，对于以下命令： 12select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1; 优化器会选择 b 索引来使用，因为其认为使用 b 索引可以防止排序，尽管 b 索引需要的时间更长。 为了解决上述这种情况，即优化器选错索引的情况，可以 使用 force index 改写语句，引导优化器选择我们期望的索引，如将 order by b limit 1 改为 order by b,a limit 1 新建一个索引，或者删除误用的索引 11 怎么给字符串字段加索引假设存在表 SUser(ID, emial, name, …)，如果分别以 emial 和 email(6) 来创建索引，其索引结构如下： 这样，对于查询语句： 1select id,name,email from SUser where email=&#x27;zhangssxyz@xxx.com&#x27;; index1 只需要回主索引取一次数据即可，而 index2 则需要回主索引取 4 次数据。 上述现象表明使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。 通过使用 1select count(distinct left(email, &#123;len&#125;) as L from SUser; 可以查看前缀长度为 len 时，区分度的相对大小。 另外，由于前缀索引并不能准确判断查询条件是否满足，因此必须要回表，也就是说不能使用覆盖索引这一优化了。 对于那些前缀区分度不大的字段，可以使用下列方法： 使用倒序索引：每次需要调用 reverse 函数，性能不太稳定，因为还是基于前缀比较 使用 hash 字段：需要增加一个字段，每次需要调用 crc32 函数，性能更加稳定。InnoDB 不支持哈希索引，可以创建自适应哈希索引 注意，上述两种方法都不支持范围查询。 12 为什么我的 MySQL 会“抖”一下一条 SQL 语句，通常都执行很快，但有时却变得特别慢，并且这样的现象很难复现。该现象可能是因为刷脏页造成的。 具体有以下几种情形会引发数据库 flush 操作： redo log 写满了，此时系统会停止所有更新操作，推进 checkpoint 系统的内存不足，当需要一些新的数据页的时候，就需要淘汰一些数据页，空出内存。如果淘汰页是脏页，需要 flush 操作 MySQL 在空闲的时候执行 flush 操作 MySQL 正常关闭时，需要对内存中所有的脏页进行 flush 操作 上述前两种情况可能都会对性能产生较为严重的影响，可以通过一些参数来进行控制： innodb_io_capacity：可以在全力刷脏页的时候提供参考 innodb_max_dirty_pages_pct（M），当前写入的序号和 checkpoint 之间的差值（N）：算得的 F1(M) 和 F2(N) 两个值，取其中较大的值记为 R，之后引擎就可以按照 innodb_io_capacity 定义的能力乘以 R% 来控制刷脏页的速度 innodb_flush_neighbors：能减少很多随机 IO，在 HDD 时代能大幅提升性能，在 MySQL 8.0 后，默认为 0 另外，redo log 设置得太小会导致 write pos 很快就追上 cp，造成虽然磁盘压力很小，但是数据库出现间歇性的性能下跌的现象。 13 为什么表数据删掉一半，表文件大小不变一个 InnoDB 表的数据可以分为两个部分：表结构定义（.frm）和表数据（.idb）。而表数据既可以存放在当前数据库共享表空间中，也可以存储在单独的文件中，该行为模式可以通过innodb_file_per_table控制。在 MySQL 5.6 之后，默认值为 ON，以下讨论基于该情况。 对于表数据的删除，实际上只是进行了删除标记而已： 如果删掉 R4 记录，引擎只会将其标记为删除，并且如果下次需要插入 300-600 之间的数据，就可以复用该空间 如果删除数据页 A 上的所有数据，整个数据页可以被复用，并且可以在任何范围内复用 如果 delete 命令删除整个库，所有的数据页都会被标记为可复用，但是磁盘上，文件不会变小 上述这种可以复用的空间称为空洞，不仅删除数据会造成，插入数据也会造成，可以参考二叉树的分裂。 为了解决上述空洞问题，可以重建表：alter table A engine=InnoDB。 在MySQL 5.5版本之前，tmp 是临时表，自动创建，该过程中，不能对 A 进行更新操作。 而在 5.6 之后，支持 Online DDL，tmp-file 是引擎自己生成的，通过 row log 实现了数据更新。alter 语句会在启动时获取 MDL 写锁，但是这个写锁在真正数据拷贝之前就退化成读锁了。 copy 和 inplace：在 5.6 之后，tmp-file 是在存储引擎层生成的，对于 Server 层是透明的，因此是一个 in-place 操作；而在 5.6 之前，采用的则是 copy 方式，Server 层创建了临时表。DDL 过程如果是 Online 的，则一定是 inplace 的，反之不成立。 14 count(*) 这么慢，我该怎么办count(*) 在不同的存储引擎中有不同的实现方式： MyISAM 把一个表的总行数存储在磁盘上，count(*) 将直接返回该数据，效率高 InnoDB 需要把数据一行一行从引擎里面读出来，然后累积计数 上面的 count(*) 是没有过滤条件的，如果有的话 MyISAM 的效率没有这么高。为什么 InnoDB 不采用 MyISAM 的方案，将表的行数存储起来？这是因为即使在同一时刻，由于 MVCC 原因，InnoDB 应该返回多少行是不确定的，如下述情形： 对于 count(*) 这样的操作，遍历任何索引树的结果逻辑上都是一样的，因此，MySQL 会找到最小的那颗树来遍历。 对于命令 show table status 中的 ROW_TABLE，该值是通过采样估计得来的，非准确值。 为了提高 InnoDB 中 count(*) 操作的效率，我们可以 使用缓存系统保存计数：如使用 Redis，但是可能会在异常情况下丢失更新，即使不丢失更新，这个值在逻辑上也是不准确的： 在 T3 时刻，会显示出 R 新记录，但是 Redis 计数还未加一。如果改变会话一的顺序： 在 T3 时刻，Redis 计数还已经加一，但是还未显示出 R 新记录。 在数据库中保存：可以解决崩溃丢失的问题，利用事务可以确保真实记录数和计数值相匹配。 从并发系统性能角度看，先插入数据，后计数值加一，这样可提高性能，因为减少了写锁的持有时间。 count(?) 操作实际执行情况： count(主键id)：InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。 count(1)：InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字 “1” 进去，判断是不可能为空的，按行累加。 count(字段)：如果定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；如果定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。 count(*)：并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。 执行效率：count(字段) &lt; count(主键id) &lt; count(1) ≈ count(*)。 16 orderby 是怎么工作的假设存在表 t(id, city, name, age, adrr), 主索引是 id，二级索引是 city。现有如下需求： 1select city,name,age from t where city=&#x27;杭州&#x27; order by name limit 1000; 全字段排序：使用 explain 命令查看该命令执行情况： 其中 Using filesort 表示需要排序，MySQL 会给每个线程分配一块内存用于排序，称为 sort_buffer。执行流程如下： 如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序（归并排序）。 RowID 排序：如果返回的查询字段很多的话，那么 sort_buffer 里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。这时只会将要排序的字段和对应的 RowID 放入 sort_buffer 中。执行情况如下： 排序之后还需要再额外回表，会造成性能损失。 对于 MySQL 来讲，如果内存够，就要多利用内存，尽量减少磁盘访问。对于 InnoDB 表来说，rowid 排序会要求回表多造成磁盘读，因此不会被优先选择。 创建索引：如果创建了 city_user(city, name) 的联合索引，那么上述语句就不需要排序了，对应执行流程： 上述索引还是会有回表的代价，实际上，对于上述查询语句，我们可以创建覆盖索引 city_user_age(city, name, age)。当使用该索引后，不需要回表： 当然，维护索引也是代价的，需要综合考虑。 问题：假设表中已经有了 city_name(city, name) 联合索引，如果有下述语句： 1select * from t where city in (&#x27;杭州&#x27;,&quot;苏州&quot;) order by name limit 100; 该业务过程是否有排序过程？如何实现在数据库端不需要排序的方案？ 有排序过程，两个城市之间的 name 并不满足递增关系；可以分两次执行下列语句： 12select * from t where city=“杭州” order by name limit 100;select * from t where city=“苏州” order by name limit 100; 然后在业务层使用归并排序即可。 17 如何正确地显示随机消息假设存在表 words(id, word)，现在需要从其中随机选择 3 个单词，可以采用如下方式： 1select word from words order by rand() limit 3; 对应的 explain 命令执行情况如下： 在 Extra 字段中可以看到需要使用临时表，并且需要执行排序操作。 在上一节中，对于InnoDB表来说，执行全字段排序会减少磁盘访问，因此会被优先选择。但是对于内存表，回表过程只是简单地根据数据行的位置，直接访问内存得到数据，根本不会导致多访问磁盘。此时 MySQL 就会选择 rowid 排序。 上面的命令对应的执行情况如下： 上面的 R 就是 random 产生的小数，W 是对应的 word。pos 实际上就是内存临时表中的每行记录的位置。 在 MySQL 中，每个引擎提供了唯一标识数据行的信息： 对于有主键的 InnoDB 表来说，这个 rowid 就是主键 ID 对于没有主键的 InnoDB 表来说，这个 rowid 就是由系统生成的 MEMORY 引擎不是索引组织表，rowid 实际上就是数组的下标 到这里，order by rand() 使用了内存临时表，内存临时表排序的时候使用了 rowid 排序方法。 如果内存临时表的大小超过了 tmp_table_size，那么内存临时表就会转变为磁盘临时表，磁盘临时表使用的引擎默认是 InnoDB。 此时采用的排序方式实际上是优先队列排序算法，而不是归并排序。对应执行流程： 在上节中，语句 1select city,name,age from t where city=&#x27;杭州&#x27; order by name limit 1000 ; 执行时，并没有采用优先队列算法，这是因为 1000 行的 （name, rowid），超过了 sort_buffer_size 的大小，只能用归并排序。 为了随机获取 3 个单词，需要对整个表进行排序，代价太大，可以使用以下代码序列： 1234567select count(*) into @C from t;set @Y1 = floor(@C * rand());set @Y2 = floor(@C * rand());set @Y3 = floor(@C * rand());select * from t limit @Y1，1； //在应用代码里面取Y1、Y2、Y3值，拼出SQL后执行select * from t limit @Y2，1；select * from t limit @Y3，1； 上述代码总的扫描行数是 C+(Y1+1)+(Y2+1)+(Y3+1)，实际上还可以继续优化： 12select id from t limit Ymin，(Ymax - Ymin);select * from t where id in (id1, id2, id3); 这样扫描的行数是 C + Ymax + 3。 18 为什么这些 SQL 语句逻辑相同，性能却差异巨大假设存在如下表： 123456789mysql&gt; CREATE TABLE `tradelog` ( `id` int(11) NOT NULL, `tradeid` varchar(32) DEFAULT NULL, `operator` int(11) DEFAULT NULL, `t_modified` datetime DEFAULT NULL, PRIMARY KEY (`id`), KEY `tradeid` (`tradeid`), KEY `t_modified` (`t_modified`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 现在有如下需求语句： 1select count(*) from tradelog where month(t_modified)=7; 该语句并不会用到 t_modified 的树搜索功能，因为 t_modified 并不是按照 month 排序的。注意 MySQL 只是不用该索引的树搜索功能，还是会用到该索引，只是用该索引进行遍历而已。 对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。注意，即使有些函数不改变有序性，MySQL 仍然不会使用索引搜索功能，如： 1select * from tradelog where id + 1 = 10000; 另外一个需求： 1select * from tradelog where tradeid=110717; 在MySQL中，字符串和数字做比较的话，是将字符串转换成数字。上述语句等价于： 1select * from tradelog where CAST(tradid AS signed int) = 110717; 该语句同样会遍历索引。 如果还存在以下表： 12345678mysql&gt; CREATE TABLE `trade_detail` ( `id` int(11) NOT NULL, `tradeid` varchar(32) DEFAULT NULL, `trade_step` int(11) DEFAULT NULL, /*操作步骤*/ `step_info` varchar(32) DEFAULT NULL, /*步骤信息*/ PRIMARY KEY (`id`), KEY `tradeid` (`tradeid`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 且存在以下查询语句： 1select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2; /*语句Q1*/ 该语句执行流程如下： 第3步，是根据 tradeid 值到 trade_detail 表中遍历查找条件匹配的行。 该现象产生的原因是 tradeid 使用了不同的编码：utf8 和 utf8mb4。改写为以下语句即可： 12select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2; 总结而言，索引字段不能做函数操作，但是可以对索引字段的参数进行函数操作。 19 为什么我只查一行的语句，也执行这么慢假设存在如下表： 12345mysql&gt; CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB; 该表中有 100000 条数据，且每条数据 id 和 c 相等。 查询长时间不返回，如下列语句： 1select * from t where id=1; 等待 MDL 锁： 等待 flush： 等待行锁 一致性读 上述语句中 select * from t where id=1 会执行回退操作，以查找适合的版本，而第二个 select 语句只要读到最新值即可。 上图中 -1 表示将前面版本的值减去 1 。 问题：对于下列语句，是怎么加锁的，又是什么时候释放的？ 123begin;select * from t where c=5 for update;commit; 在 Read Committed 隔离级别下，会锁上聚簇索引中的所有记录；在 Repeatable Read 隔离级别下，会锁上聚簇索引中的所有记录，并且会锁上聚簇索引内的所有 GAP； 在上面两个隔离级别的情况下，如果设置了 innodb_locks_unsafe_for_binlog 开启 semi-consistent read 的话，对于不满足查询条件的记录，MySQL 会提前放锁，不过加锁的过程是不可避免的。对于 c &#x3D; 5 这一行的行锁，还是会等到 commit 的时候才释放的。 20 幻读是什么，幻读有什么问题假设存在如下表： 12345678910CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`)) ENGINE=InnoDB;insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25); 如果执行下列语句： 123begin;select * from t where d=5 for update;commit; 如果只在 id &#x3D; 5 这一行加锁，其他行不加锁的话，则存在以下场景： 即 Session A 发生了幻读，并且还违背了 Session A 对所有 d&#x3D;5 行加锁的语义。 如果把扫描过程中碰到的行，也都加上写锁，再来看看执行效果： 此时，虽然可以防止 Session B 对数据的更新，但是还是不能防止幻读现象。 由此，引入间隙锁（Gap Lock），和间隙锁存在冲突的，是往这个间隙中插入一个记录的操作，间隙锁之间并不存在冲突，如下图，Session A 和 Session B 之间不会存在冲突： 间隙锁的引入，会带来一些新的问题，可能会导致语句锁住更大的范围，影响并发度。 间隙锁是在可重复读级别下才会生效的，如果设置隔离级别为读提交的话，就没有间隙锁了。但是这样的话需要把 binlog 设置为 row，这也是不少公司使用的配置组合。 21 为什么我只改一行的语句，锁这么多本章引入 next-key lock 概念：该锁实际上是由 GAP 锁和行锁构成的，锁表示为前开后闭区间。加锁单位是 next-key lock，但是具体执行的时候，分成间隙锁和行锁两段来执行。本章中隔离级别是 RR。 加锁规则如下： 原则 1：加锁的基本单位是 next-key lock 原则 2：查找过程中访问到的对象才会加锁 优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁 优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁 特例：唯一索引上的范围查询会访问到不满足条件的第一个值为止 初始化表： 12345678910CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`)) ENGINE=InnoDB;insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25); 等值查询间隙锁 加锁范围是（5，10） 非唯一索引等值锁 加锁范围是（0，10），lock in share mode 只会在 c 索引上加锁（原则2），如果是 for update，那么对应主键索引上满足的行加上行锁 主键索引范围锁 加锁范围是 [10，15] 非唯一索引范围锁 加锁范围是 （5，15] 唯一索引范围锁特例 加锁范围是 （10，20] 非唯一索引上存在”等值”的例子：插入(30,10,30)，索引 c 如下： 加锁范围： limit 语句加锁 加锁范围： 死锁 A 加锁范围 （5，15），B 加锁 （5，10]，此时 B 已经加了 GAP 锁，但是还未加行锁，进入锁等待。 问题：解释下列现象产生的原因： 由于是 desc，加锁顺序从大到小，A 在索引 c 上加锁（5，25），在主键上加行锁 10，15 和 20。 22 MySQL 有哪些“饮鸩止渴”提高性能的方法短连接风暴：如果使用的是短连接，在业务高峰期的时候， 可能出现连接数突然暴增的状态。有以下方法： 先处理掉那些占着连接但是不工作的线程，即 kill 掉 sleep 状态的会话 减少连接过程的消耗，可以让连接跳过权限检验阶段 增加 max_connections 参数，但是可能会让 CPU 浪费在权限验证等逻辑上 慢查询性能问题： 索引没有设计好：先在备库上创建相应索引，然后主备切换，接着为原来主库加上索引 语句没写好：可以创建重写规则： 1234insert into query_rewrite.rewrite_rules(pattern, replacement, pattern_database) values (&quot;select * from t where id + 1 = ?&quot;, &quot;select * from t where id = ? - 1&quot;, &quot;db1&quot;);call query_rewrite.flush_rewrite_rules(); MySQL 选错了索引：通过给查询语句加上 force index QPS 突增问题： 由全新业务的 bug 导致：从数据库白名单删去 新功能使用的是单独的数据库用户：删除用户帐号 新增的功能跟主体功能是部署在一起的：使用查询重写，把压力最大的 SQL 语句重写为 select 1 返回 23 MySQL 是怎么保证数据不丢的只要 redo log 和 binlog 保证持久化到磁盘，就能确保 MySQL 异常重启后，数据可以恢复。 binlog 写入机制：事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。一个事务的 binlog 是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。 注意： 图中的 write，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，速度较快 图中的 fsync，才是将数据持久化到磁盘的操作 可以通过控制 sync_binlog 参数来控制 write 和 fsync 的时机： sync_binlog&#x3D;0 的时候，表示每次提交事务都只 write，不 fsync sync_binlog&#x3D;1 的时候，表示每次提交事务都会执行 fsync sync_binlog&#x3D;N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync 如果使用第三种方案，主机发生异常重启，会丢失最近 N 个事务的 binlog 日志。 redo log 写入机制：首先写入 redo log buffer，然后进行持久化。 为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数： 设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘 设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache InnoDB 有一个后台进程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的page cache，然后调用 fsync 持久化到磁盘。 存在以下几种场景，让一个没有提交的事务的 redo log 写入到磁盘中： 后台线程定时刷入 redo log redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘 并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘 组提交机制：日志逻辑序列号（LSN）单调递增，用来对应 redo log 的写入点，当某个事务提交时，其会将 LSN 之前的数据持久化。 由此可以得到，一次组提交里面，组员越多，节约磁盘 IOPS 的效果越好。并发更新场景下，第一个事务写完 redo log buffer 以后，接下来这个 fsync 越晚调用，组员可能越多，节约 IOPS 的效果就越好。 在两阶段提交流程中： MySQL 会延迟 fsync 调用时机，以此希望组员占多数： 如果想要提升 binlog 组提交的效果，可以通过设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 来实现。 综上，WAL 机制主要得益于两个方面： redo log 和 binlog 都是顺序写，磁盘的顺序写比随机写速度要快； 组提交机制，可以大幅度降低磁盘的 IOPS 消耗 24 MySQL 是怎么保证主备一致的binlog 既可以用来归档，也可以用来做主备同步。MySQL 的高可用架构已经呈现出越来越复杂的趋势，但都是从最基本的一主一备演化过来的，备库通过执行 binlog，实现和主库数据的一致性。 一主一备部署： 上图中，虽然备库没有被直接访问，但是还是建议将其设置为只读模式： 有时候一些运营类的查询语句会被放到备库上去查，设置为只读可以防止误操作 防止切换逻辑有 bug，比如切换过程中出现双写，造成主备不一致 可以用 readonly 状态，来判断节点的角色 设置为只读模式对于超级用户是无效的，而用于同步更新的线程，就拥有超级权限。主备同步流程图如下： 其中，io_thread 用于和主库建立长连接，sql_thread 则用于读取中转日志（relay log），解析出其中的指令，并且执行。MySQL 后来由于多线程复制方案的引入，sql_thread 演化成为了多个线程。 binlog 格式对比： statement：对应的实际上就是用户输入的 sql 语句，但是可能带来主备数据不一致的情况，如 delete 语句带 limit，主库和从库使用不同的索引导致删除的数据不同 row：记录的是真实的命令和对应的数据，如果 delete 语句带 limit，会产生类似 delete &lt;some-id&gt; 格式的语句，不会产生不一致的状况。另外，使用 row 格式可以更容易的恢复数据，因为对于 insert，delete 还是 update 语句都会把修改的行记录下来 mixed：row 格式很占用空间，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式 在 mixed 格式下，下列语句会采用 statement 格式存储，只不过在传输给从库的过程中，会加入 SET TIMESTAMP 命令，从而确保数据的一致性。 1insert into t values(10,10, now()); 因此，用 mysqlbinlog 解析出日志，然后把里面的 statement 语句直接拷贝出来执行，这个方法是有风险的。因为有些语句的执行结果是依赖于上下文命令的，直接执行的结果很可能是错误的。 双主结构：节点 A 和节点 B 互为主备关系，这样在切换的时候就不用再修改主备关系。 可以通过在 binlog 中插入 server_id 来判断该 binlog 是由谁产生的，从而解决循环复制问题。 但是如果使用命令 set global server_id 修改 server_id，或者下图三节点情况，会引发循环复制问题， 25 MySQL 是怎么保证高可用的正常情况下，只要主库执行更新生成的所有 binlog，都可以传到备库并被正确地执行，备库就能达到跟主库一致的状态，这就是最终一致性。但是，MySQL 要提供高可用能力，只有最终一致性是不够的。 主备延迟：主库执行完事务并生成 binlog，到备库执行完该事务的延迟即为主备延迟。备库会维护一个 seconds_behind_master 表示备库相较于主库，延迟了多少。主备延迟最直接的表现是，备库消费中转日志（relay log）的速度，比主库生产 binlog 的速度要慢。 主备延迟来源： 备库所在机器的性能要比主库所在的机器性能差，比较少见，一般采用对称部署 备库的压力大，备库可以提供一些读能力，可能在执行一些 OLAP 事务，影响了同步速度，可以采用一主多从 大事务执行时间长，间接造成主备延迟增大，如不要一次性地用 delete 语句删除太多数据 备库的并行复制能力 由于主备延迟的存在，所以在主备切换的时候，就相应的有不同的策略： 可靠性优先策略：缺点是存在不可用时间段，优点在于维护了一致性 可用性优先策略：不等主备数据同步，直接把连接切到备库 B，并且让备库B可以读写，那么系统几乎就没有不可用时间，但是引入了数据不一致，假设存在如下表： 1234567mysql&gt; CREATE TABLE `t` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `c` int(11) unsigned DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB;insert into t(c) values(1),(2),(3); 当 binlog_format&#x3D;mixed 时，存在如下不一致状态： 当 binlog_format&#x3D;row 时，存在如下不一致状态： 问题：在监控系统中，可能会看到以下监控图像，可能是什么原因造成的？ 一种可能是大事务，另外一种是备库启动了一个长事务，主库操作被阻塞住了。 26 备库为什么会延迟好几个小时如果备库执行日志的速度持续低于主库生成日志的速度，那这个延迟就有可能成了小时级别，为此备库引入了并行复制能力。 备库多线程复制模型： 此时，coordinator 就是原来的 sql_thread, 不过现在它不再直接更新数据了，只负责读取中转日志和分发事务。真正更新日志的，变成了 worker 线程。而 worker 线程的个数，就是由参数slave_parallel_workers 决定的。 由于 CPU 调度，分发顺序可能和完成顺序不同，因此，在 coordinator 分发任务的时候，需要满足： 不能造成更新覆盖。这就要求更新同一行的两个事务，必须被分发到同一个 worker 中 同一个事务不能被拆开，必须放到同一个 worker 中 MySQL 5.6 的并行复制策略：支持粒度是按库并行，将不同的 DB 里面的事务分发给不同的 worker，需要平衡好各个 DB 的访问。 MySQL 5.7 的并行复制策略：提供了 slave-parallel-type 参数用来控制复制策略： DATABASE：使用按库并行策略 LOGICAL_CLOCK：可以通过 binlog-transaction-dependency-tracking 参数来设置： COMMIT_ORDER：同时处于 redo log prepare 状态的事务，在备库执行时是可以并行的；处于 prepare 状态的事务和处于 commit 状态的事务之间，在备库执行的时候也是可以并行的，根据同时进入 prepare 和 commit 来判断是否可以并行的策略 WRITESET：如果两个事务没有操作相同的行，也就是说它们的 writeset 没有交集，就可以并行 WRITESET_SESSION，是在 WRITESET 的基础上多了一个约束，即在主库上同一个线程先后执行的两个事务，在备库执行的时候，要保证相同的先后顺序 27 主库出问题了，从库怎么办互联网应用场景是读多写少，数据库架构首先可能面临的是读性能的问题，可以采用一主多从架构来缓解该情况： 其中 A 和 A‘ 互为主备，BCD 是A 的从库。在主库 A 发生故障后，BCD 需要修改 master 为 A’ 节点。 基于位点的主备切换：当需要修改 B 设置为 A‘ 从库的时候，需要执行 change master 命令： 1234567CHANGE MASTER TO MASTER_HOST=$host_name MASTER_PORT=$port MASTER_USER=$user_name MASTER_PASSWORD=$password MASTER_LOG_FILE=$master_log_name MASTER_LOG_POS=$master_log_pos 其中，MASTER_LOG_FILE 和 MASTER_LOG_POS 表示需要从主库的 master_log_name 文件的master_log_pos 这个位置的日志继续同步，这个位置就是我们所说的同步位点。 一种获取同步位点的方法是这样的： 等待新主库 A’ 把中转日志（relay log）全部同步完成； 在 A’ 上执行 show master status 命令，得到当前 A’ 上最新的 File 和 Position； 取原主库 A 故障的时刻 T； 用 mysqlbinlog 工具解析 A’ 的 File，得到 T 时刻的位点。 这个位点并不精确，因为 A 故障的时刻 T 可能也已经将 binglog 传给 A‘ 和 BCD 了，如果用上述同步位点，可能会造成主键冲突等错误，为此需要先主动跳过这些错误： 主动跳过一个事务：set global sql_slave_skip_counter=1; 设置跳过指定的错误：ste slave_skip_errors = &quot;1032,1062&quot;，分别表示跳过唯一键冲突和删除找不到指定行 基于 GTID 的主备切换：GTID 是全局全局事务 ID，被定义为GTID=source_id:transaction_id，可以通过 session 变量 gtid_next 来为提交的事务分配 ID。在该模式下，当需要修改 B 设置为 A‘ 从库的时候，需要执行 change master 命令： 123456CHANGE MASTER TO MASTER_HOST=$host_name MASTER_PORT=$port MASTER_USER=$user_name MASTER_PASSWORD=$password master_auto_position=1 最后一行表示使用 GTID 协议，这样我们就无需指定同步位点了。假设在该时刻下，实例 A’ 的 GTID 集合记为 set_a，实例 B 的 GTID 集合记为 set_b，对应的切换逻辑： 实例 B 指定主库 A’，基于主备协议建立连接。 实例 B 把 set_b 发给主库 A’。 实例 A’ 算出 set_a 与 set_b 的差集，也就是所有存在于 set_a，但是不存在于 set_b 的 GITD 的集合，判断 A’ 本地是否包含了这个差集需要的所有 binlog 事务。a. 如果不包含，表示 A’ 已经把实例 B 需要的 binlog 给删掉了，直接返回错误；b. 如果确认全部包含，A’ 从自己的 binlog 文件里面，找出第一个不在 set_b 的事务，发给 B； 之后就从这个事务开始，往后读文件，按顺序取 binlog 发给 B 去执行。 28 读写分离有哪些坑在上一节中已经提到了一主多从的结构，该结构中是客户端主动做均衡负载，需要将数据库的连接信息放在客户端的连接层。 另外一种架构是带有 proxy 服务器的架构，如下图。 客户端直连方案，少一层 proxy 转发，所以查询性能稍微好一些，并且架构简单；带 proxy 服务器对客户端友好，客户端不需要关注后端细节，但是 proxy 容易成为瓶颈。 不论采用那种架构，客户端先在主库上写，后在从库上查询，很有可能查询不到自己刚刚写入的，这是由主从延迟决定的。一般，有以下方法解决这种过期读的问题： 强制走主库方案：对于必须要拿到最新结果的请求，强制将其发到主库上；对于可以读到旧数据的请求，才将其发到从库上 Sleep 方案：主库更新后，读从库之前先 sleep 一下，这个方案的假设是，大多数情况下主备延迟在 1 秒之内，做一个 sleep(1s) 可以有很大概率拿到最新的数据 判断主备无延迟方案： 每次执行前，查看 seconds_behind_master 是否为 0，不为 0 则等待 对比位点确保主备无延迟，判断读到的主库的最新位点和备库执行的最新位点是否相等 对比 GTID 集合确保主备无延迟，对比备库收到的所有日志的 GTID 集合和备库所有已经执行完成的 GTID 集合是否相等，仍然可能不精确 配合 semi-sync（半同步复制）： 事务提交的时候，主库把 binlog 发给从库； 从库收到 binlog 以后，发回给主库一个 ack，表示收到了； 主库收到这个 ack 以后，才能给客户端返回“事务完成”的确认。 等主库位点方案：select master_pos_wait(file, pos[, timeout]);需要客户端的主动参与 等 GTID 方案：和等主库位点类似，select wait_for_executed_gtid_set(gtid_set, 1);，需要客户端主动参与 29 如何判断一个数据库是不是出问题了在一主一备的双 M 架构里，主备切换只需要把客户端流量切到备库；而在一主多从架构里，主备切换除了要把客户端流量切到备库外，还需要把从库接到新主库上。 主备切换有两种，一种是主动切换，一种是被动切换。而其中被动切换，往往是因为主库出问题了，由 HA 系统发起的。 如何判断一个主库出现问题了？ select 1 判断：select 1 成功返回，只能说明这个库的进程还在，并不能说明主库没问题，如果并发线程已经达到 innodb_thread_concurrency ，这时候系统已经不行了，但是 select 1 仍然能够返回 查表判断：定期执行 select * from mysql.health_check; ，可以检测出由于并发线程过多导致的数据库不可用的情况，但是不能检测出来因为 binlog 所在磁盘的空间占用率达到 100% 的情况 更新判断：定期执行 update mysql.health_check set t_modified=now(); ，为了防止主备更新错乱，可以修改表 health_check(server_id, t_modified)，但是可能由于 IO 瓶颈，导致判定慢的问题 内部统计：利用 performance_schema 库，但是对 MySQL 有性能损失 31 误删数据后除了跑路，还能怎么办MySQL 的高可用架构并不能预防误删数据，主库的 drop table 命令会传到从库，从而导致整个集群的实例都会执行这个命令。不同的误删数据存在不同的解决方案： 误删行数据：如果确实误删了行数据，可以使用 Flashback 工具将数据恢复过来，其原理是修改 binglog 的内容，拿回到原库重放，需要确保 binlog 是 row 类型。Flashback 修改过程如下： 对于 insert 语句，将其修改为 delete 语句 对于 delete 语句，将其修改为 insert 语句 而对于 update 语句，只需要调换修改前和修改后的记录即可 然后在从库里面进行重放，类似 git revert 操作，将事务产生的作用消除。 另外，也可以将 sql_safe_updates 参数设置为 on，这样一来，如果忘记在 delete 或者 update 语句中写 where 条件，或者 where 条件里面没有包含索引字段的话，这条语句的执行就会报错。 误删库&#x2F;表：这种情况下，要想恢复数据，就需要使用全量备份，加增量日志的方式了，需要注意增量日志中需要跳过误操作的语句，可以基于位点或者 GTID 方式跳过。该种方式较慢，因为 mysqlbinlog 工具并不能指定只解析一个表的日志，而且应用日志的过程是单线程。 一种优化的方案如下： 这时，可以用上并行复制技术，加快数据恢复过程。 上面两种方案都存在恢复时间不可控的问题，还有另外一种是延迟复制备库，通过 CHANGE MASTER TO MASTER_DELAY = N ，可以让备库和主库有 N 秒的延迟，当主库上误删后，可以从延迟复制备库上执行 stop slave ，然后跳过误删操作即可。 预防该问题的建议有帐号权限分离，规范操作流程。 rm 删除数据：只要集群上还有其他节点，就可以选出新的主库，并且后台恢复被删除数据，让故障节点后续上线即可。预防措施有跨机房或者城市保存备份。 32 为什么还有 kill 不掉的语句MySQL 中有两种 kill 命令，kill query thd_id 和 kill [connection] thd_id。前者用于终于这个线程中正在执行的语句，后者用于断开这个线程的连接，当然如果有语句正在执行，则先停止正在执行的语句。 在收到 kill query thread_id_B 命令后，处理 kill 命令线程实际上需要： 把 session B 的运行状态改成 THD::KILL_QUERY 给 session B 的执行线程发一个信号，用于帮助 B 跳出等待，来响应状态 THD::KILL_QUERY 如果 set global innodb_thread_concurrency&#x3D;2，然后执行该序列： 由于 C 在等待行锁的时候使用 pthread_cond_timedwait，其等待逻辑是每 10 毫秒判断一下是否可以进入InnoDB执行，如果不行，就调用 nanosleep 函数进入 sleep 状态。尽管发送了信号给 C，但是其只是判断能否跳出 sleep 阶段，最终没有执行对应的响应函数，从而导致 kill query 失效。 在收到 kill connection 命令时： 线程状态设置为 KILL_CONNECTION 关掉线程的网络连接 此时，如果调用 show processlist 时， C 的状态将是 killed 状态。 kill 无效的情况分为： 线程没有执行到判断线程状态的逻辑：如上述并发连接设置为 2 时的情形 终止逻辑耗时较长：如超大事务执行期间被 kill，需要回滚；大查询回滚；DDL 命令执行到最后阶段被 kill 关于客户端的误解： 如果库里面的表特别多，连接就会很慢：当使用默认参数连接的时候，MySQL 客户端会提供一个本地库名和表名补全的功能，该功能在表很多的时候就会耗时，这是客户端慢，可以加参数 -A 关闭补全 -quick 参数：跳过自动补全；不缓存，服务器发送一个响应，客户端处理一个，而不是先缓存服务器所有响应结果；不会把执行命令记录到本地的命令历史文件 33 我查这么多数据，会不会把数据库内存打爆对于全表查询，MySQL 服务端并不是从引擎层取完全部数据后再发送给客户端的，而是采用边读边发策略，其流程如下： 获取一行，写到 net_buffer 中 重复获取行，直到 net_buffer 写满，调用网络接口发出去 如果发送成功，就清空 net_buffer，然后继续取下一行，并写入 net_buffer 如果发送函数返回 EAGAIN 或 WSAEWOULDBLOCK，就表示本地网络栈（socket send buffer）写满了，进入等待，直到网络栈重新可写，再继续发送 如果客户端故意不读取 socket receive buffer 中的内容，show processlist 命令就会显示服务器处于 sending to client 状态，表示服务器端的网络栈写满了。 如果状态是 Sending data，则表示服务器端处于执行器过程中的任意阶段。 InnoDB 采用 Buffer Pool 来进行查询加速，并且逐出策略采用的是 LRU，但是，InnoDB 对原始 LRU 算法进行了改造，以防止全表查询对 Buffer Pool 的污染： 在 InnoDB 实现上，按照 5:3 的比例把整个 LRU 链表分成了 young 区域和 old 区域。图中 LRU_old 指向的就是 old 区域的第一个位置，是整个链表的 5&#x2F;8 处。也就是说，靠近链表头部的 5&#x2F;8 是 young 区域，靠近链表尾部的 3&#x2F;8 是old 区域。 插入过程还是先从 tail 删除数据，但是新的数据页会被放到 LRU_old 指向的位置 对于 old 区域中的数据页，如果其存在时间长于 1s，位置保持不变，否则将其移动到 head 位置 对于全表查询，数据页读入后顺序访问，基本上不会超过 1s，从而缓存页逐出，加入的区域是限制在 old 区域的，不会影响命中率 类似于 JVM GC 中分代回收机制，把数据分成新生代和老年代，一个用于存储短时间内就会被清理的对象，一个用于存储存活时间长的对象，只不过 JVM 里面把短时间内被清除的区域叫做”young“，InnoDB 里面却是叫做”old”。 34 到底可不可以使用 join假设存在以下表和对应的数据： 1234567891011121314151617181920212223CREATE TABLE `t2` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`)) ENGINE=InnoDB;delimiter ;;create procedure idata()begin declare i int; set i=1; while(i&lt;=1000)do insert into t2 values(i, i, i); set i=i+1; end while;end;;delimiter ;call idata();create table t1 like t2;insert into t1 (select * from t2 where id&lt;=100) Index Nested-Loop Join（NLJ）：下列语句使用 t1 作为驱动表，执行器对表 t1 进行全表扫描，每次取出数据后对表 t2 进行树搜索，然后合并查询结果。驱动表选择小表的时间成本更低。 1select * from t1 straight_join t2 on (t1.a=t2.a); Simple Nested-Loop Join：下列语句由于 t2 上面不存在 b 索引，每次从 t1 上取出一行的时候，都需要对 t2 进行一次全表扫描，太笨重，MySQL 中使用 Block Nested-Loop Join。 1select * from t1 straight_join t2 on (t1.a=t2.b); Block Nested-Loop Join（BNL）：不再是每次取出 t1 中一行数据的时候，都对 t2 进行一次全表扫描，而是先将驱动表数据放入 join_buffer 中，然后遍历 t2 以查找满足条件的结果。对应流程： 如果 join_buffer 太小不足以容纳 t1 全部数据，则分批加载 t1 的数据到 join buffer 中，重复上述过程即可。此时，应该选择小表当驱动表。 小表指代的是将两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。 12select * from t1 straight_join t2 on (t1.b=t2.b) where t2.id&lt;=50;select * from t2 straight_join t1 on (t1.b=t2.b) where t2.id&lt;=50; 应该选择 t2 作为驱动表，第二条语句效率更高。 12select t1.b,t2.* from t1 straight_join t2 on (t1.b=t2.b) where t2.id&lt;=100;select t1.b,t2.* from t2 straight_join t1 on (t1.b=t2.b) where t2.id&lt;=100; 由于 t1 只需要将 b 加载到 join buffer 中，t1 应该作为驱动表。 35 join 语句怎么优化Multi-Range Read（MRR）：该优化的主要目的是尽量使用顺序读盘，通过将回表的 id 值放入 read_rnd_buffer 中，然后将其排序，接着依次到主键 id 索引中查记录，并作为结果返回。该优化基于假设：大多数的数据都是按照主键递增顺序插入得到的，所以我们可以认为，如果按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。 Batched Key Access：该算法实际上是对 NLJ 算法的优化。NLJ 算法执行的逻辑是：从驱动表 t1，一行行地取出 a 的值，再到被驱动表 t2 去做 join。也就是说，对于表 t2 来说，每次都是匹配一个值。BKA 则是从表 t1 里一次性地多拿些行出来，一起传给表 t2。NLJ 应用 BKA 算法如下： BNL 算法的性能问题：使用 BNL 算法时，可能会对被驱动表做多次扫描，除了会造成 IO 压力大之外，还可能会影响 LRU 算法的命中率，因为多次访问被驱动表会将相应的页移动到 young 区域，另一方面，如果被驱动表很大，所有的页都在 old 区域中，这样后面访问的会将前面访问的页逐出，命中率下降。 BNL 算法的优化： 在被驱动表上面建立索引，以使其可以应用 BKA 算法 使用临时表，先过滤数据，后创建索引，最后执行 join 使用 Hash Join：BNL 需要进行大量比较，可以在 join_buffer 中创建对应的哈希表，这样就能大幅减少比较次数，当然也可以客户端实现该逻辑 36 为什么临时表可以重名临时表和内存表并不相同： 内存表：指的是使用 Memory 引擎的表，建表语法是 create table … engine&#x3D;memory。系统重启后表的数据会被清空，但是表的结构还在 临时表：可以使用各种引擎类型，有写到内存的，也有写到磁盘上的 临时表的特征： 建表语法：create temporary table table-name Session 可见：只能被创建它的 Session 访问 可以和普通表重名 Session 内同时存在同名的临时表和普通表的时候，show create 语句，以及增删改查语句访问的是临时表 show tables 不显示临时表 在 Session 结束的时候自动删除 临时表的应用：由于其可以和普通表重名，也不需要担心数据删除问题，其可以应用于分库分表的查询中： 对于 partition_key 上的查询，其性能较优，但是对于下列语句： 1select v from ht where k &gt;= M order by t_modified desc limit 100; 这时候，由于查询条件里面没有用到分区字段 f，只能到所有的分区中去查找满足条件的所有行，然后统一做 order by 的操作。有两种思路： 在 proxy 层的进程代码中实现排序：对中间层开发能力要求高，对 proxy 端压力较大 把各个分库拿到的数据，汇总到一个 MySQL 实例的一个表中，然后在这个汇总实例上做逻辑操作 可以将临时表放到分库之一上。 临时表重名区分机制： 临时表结构定义在临时目录下，前缀是#sql&#123;进程id&#125;_&#123;线程id&#125;_序列号 临时表数据存放在临时文件表空间，专门用来存放临时文件的数据，不需要额外创建 idb 文件 内存中会根据 table_def_key 区分，普通表由 库名+表名 组成，临时表则还增加了 server id + thread id 在实现上，每个线程都维护了自己的临时表链表。这样每次session内操作表的时候，先遍历链表，检查是否有这个名字的临时表，如果有就优先操作临时表，如果没有再操作普通表；在session结束的时候，对链表里的每个临时表，执行 “DROP TEMPORARY TABLE +表名”操作。 临时表和主备复制： binlog 格式为 row：跟临时表有关的语句，就不会记录到 binlog 里 binglog 格式为 statement&#x2F;mixed ：binlog 中才会记录临时表的操作，否则可能造成主备数据不一致 如果主库上创建了两个同名的临时表，那么备库则根据如下规则构建 table_def_key：库名 + 临时表名 + server_id(master) + thread_id(session) 临时表由于表结构文件存放于 tmpdir 下，执行 rename 会报错，因为其根据 库名/表名.frm 去磁盘查找文件。 37 什么时候会使用内部临时表假设存在以下表和数据： 1234567891011121314create table t1(id int primary key, a int, b int, index(a));delimiter ;;create procedure idata()begin declare i int; set i=1; while(i&lt;=1000)do insert into t1 values(i, i, i); set i=i+1; end while;end;;delimiter ;call idata(); union 执行流程： 1(select 1000 as f) union (select id from t1 order by id desc limit 2); 以上语句执行过程将会使用到临时表（Using temporary），临时表主键是 f，可以用于唯一性约束： 如果上面查询使用的 union all ，没有去重的语义，这时就不需要临时表了，因为执行器会依次执行子查询，得到的结果直接作为结果集的一部分，发给客户端。 group by 执行流程： 1select id%10 as m, count(*) as c from t1 group by m; 以上语句将会使用到临时表(m, c)，其中 m 表示 id%10，c 表示 count，每次遍历叶子节点的时候插入临时表中，最后将会排序并且返回结果： 如果不需要对结果进行排序，可以添加 order by null 子句，这样就能跳过排序阶段，直接返回临时表中的数据。 上面用到的是内存临时表，对应大小参数 tmp_table_size ，如果在执行那个过程中发现内存临时表空间不足，这时就会转换为磁盘临时表（默认使用 InnoDB 引擎），对性能有损失。 group by 优化方法： 索引：如果 group by 的对象在扫描过程中已经有序了，那么该语句只要执行一次扫描就行了，可以通过建立 generated column 来实现列数据的关联更新： 1alter table t1 add column z int generated always as(id % 100), add index(z); 直接排序：如果数据量很大，超过了 tmp_table_size ，那么就需要建立对应的磁盘临时表，这个过程对性能有损失。可以用 SQL_BIG_RESULT 来告诉优化器，直接使用磁盘临时表，但是优化器觉得磁盘临时表是 B+ 树存储，存储效率不如数组来得高，将会直接使用数组来存储。 1select SQL_BIG_RESULT id%100 as m, count(*) as c from t1 group by m; 38 都说 InnoDB 好，那还要不要使用 Memory 引擎内存表的数据组织结构：假设存在以下数据： 1234create table t1(id int primary key, c int) engine=memory;create table t2(id int primary key, c int) engine=innodb;insert into t1 values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(0,0);insert into t2 values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(0,0); 上述现象的原因在于两个引擎的索引组织方式： InnoDB：把数据直接放在主键索引上，其他索引上保存的是主键id，索引组织表 Memory：把数据单独存放，索引上保存数据位置的数据组织形式，堆组织表 两种引擎不同点： InnoDB 表的数据总是有序存放的，而内存表的数据就是按照写入顺序存放的 当数据文件有空洞的时候，InnoDB 表在插入新数据的时候，为了保证数据有序性，只能在固定的位置写入新值，而内存表找到空位就可以插入新值； 数据位置发生变化的时候，InnoDB 表只需要修改主键索引，而内存表需要修改所有索引； InnoDB 表用主键索引查询时需要走一次索引查找，用普通索引查询的时候，需要走两次索引查找。而内存表没有这个区别，所有索引的“地位”都是相同的 InnoDB 支持变长数据类型，不同记录的长度可能不同；内存表不支持 Blob 和 Text 字段，并且即使定义了 varchar(N)，实际也当作 char(N)，也就是固定长度字符串来存储，因此内存表的每行数据长度相同。 hash 索引和 B-Tree 索引：实际上，内存表同样支持 B-Tree 索引： 1alter table t1 add index a_btree_index using btree (id); t1 的数据组织形式就变为这样： 可以同时快速支持点查询和范围查询。 不建议在生产环境上使用内存表的原因： 锁粒度：锁粒度是表锁，并发性能太差 数据持久化： 在 M-S 架构下，备库硬件升级，备库重启清空内存表，客户端这时候发起的 update 语句可能找不到要更新的行 双 M 架构：在备库重启的时候，备库 binlog 里的 delete 语句就会传到主库，然后把主库内存表的内容删除。这样你在使用的时候就会发现，主库的内存表数据突然被清空了。 如果确实需要内存表，备库又要重启，可以在重启前修改引擎为 InnoDB，在重启后修改引擎为 Memory；另外，在不会耗费过多内存的时候，可以使用内存临时表，因为临时表不会被其他线程访问，临时表重启后也是需要删除的，清空数据这个问题不存在。 39 自增主键为什么不是连续的假设存在以下表和数据： 1234567CREATE TABLE `t` ( `id` int(11) NOT NULL AUTO_INCREMENT, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `c` (`c`)) ENGINE=InnoDB; 自增值的保存：MyISAM 引擎的自增值保存在数据文件中；InnoDB 引擎的自增值，保存在内存里，并且到了 MySQL 8.0 版本后，才有了“自增值持久化”的能力，对于之前的版本，在重启后则需要查找 max(id)。 自增值修改机制：如果要插入的值大于等于当前自增值，新的自增值就是“准备插入的值+1”，否则，自增值不变。 自增值修改时机：在真正执行插入数据的操作之前。 自增值不连续的原因： 唯一键冲突：假设表中已经存在 (1, 1, 1)，如果再插入 （null, 1, 1），自增值先修改为 3，但是之后该插入因为 c 唯一键冲突 事务回滚： 123456insert into t values(null,1,1);begin;insert into t values(null,2,2);rollback;insert into t values(null,2,2);// 插入的行是(3,2,2) 自增值的回退会导致下一次分配需要做额外工作，如查询数据中是否存在该 id，或者修改修改自增 id 锁为事务级别，这样又会带来并发性能的下降。 批量插入数据：包含的语句类型是 insert … select、replace … select 和 load data 语句，这时会先分配 1 个自增值，2 个自增值，4 个自增值…最后一次插入如果没有用完也会被浪费掉。 123456789insert into t values(null, 1,1);insert into t values(null, 2,2);insert into t values(null, 3,3);insert into t values(null, 4,4);create table t2 like t;insert into t2(c,d) select c,d from t;// 对表t的所有记录和间隙加锁，否则主备数据可能不一致insert into t2 values(null, 5,5);// 插入的行是(8, 5, 5) 40 insert 语句的锁为什么这么多假设存在以下表和执行语句： 1234567891011121314CREATE TABLE `t` ( `id` int(11) NOT NULL AUTO_INCREMENT, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `c` (`c`)) ENGINE=InnoDB;insert into t values(null, 1,1);insert into t values(null, 2,2);insert into t values(null, 3,3);insert into t values(null, 4,4);create table t2 like t 假设在 RR 隔离级别下，binlog_format&#x3D;statement 时执行： 1insert into t2(c,d) select c,d from t; 这个语句需要对表 t 的所有行和间隙加锁，原因如下： 实际的执行效果是，如果 session B 先执行，由于这个语句对表 t 主键索引加了 $(-\\infty,1]$ 这个 next-key lock，会在语句执行完成后，才允许 session A 的 insert 语句执行。 但如果没有锁的话，就可能出现 session B 的 insert 语句先执行，但是后写入 binlog 的情况。binglog 情况如下： 12insert into t values(-1,-1,-1);insert into t2(c,d) select c,d from t; 这个语句到了备库执行的话，就会出现主备不一致。 按需加锁：并不是所有的 insert ... select 语句都会对目标表锁全表，而是只锁住需要访问的资源。 1insert into t2(c,d) (select c+1, d from t force index(c) order by c desc limit 1); 这个语句的加锁范围，就是表 t 索引 c 上的 $(4, \\infty]$ 这个 next-key lock 和主键索引上 id&#x3D;4 这一行。 唯一键冲突： session A 执行的 insert 语句，发生主键冲突的时候，并不只是简单地报错返回，还在冲突的索引上加了锁，持有索引 c 上的 (5,10] 共享 next-key lock（读锁）。 该过程加锁顺序如下，其加锁涉及到 next-lock 退化： insert into … on duplicate key update：插入一行数据，如果碰到唯一键约束，就执行后面的更新语句。 41 怎么最快地复制一张表insert ... select：在源表比较小的时候可以实现两表之间数据拷贝。 mysqldump：将数据导出成一组 INSERT 语句： 123mysqldump -h$host -P$port -u$user --add-locks=0 --no-create-info --single-transaction --set-gtid-purged=OFF db1 t --where=&quot;a&gt;900&quot; --result-file=/client_tmp/t.sql 之后，通过下列语句进行数据导入： 1mysql -h127.0.0.1 -P13000 -uroot db2 -e &quot;source /client_tmp/t.sql&quot; 导出 CSV 文件：下列语法将会直接将结果导出到服务端本地目录： 1select * from db1.t where a&gt;900 into outfile &#x27;/server_tmp/t.csv&#x27;; 之后，通过下列语句导入数据： 1load data infile &#x27;/server_tmp/t.csv&#x27; into table db2.t; 上述语句执行流程如下： 显然，load data 语句并不会引发主备不一致的状态。 另外，load data 不加 local 的话，是读取服务端的文件，加上 local，则是读取的是客户端的文件；上述方法并不会导出表结构文件，如果需要同时导出表结构定义文件和 csv 数据文件，可以借助 mysqldump 工具生成表结构： 12mysqldump -h$host -P$port -u$user ---single-transaction --set-gtid-purged=OFF db1 t --where=&quot;a&gt;900&quot; --tab=$secure_file_priv 物理拷贝方法： 需要注意，在第 3 步执行完 flush table 命令之后，db1.t 整个表处于只读状态，直到执行 unlock tables 命令后才释放读锁。 42 grant 之后要跟着 flush privileges 吗 grant 语句会同时修改数据表和内存，判断权限的时候使用的是内存数据，故不需要 flush privileges 命令。 由于全局权限会保存到连接线程对象中，之后在这个连接中执行的语句，所有关于全局权限的判断，都直接使用线程对象内部保存的权限位，因此，修改全局权限对已存在的连接不生效。 而在 db 权限，表权限和列权限中，由于执行语句时是访问对应内存存储，对所有连接立即生效。 另外，不使用标准的语法 grant 和 revoke 语句操作权限，而是使用 DML 操作系统权限表（不规范的操作），也需要 flush privileges 命令： 43 要不要使用分区表常见的分区方式有 range，hash 和 list 分区，下面的表中采用了 range 分区： 1234567891011CREATE TABLE `t` ( `ftime` datetime NOT NULL, `c` int(11) DEFAULT NULL, KEY (`ftime`)) ENGINE=InnoDB DEFAULT CHARSET=latin1PARTITION BY RANGE (YEAR(ftime))(PARTITION p_2017 VALUES LESS THAN (2017) ENGINE = InnoDB, PARTITION p_2018 VALUES LESS THAN (2018) ENGINE = InnoDB, PARTITION p_2019 VALUES LESS THAN (2019) ENGINE = InnoDB,PARTITION p_others VALUES LESS THAN MAXVALUE ENGINE = InnoDB);insert into t values(&#x27;2017-4-1&#x27;,1),(&#x27;2018-4-1&#x27;,1); 且在磁盘中存在以下文件： 对于引擎层来说，这是 4 个表，但是对于 Server 层来说，这是 1 个表。 引擎层行为： 如果是按照一个表来看的话，加锁范围是 （2017-4-1，2018-4-1），那么 B 的第一条语句应该被 block，从而反推出引擎层是将其当作 4 个表来看的。 Server 层行为： 如果按照多个表来看的话，只会获取 p_2018 的 MDL 锁，那么 B 的第一条语句应该执行成功，从而反推出 Server 层是将其当作 1 个表来看的。 使用分区表的注意点： MySQL 在第一次打开分区表的时候，需要访问所有的分区 在 Server 层，认为这是同一张表，因此所有分区共用同一个 MDL 锁 在引擎层，认为这是不同的表，因此 MDL 锁之后的执行过程，会根据分区表规则，只访问必要的分区 应用场景：对业务透明，很方便地清理历史数据","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.zsstrike.tech/tags/MySQL/"}]},{"title":"LeetCode Rush","slug":"LeetCode-Rush","date":"2021-07-12T14:36:59.000Z","updated":"2022-11-28T12:09:41.515Z","comments":true,"path":"2021/07/12/LeetCode-Rush/","link":"","permalink":"http://blog.zsstrike.tech/2021/07/12/LeetCode-Rush/","excerpt":"本文用于记录在算法题目中使用的技巧或者方法，以备查阅。","text":"本文用于记录在算法题目中使用的技巧或者方法，以备查阅。 链表问题 使用双指针可以解决找固定位置相关问题，环形链表问题，相交链表问题，旋转链表问题 尝试设置 dummyHead 节点，用于处理特殊情况 只知道要删除的节点，实现该节点的删除 头插法和尾插法的不同点，可以使用头插法实现链表的反转，也可以直接使用迭代方法 链表反转问题：全部反转（迭代，递归，头插法），部分反转（递归，迭代），k 个一组翻转（递归，迭代） 合并链表问题：合并两个链表，合并 k 个链表 判断回文链表：使用额外数据结构，反转后半部分的链表再进行比较，使用后序遍历来比较（链表只存在前序和后序） 二叉树问题 二叉树遍历分为 BFS（层次遍历），DFS（前序，中序，后序遍历） 在解决二叉树的问题过程中，通常和其递归遍历框架存在关系，而在编写递归算法中，最关键的是要明确函数的定义是什么，然后使用这个递归推到最终结果，而不要跳入到递归的细节中去 递归解决二叉树问题：左右翻转二叉树，二叉树展开为链表（先序），填充每个节点的下一个右侧节点指针（注意不同父节点下的关系），将数组构建为最大二叉树，二叉树最大深度，判断是否存在根节点到叶节点路径总和等于目标值，统计二叉树中某条路径（不要求从根节点开始，到叶子节点结束）和为目标值的总的数目 使用前序－中序构造二叉树问题，使用后序－中序构造二叉树问题：关键点在于找到根结点，然后递归构建左右子树 二叉树的最近公共祖先：维护每个节点的父节点，或者是使用递归查找（注意结束条件） 二叉树的序列化和反序列化：使用前序遍历，后序遍历或者层次遍历，不过也需要记录空节点，不可仅仅使用中序遍历 寻找重复的子树：关键在于如何将二叉树序列化起来，然后和已经存在的二叉树进行比较，看看是否重复 二叉搜索树的中序遍历结果是有序的，可以解决：BST 第 K 小的元素，BST 转化为累加树 二叉搜索树的基本操作：删除，插入，搜索，验证 BST（记录当前树范围） 递归解决二叉搜索树问题：不同的二叉搜索树数量（节点值 1 到 n），不同的二叉搜索树所有种类（节点值 1 到 n） 二叉树中二叉搜索子树的最大键值和：使用后序遍历可以减少时间复杂度，因为当前节点所做的事情依赖于左右子树，涉及到 BST 的验证 扁平化嵌套列表迭代器：将其当做是一个多叉树进行遍历保存，使用懒加载模式 完全二叉树的节点数：考虑左右二叉树是否为满二叉树，分解问题，减少时间复杂度 恢复二叉搜索树：中序遍历有序性，找出两个交换的节点；使用莫里斯遍历 字典序的第 K 小数字：构建一个 10 叉树，根据 cur，next 变量统计当前统计过的数字，和 k 进行比较 图问题 图中所有可能路径：涉及到图遍历框架，注意和回溯算法的区别，图遍历中 visited 不会重新被设置为 false 拓扑排序：使用 BFS 算法，构建入度数组，之后将入度为 0 的节点入队，使用 DFS 算法，后序遍历并且反转即为拓扑序 判断图中是否存在环：利用拓扑排序判断，或者是深度优先遍历，后者还可以获得当前的环节点 Dijkstra 算法：用于查找图中某个节点到其他所有节点的最短路径（不含有负权重），可使用优先队列实现贪心特性，实际上可以扩展成在图中求最值的算法，相关问题如网络延迟时间，概率最大的路径，最小体力消耗路径 找到最终的安全状态：找到所有路径通向终端节点的安全节点，可以使用深度优先搜索 + 三色标记法，也可以使用拓扑排序，先反转图，再使用入度统计信息 可能的二分法：给定一组节点，其中规定那些边不能存在，判断能否组成二分图，可以使用 dfs 进行着色，不能存在的边对应的两个端点是不同的颜色 数组和字符串问题 田忌赛马算法决策：比不过的时候选择最差的元素进行比较，如优势洗牌 二分搜索模板：基本的二分搜索，寻找左侧边界的二分搜索，寻找右侧边界的二分搜索，使用闭区间需要注意索引溢出的问题 二分搜索推广：找到 x，f(x) 和 target，然后套用二分搜索模板即可，相关问题有珂珂吃香蕉，在 D 天内送达包裹的能力，分割数组的最大值，寻找两个正序数组的中位数 双指针技巧：快慢指针通常用于解决链表中的问题，如判定环形链表以及环的起始位置，寻找中点，倒数第 n 个元素；左右指针通常用于解决二分搜索，两数之和，反转数组，滑动窗口，寻找旋转排序数组中的最小值，通过删除字母匹配到字典里最长单词， 寻找峰值等算法 滑动窗口模板：维护一个窗口，不断滑动并且更新数据结构，相关问题如最小覆盖子串，字符串排列子串判定，找所有字母的异位词，最长无重复子串，长度最小的子数组，串联所有单词的子串（相同长度），最大连续 1 的个数，考试的最大困扰度 常数时间获取随机数问题：需要使用数组来存储数据，相关问题如常数时间插入、删除和获取随机元素，黑名单中的随机数等 单调栈使用技巧：不同字符的某个子序列，不同字符的最小子序列等 双指针技巧：删除排序数组中的重复项，删除排序链表中的重复元素，移除元素，移动零，合并区间，最长回文子串等 两数之和问题：借助哈希表或者排序实现，如找到和为 target 的两个数，TwoSum 数据结构设计，推广问题如三数之和，四数之和，以及 nSum 问题 数组前缀和技巧：寻找数组左右和相等的中心索引，杨辉三角II 二维数组相关：旋转矩阵，对角线遍历 整数转罗马数字：数组统计，再使用贪心算法 柱状图中最大的矩形：使用单调栈和哨兵，在弹出栈顶元素的时候已经可以得到左右边界，可以计算出来面积 最大矩形：先统计左边 1 的个数，再使用直方图面积法求解 课程表 III：给定 [dur, ddl]，求能修读的最大课程数目，使用贪心算法实现 最长重复子串：使用字符串哈希来检查相同的字符串是否存在，提高效率 寻找最近的回文数：给定一个数字，返回和其相差绝对值最小的回文数，附近回文数主要分为 5 类 最长连续序列：返回未排序数组中的最长连续序列的值，使用哈希表可以降低至 O(n) 下一个更大元素 III：给定一个数 n，返回其数字重排后下一个最小的大于 n 的数，通过逆序对解决 多数元素：给定数组，找出其中的出现次数超过 n &#x2F; k 的数目，可以使用投票法，分三类讨论 寻找重复数：在一个长度为 n + 1 的数组中，其元素都在 [1, n]，使用原地算法，可以通过索引负数标记 环形数组是否存在循环：在数组中判断是否存在循环，同样通过快慢指针解决 数组嵌套：在 [0..n-1] 中找到最大循环集合，注意只可能存在不相交的循环，因为每个元素都不相同 任务调度器：给定任务，在冷却期间不能运行相同任务，使用数学思想进行计算，关注最大执行次数的任务 在 LR 字符串中交换相邻字符：能否在 LR 规则下进行字符串转换，看源和目的字符串中 L 和 R 的位置即可 子数组按位或操作：返回可能或操作的结果数量，注意使用剪枝防止超时，整个数组或操作结果作为哨兵 漂亮数组：构造数组，使得不存在 num[j] * 2 &#x3D; num[i] + nums[k]，使用分治法 坏了的计算器：采用逆向思维，才能贪心地执行除法，从而得到最小的转移步数 优美的排列 II：n 个元素的数组，使得数组相邻元素之差只能存在 k 个不同值，考虑大小交错排列法 加油站：只需要从不能通过的地方重新开始测试即可 只出现一次的数字：其他数字重复出现多次，只有一个数字只出现一次，使用位统计方法 最大数：两个数据排序关系只需要看两个数组组成不同数字大小即可 Excel 表列名称：实际上是偏移为 [1..26] 的进制转换问题，每次取余前先自减取得偏移为 0 的数字即可 超级丑数：找到第 n 个丑数，丑数序列乘以质数列表还是丑数序列，可以采用多指针的方式解决 摆动排序 II：可以参考桶排序，来构造摆动数组 递增的三元子序列：通过记录最小和次小值，再判断是否存在数字大于次小值即可 水壶问题：可以使用模拟的方法来进行搜索，或者直接使用数学公式 消除游戏：可以考虑约瑟夫环例子推出最终结果 有序矩阵中第 K 小的元素：可以使用优先队列进行归并，也可以使用二分法 汉明距离总和：统计每个位上 1 的数目，再进行数学计算即可 日程表安排 II：可以使用差分数组，也可以使用线段树 132 模式：直接遍历会超时，可以考虑使用数据结构维护左右两边的数据信息，再进行一次遍历即可 替换后的最长重复字符：以每个字符作为结尾字符，满足最左边的索引，找到最大区间即可 重建序列：根据给定的子序列重新构建一个序列，使用拓扑排序即可 到达终点数字：问题可以转化为给定数字，添加正负号凑出目标值问题，根据差值进行判断 统计子串中的唯一字符：可以转化为每个字符在所有子串中的贡献度，根据该字符前后相同字符位置可以确定 回文素数：根据现有的数找到下一个回文数，然后查看是否是素数，或者利用偶数长度的回文数只有 11 性质 索引处的解码字符串：逆向思考，先求出整个字符串的长度，如果字符串重复多次组成，那么索引取模结果相同 分发糖果：分发糖果使得相邻得分高的人得到更多糖果，可以采取左右两遍遍历的方法，并取最大值 环形子数组的最大和：最大和只可能出现在中间位置，或者头尾两端，前者直接求子数组最大值，后者使用数据总和减去子数据最小值即可，两者取较大值即可 字符串轮转：判断 s2 是否是由 s1 轮转得到的，碰到环形数组问题，直接原数据复制成两倍即可 使数组相等的最小开销：此时开销对应的数值相当于是数组中有多少个相同元素，取加权中位数即可 和至少为 K 的最短子数组：使用前缀和与单调队列即可，两种情况可以让队列进行优化 全局倒置与局部倒置：只需要检测是否存在非局部倒置即可，维护一个 max 前缀数组即可 第 N 个神奇数字：使用容斥原理和二分查找快速查找对应元素，lcm 和 gcd 算法 数据结构设计问题 并查集算法：解决图论中动态连通性的问题，优化技巧有增加秩，路径压缩，涉及到等价关系的算法可以考虑该数据结构，如等式方程的可满足性 LRU 缓存算法：按照访问顺序的淘汰策略，使用 LinkedHashMap 数据结构即可实现，注意其遍历顺序 LFU 缓存算法：按照访问频次的淘汰策略，如果最低访问频次有多个，淘汰最旧的数据，使用 HashMap，借助 LinkedHashSet 实现，注意其遍历顺序 最大频率栈：每次 pop 掉频率最大的数据，使用 HashMap 结构实现快速索引 数据流的中位数：使用两个优先级队列，并保持两个队列间数字的大小顺序 合并 k 个有序链表：使用优先级队列实现，如返回朋友圈前 10 条动态，第 K 个最小的素数分数 单调栈：指每次在 push 的时候，保持栈中的大小顺序，用于处理 Next Greater Element 问题，如下一个更大元素 I，下一个更大元素 II，每日温度等 单调队列：每次添加元素的时候，保持队列中的大小顺序，用于处理和滑动窗口相关的问题，如滑动窗口最大值 优先队列实现：使用二叉堆来实现，涉及到的操作主要有 sink，swim，offer 和 poll 方法， 栈实现队列和队列实现栈：双栈一个用于 offer，一个 用于 poll；队列将队头元素调整到队尾 找到处理最多请求的服务器：需要统计空闲服务器和处理器中的服务器，采用 TreeSet 和优先队列 动态规划问题 该类问题存在重叠子问题，具备最优子结构和状态转移方程，解决方案通常有带备忘录的递归（debug 时可以缩进查看调用栈），dp table 的迭代解法，注意迭代的方向需要根据已知的 dp 状态来确定，如零钱兑换 I，斐波拉契数列，下降路径最小和等 编辑距离：dp[m, n] 表示子串 s1[0..m] 和 s2[0..n] 的编辑距离，如果想要具体的编辑方案，可以加上对应的选择即可 最长递增子序列：dp[n] 表示将其当作最后一个数字时的最长递增子序列长度，使用 patience sorting 可以降低时间复杂度，推广问题有信封嵌套问题，需要注意其排序方法，最长等差子序列，需要注意超时 最大子序和：包含正负数，dp[n] 表示 num[n] 作为子序中的最后一个数，最大的子序和，而不是 nums[0..n] 中的最大子序和 最长公共子序列：使用 dp[m, n] 表示子串 s1[0..m] 和 s2[0..n] 中最长公共子序列的长度，相关问题有两个字符串的删除操作，两个字符串的最小 ASCII 删除和等 最长回文子序列：在子串 s[i..j] 中，最长回文子序列的长度为 dp[i, j]，然后迭代即可，由于回文子序列特殊性，也可以转化为最长公共子序列问题 0-1 背包问题：对于前 i 个物品，当前背包的容量为 w，这种情况下可以装的最大价值是 dp[i, w]，迭代即可，相关问题有分割等和子集 完全背包问题：物品的个数是无限的，通常涉及到将背包刚好填满的情况，相关问题如零钱兑换 II，注意其和爬楼梯算法的区别，前者求组合数，后者求排列数 区间调度问题：选择出来不重叠的区间的最大的个数，贪心算法需要选择 end 最小的，相关问题如无重叠区间，用最少数量的箭引爆气球 视频拼接问题：需要注意排序的方案，start 升序排列，start 相同则按照 end 降序排列 跳跃游戏：跳跃游戏 I 需要找到每次跳跃的最大距离，作为下一次的跳跃起点，跳跃游戏 II 同样道理，注意判定每次增加步数的逻辑，两者也可以使用动态规划来解决 最小路径和问题：dp[i, j] 表示 nums[i, j] 到 nums[0, 0] 的最小路径和 地下城游戏：dp[i, j] 表示从位置 [i, j] 出发，到达 [m, n] 的最少血量，注意其和最小路径和问题的对比 自由之路：dp[i, j] 表示指向 ring[i]，需要输入 key[j..n] 的最小的操作次数 K 站中转内最便宜的航班：dp[i, j] 表示经过最多 i 次中转，到达 j 的最便宜的航班 正则表达式匹配：dp(i, j) 表示 s[i..] 能否被 p[j..] 匹配，使用递归；或者 dp[i, j] 表示 s[0..i] 能否被 p[0..j] 匹配，使用迭代；简单的变体是通配符匹配问题 鸡蛋掉落：使用 dp(k, n) 表示从 n 层楼中，使用 k 个鸡蛋，确定 f 的最小确切尝试次数，或者可以使用 dp[k, m] 表示 k 个鸡蛋，最多 m 次操作可以确定 f 的最高的层数，使用迭代解决 戳气球：使用 dp[i, j] 表示开区间 (i, j) 内能得到的最大分数，通过最后一步思考状态转移方程 博弈问题：预测赢家问题中每个人从两端选择数字，最终数字和大的获胜，使用 dp[i, j] 表示从 nums[i, j] 中选择时的情况，dp[i, j, 0] 表示先手分数情况，dp[i, j, 1] 表示后手分数情况，相关问题有石子游戏 两键键盘：输出 n 个字符最少的操作次数，另外也可以考虑分解质因数方法 四键键盘：N 次操作输出 A 的最多的个数，考虑最后的一次操作，不是 A，就是 C-V，以此作为递推式 股票买卖问题：定义 dp[n, k, 0] 表示在第 n 天，最多 k 次交易，未持有股票的最大收益，而 dp[n, k, 1] 表示在第 n 天，最多 k 次交易，持有股票的最大收益，找到状态转换关系，使用迭代即可解决，注意一些问题也可以使用贪心算法解决 打家劫舍问题：dp[i] 表示 num[i..] 开始，能获得的最大价值，变体如增加首尾限制，二叉树限制等 字符匹配算法：使用 dp[i, j] = next 表示当前状态 i，遇到字符 j 后下一个状态是 next，以此为状态机，来进行匹配，第一次相当于 pat 与 pat[1..] 匹配，第二次相当于 pat 与 text[0..n] 匹配；也可以使用 KMP 算法，使用 next 数组表示最大相同真前后缀的前缀末尾索引 构造回文的最小插入次数：dp[i, j] 的表示对字符串 s[i..j]，最少需要进行 dp[i, j] 次插入才能变成回文串，注意状态转换关系 IPO：使用贪心算法，每次选择能投资项目中的最大利润 不含连续 1 的非负整数：定义 dp[i] 为长度为 i 的有效数字的个数，然后对每位进行操作统计 解码方法 II：定义 dp[i] 表示 str[i..n] 可被解码方法的数目，找到递推关系即可 超级洗衣机：使用贪心算法，考虑左右两组传递次数和某一个洗衣机最大传递次数 k 个逆序对数组个数：定义 dp[i, j] 为 1 - i 自然数中，j 个逆序对的个数，分析数字 i 的位置 恰有 K 根木棍可以看到的排列数目：定义 dp[i, j] 表示 i 根木棍中可以看到 j 个木棍，分析长度为 1 的木棍所在位置 猜数字大小 II：定义 dp[i, j] 表示从范围 [i, j] 猜数字的最小的代价，从而可构造转移方程 交错字符串：定义 dp[i, j] 表示 s1[0, i) 和 s2[0, j) 是否能构成 s3[0, i+j)，从而构建转移方程 三个无重叠子数组的最大和：定义 dp[i,k] 表示 nums[0..i] 构成 k 个不重叠子数组最大和值，考虑 nums[i] 是否在里面，从而构造转移方程 不同的子序列：定义 dp[i, j] 表示 s[i..] 中含有 t[j..] 子序列个数，构造对应的转移方程 最低加油次数：给定加油站和初始油量，判断最少需要加多少次油才能到达目的地，使用贪心 最后一块石头的重量 II：两块石头重量的差值当作新的石头放入其中，转换为 0-1 背包问题 一和零：使用背包问题，dp[i, j, k] 表示前 i 个字符串里面，最大 j 个 0，k 个 1 的子集长度 环绕字符串中唯一的子字符串：通过考虑以某字符结尾的最大长度，就可以计算出来不同子串的长度 我能赢吗：博奕类问题，每一步遍历当前所有选择看是否会导致赢的结果 设置交集大小至少为 2：区间排序，按照首点升序，尾点降序进行排列，每次贪心选择区间前两个元素 新 21 点：定义 dp[x] 为起始分数为 x 时获胜概率，得到转移方程 组合数取模：不能使用直接方法取模，会造成精度损失，推荐使用打表方法，采用递推式来解决 最小差值 II：每个元素只能加 k 或者减 k，可以先排序，然后中间切断，最大值最小值只在端点出现 使序列递增的最小交换次数：每次只交换相同位置处的数组，定义 dp[i, j] 表示 [0..i] 的数组在 i 处交换和不交换时使序列递增的次数 分汤：首先需要向上取整，定义 dp[i, j] 表示分别剩余 i 和 j 的汤时所求的概率值，另外当 n 超过一定值时，其会趋于 1 最小移动总距离：机器人到工厂的位置时不严格单调增的，定义 dp[i, j] 表示前 i 个工厂修复前 j 个机器人的最小移动总距离 完美分割的方案数：定义 dp[i, j] 表示前 i 个字段分割成 j 段的方案数，再维护一个前缀和 g[i, j]，表示 dp[t, j] 的前 i 项和 不重叠回文子字符串的最大数目：定义 dp[i] 表示前 i 个字符中可以组成多少个长度大于等于 k 且不重叠的子串，中心扩展法求回文 最大平均值和的分组：定义 dp[i, j] 表示前 i 个元素分为最大 j 个非空子数组时最大平均值 回溯算法（DFS 算法） 回溯算法和动态规划递归算法类似，不过不同的是回溯算法不存在重叠子问题，回溯算法的核心就是做选择，回溯，撤销选择，相关问题如全排列，n 皇后，数独问题，全排列 II（重复数字） 集合划分：使用两种视角，桶的视角即为每次刚好填满一个桶，站在数字的视角，即为每次进入一个桶，两种算法通过剪枝可以减少运行时间 子集，组合和排列问题：子集和组合问题都是从当前位置的下个位置开始回溯，但是排列问题不同，另外可以考虑递归思路 括号生成：合法括号对性质，在回溯中添加左右括号剩余数量（有效信息） 单词搜索 II：在二维棋盘中找到某个单词是否存在，注意剪枝以防超时 祖玛游戏：为了提高效率，可以使用剪枝或者使用备忘录来检测已经判断过的字符串 字典序排序：使用 DFS 可以进行搜索排序，使用迭代可以得到 O(1) 空间复杂度 火柴拼正方形：每个位置可以选择放入四个边中之一，只要满足其边长不会超过总长度的四分之一即可 到达首都的最少油耗：考虑每条边上的流量，必定是子树元素个数之和除以车子容量大小，然后深搜即可，注意第一个深搜条件 BFS 算法 BFS 用于找寻最短路径一类的问题，主要通过队列和设置 visited 集合来解决，可采取的优化如双向 BFS 搜索，相关问题如打开转盘锁，二叉树的最小深度等问题 滑动拼图：将二维数组变为字符串处理，同时利用已知信息建立邻居映射 单词接龙 II：使用 BFS 查找最短路径，可以采用双端 BFS 优化，使用 DFS 构造路径 最小高度树：使用拓扑排序思想，将入度为 1 的数字放到队列中去，然后进行 BFS 即可 并查集算法 除法求值：给定一组除法式子，判断给定查询的结果，使用并查集可以让有倍数关系的变量在一起 好路径的数目：给定一个树图，统计路径中值最大的节点在端点处的数目，可以采用并查集一步步加入 数学运算技巧 常用的位操作：使用异或判断两个数字是否异号，使用异或交换两个数字，使用 n &amp; (n - 1) 消除数字 n 的二进制表示中的最后一个 1，相关问题如汉明重量，判断是否为 2 的指数，查找只出现一次的元素，两数之和 阶乘算法题：阶乘后零的个数取决于因子 5 的个数，相关问题有阶乘后的零的个数，阶乘后 k 个零 高效寻找素数：在进行因子判断的时候，只需要遍历到 sqrt(x) 即可，相关问题如计算质数的个数，注意两层循环的起始和终止条件 高效模幂运算：使用递归处理数组指数，模运算的防溢出运算，高效求幂，相关问题如超级次方 寻找缺失元素：情况一是 [0, n] 的序列放到长度为 n 的数组中，情况二是 [1, n] 的序列放到长度为 n 的数组中 同时寻找缺失和重复的元素：使用映射来表示某个数字已经存在，通常这样的问题需要索引和数字一起使用 水塘抽样算法：对于第 i 个元素，应该有 1/i 的概率选择该元素，1 - 1/i 的概率保持原有的选择，可以推广到随即抽取 k 个元素，相关问题如链表随机节点，随机数索引 一行代码解决的问题：Nim 游戏，石子游戏，电灯开关问题（因数个数问题） 反直觉概率问题：男孩女孩问题，生日悖论，三门问题（概率浓缩） 洗牌算法：Fisher-Yates 算法每次迭代模拟了从剩余数字中选择一个放到对应位置上的过程，相关问题如打乱数组 随机数生成问题：基于 (rand(X) - 1) * Y + rand(Y)可以均匀生成 [1, XY] 之内的随机数，随后使用拒绝采样挑选随机数 可怜的小猪：小猪试毒问题改编而来，在给定 n 只小猪，m 次尝试时，每只猪提供的信息是 m + 1 次，整个的信息个数就是 pow(m + 1, n) 数组中两个数的最大异或值：从高位到低位依次选择，利用异或运算的特殊性，三数之间两数异或等于第三个数，一步步向下迭代求解 不可能得到的最短骰子序列：上一个出现的序列后面的子数组应包含所有的数字，否则不能构造对应的子序列 镜面反射：根据 p 和 q 的奇偶性来判断翻转次数和折射次数，从而得到最终结果 其他算法技巧 前缀和数组：适用于原始数组不变，频繁查询某个区间的累加和，相关问题如和为 k 的连续子数组个数，使用前缀和与哈希表解决，思想和 twoSum 类似 差分数组：主要用于频繁对原始数组的某个区间的元素进行加减，对区间 [i, j] 的修改实际上只需要修改 diff 数组的两个元素即可，相关问题如航班预订统计，得分最高的最小轮调 树状数组（二叉索引树）：用于处理区间统计量问题，只适用于点变化的情况 区间树：用于处理区间统计量问题，适用于区间变化的情况，比树状数组应用广泛 快速选择算法：存在于快速排序算法中，每次 partition 都会使得左边的数字小，右边的数字大，相关问题如数组中的第 k 个最大元素 分治思路：归并排序，分而治之，相关问题如为运算表达式设计优先级 区间问题：关键在于排序，以及判断两个区间相交与否，相关问题如删除被覆盖区间，区间合并，区间列表的交集，两个矩形覆盖的面积 使用哈希表：使用哈希表可以快速统计相同位置上出现的次数，相关问题如回旋镖的数量，将数据流变为多个不相交的区间，随机翻转矩阵，缺失的第一个正整数（O(n)） 排除法：搜索名人 高频面试算法 分割数组为连续子序列：使用两个哈希表，分别统计出现次数和能在结尾放置的次数，优先放在上个序列的结尾 吃葡萄：将问题转化为三角形边长平分问题，并分析极端情况 烧饼排序算法：使用递归，首先让最大烧饼在最底层，类似于汉罗塔问题 字符串相乘：模拟乘法运算过程，注意参与乘法运算两个数的索引和结果索引间的关系 实现一个计算器：先解决加减，后解决乘除，最后考虑括号，使用单栈方法和双栈方法 接雨水问题：考虑当前节点能接的水是多少，从而解决问题，可以对状态进行优化，相关问题如一维数组接雨水，二维数组接雨水 寻找最长回文子串：关键在于奇数长度和偶数长度的回文子串的判定 括号相关问题：使用栈结构进行模拟，匹配问题则找到需要的右括号的数量，相关问题如判断合法括号串，使括号有效的最小插入及变体（一个左括号匹配两个右括号），有效的括号字符串（加入 *），最长有效括号 判定完美矩形：利用面积和顶点出现次数作为判别条件 考场就座：最大化学生之间的间隔，使用 TreeSet 作为数据结构，按照 distance 排序，需要处理边界情况 高效判定子序列：使用双指针可以解决该问题，但是如果需要匹配的子序列比较多的时候，可以结合二分查找提高效率 Java 编程知识点 IntegerCache：在自动装箱时，为了提高性能，Java 缓存了 [-128, 127] 的整形值引用，因此，当我们比较两个 Integer 的时候，应该使用 equals 方法，或者借助 intValue 方法，而不是 == random.nextInt() 和 nextInt(upperBound)：不加参数的话，int 类型 32 位都会随机 0 或者 1，可能会产生负数，使用取模可能会产生负数，加参数的话返回 [0, upperBound) 之间的整型 在 Java 中使用 int[] 作为 HashMap 的键并不会得到想要的结果，因为其会使用 int[] 的索引作为 hashcode，可以使用 List&lt;Integer&gt; 或者重写一个类，该类重新实现 hashcode 和 equals 算法，亦或直接使用 TreeMap Object.hashCode 是一个实例方法，不允许 null；Objects.hashCode(obj) 是静态方法，允许 obj 为 null；Objects.hash(obj…) 也是静态方法，接受多个参数，返回所有参数的总的哈希值 Integer.valueOf() 和 Integer.parseInt() 两者都是对字符串进行解析，不过它们的返回值类型不同，前者是 Integer，后者是 int List&lt;Integer&gt; 到 int[] 类型的转换不能直接使用 toArray，可以借助流：list.stream().mapToInt(Integer.intValue).toArray() 想要对自定义类实现排序，要么实现 Comparable&lt;T&gt; 接口，重写 compareTo 方法，或者是排序的时候传入一个 Comparator&lt;T&gt; 对象，重写 compare 方法 想要对 int[] 类型进行降序排序，不能直接用 Arrays.sort，其只对 T[] 提供自定义比较器，可以使用流操作来实现：Arrays.stream(arr).boxed().sorted((a, b) -&gt; b - a).mapToInt(Integer::intValue).toArray() 在 Java 设计中 Stack 继承自 Vector，这是一种错误的设计，应该使用组合而不是继承关系。官方推荐写法：Deque&lt;Integer&gt; stack = new ArrayDeque&lt;&gt;()","categories":[],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://blog.zsstrike.tech/tags/Algorithm/"}]},{"title":"《操作系统概念》笔记","slug":"《操作系统概念》笔记","date":"2021-03-31T13:30:49.000Z","updated":"2022-05-16T07:41:46.175Z","comments":true,"path":"2021/03/31/《操作系统概念》笔记/","link":"","permalink":"http://blog.zsstrike.tech/2021/03/31/%E3%80%8A%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%A6%82%E5%BF%B5%E3%80%8B%E7%AC%94%E8%AE%B0/","excerpt":"本文总结了《操作系统概念》中的知识点，以便查阅。","text":"本文总结了《操作系统概念》中的知识点，以便查阅。 第二章 操作系统结构操作系统的服务：主要包括以下功能： 用户界面 程序执行 IO 操作 文件系统操作 通信：通过共享内存或者消息交换实现 错误检测 资源分配 记账：记录用户使用资源类型和数量 保护和安全 系统调用：提供操作系统的服务调用接口，该部分通常使用 C 或者 C++ 实现，当然，对某些底层硬件操作可能需要使用汇编。向操作系统传参的方法有：寄存器传参，栈传参，表传参。 系统调用类型：进程控制，文件管理，设备管理，信息维护，通信，保护。 操作系统的结构： 简单结构：如 MS-DOS，并没有仔细划分为模块 分层方法：每层上调用的接口是不同的，用于区分用户接口和不同等级的系统接口 微内核：主要功能在于为客户端程序和运行在用户空间的各种服务提供通信，通信是通过消息传递实现的 模块：最佳方法是可加载的内核模块，常见于现代 Linux 系统中 操作系统的引导：加载内核以启动计算机的过程，称为系统引导。大多数的操作系统都有一个引导程序，改程序能够定位内核，并将其加载到内存以开始执行。当 CPU 收到一个重置事件时，指令寄存器会加载某个预先定义的内存位置，并从该位置执行，该位置实际上就是引导程序所在位置。 第三章 进程进程概念：进程是执行的程序，包括代码段，数据段，程序计数器的值，堆和栈等。程序是被动实体，而进程是活动实体。 进程状态：就绪，运行，等待，新建，终止。 进程控制块（PCB）：包含有和进程相关的信息，如进程状态，进程 id，程序计数器，内存管理信息等。 调度队列：进程在进入系统的时候，会被加入到进程队列，该队列包含有所有进程。驻留在内存中，就绪的进程则保存在就绪队列上。通常采用双向链表实现，链表指向不同进程的 PCB。 上下文切换：切换 CPU 到另外一个进程需要保存当期进程的状态和恢复另外一个进程的状态。 进程间通信（IPC）：提供进程间通信的好处有信息共享，计算加速和模块化。有两种基本模型：共享内存和消息传递。 客户机-服务器通信： 套接字：常用且高效，但是输入分布式进程间一种低级形式的通信 远程服务调用（RPC）：和 IPC 的消息不同，RPC 通信交换的信息具有明确的结构，不再仅仅是数据包。 管道：分为单向和双向（半双工和全双工），是否允许非父子关系的进程之间相互通信。 第四章 多线程编程线程：是 CPU 调度的基本单位，包含有 ID，程序计数器，寄存器组等，和同一进程下的其他线程共享代码段，数据段和其他资源。 多线程编程的优点：响应性，资源共享，经济和可伸缩性。 并发和并行：某个时间段内多个任务交替执行，叫做并发；同一时间点，有不同的任务运行，叫做并行。 多线程模型：有用户线程和内核线程，用户线程位于内核之上，管理无需内核支持，内核线程由操作系统支持和管理，常见的有三种模型： 多对一模型：一个线程执行阻塞调用，整个进程都会阻塞，另外，由于任一时间只有一个线程可以访问内核，不能很好地在多处理核系统上运行。 一对一模型：优点是可以在多个核上运行，缺点是每次创建一个用户线程都需要创建一个内核线程，不能创建太多的线程。 多对多模型：解决了多对一模型和一对一模型中的缺点。多路复用多个用户线程到相同数量或者更少的内核线程，但也允许绑定某个用户线程到一个内核线程，该变种有时称为双层模型。 轻量级进程（LWP）：在多对多模型或者双层模型之间的一个数据结构，对于用户级线程，LWP 表现为虚拟处理器，每个 LWP 和一个内核线程相连，只有内核线程才能通过系统调度以便运行于物理处理器。 第五章 进程调度基本概念：进程的执行包括有 CPU 执行和 IO 等待过程，IO 密集型程序表现为大量短 CPU 执行，CPU 密集型程序只有少量长 CPU 执行。每当 CPU 空闲的时候，操作系统就会从就绪队列中选择一个进程来执行。在此过程中，调度程序需要完成上下文切换，切换到用户模式等。 调度方案：抢占式和非抢占式的。非抢占式调度下，一旦某个进程分配到 CPU，就会一直使用 CPU，知道它变为终止或者等待状态。 调度准则： CPU 使用率 吞吐量 周转时间：从进程提交到进程完成的时间段 等待时间：在就绪队列中等待的时间 响应时间：从提交请求到第一次响应的时间 调度算法：如何从就绪队列中选择进程以便为其分配 CPU 先到先服务调度（FCFS） 最短作业优先调度（SJF） 优先级调度（PS）：低优先级无穷等待解决方案之一是老化，即逐渐增加在系统中等待时间长的进程的优先级 轮转调度（RR）：性能很大程度上取决于时间片的大小 多级队列调度：将就绪队列分成多个单独队列，每个队列可以采用不同的调度算法 多级反馈队列调度：在多级队列调度的时候，进程只会在某个队列中，队列之间不允许迁移，而多级反馈队列允许进程在队列之间切换。 第六章 同步生产者消费者问题：两个不同的进程对 counter&#x3D;4 分别进行加一和减一。如果这两个进程并发执行，得到的结果可能是 3,4,5。语句执行方式为 load，add&#x2F;sub，store。像这样得到不正确的状态的过程称为竞争。 临界区问题：进程在临界区内可能会修改公共变量，更新文件内容等。临界区问题的解决方案应该满足如下要求： 互斥：只允许单个进程在临界区内运行 进步：如果没有进程在临界区，那么可以选择某个进程进入 有限等待 互斥锁（mutex lock）：在进入临界区应该得到锁，在退出的时候释放锁。accquire 和 release 两个操作都是原子执行的。如果在获取的时候忙等待，那么这样的锁也被称为自旋锁。不过自旋锁也有一个优点，当进程在等待锁的时候，没有上下文的切换。当使用锁的时间较短的时候，自旋锁还是可用的。 信号量（semaphore）：信号量 S 是一个整形变量，通过 wait 和 signal 操作。二进制信号量类似于互斥锁。 死锁与饥饿：死锁指的是多个进程间相互等待资源，饥饿则是无限等待信号量。 经典同步问题：有界缓冲问题，读者作者问题，哲学家就餐问题。 第七章 死锁死锁产生的必要条件： 互斥 占有并等待：一个进程占有一个资源并且等待另外的资源 非抢占：资源只能被进程完成任务后自愿释放 循环等待 处理死锁问题： 死锁预防和死锁避免：死锁预防指通过破坏死锁产生的必要条件来预防死锁，如一个进程同时获取所有它所需要的所有资源，按照顺序获取资源等。思索避免算法有银行家算法。 死锁检测和恢复：通过从资源分配图中判断是否存在环，恢复的时候可以通过进程终止和资源抢占。 第八章 内存管理策略逻辑地址和物理地址：CPU 生成的地址通常称为逻辑地址，而内存单元看到的地址通常称为物理地址。 交换：进程必须在内存中以便执行，不过，进程可以暂时从内存换出到磁盘中，之后再次换入。 碎片：外部碎片指的是尚未分配的内存因为已经分配的内存而产生，内部碎片指的是分配大于进程所需的多余的内存。外部碎片问题的解决方案之一就是紧缩，指在内存中移动程序，并且不影响程序的运行。 分段：逻辑地址空间由一组段构成，逻辑地址通过&lt;段号，偏移&gt;组成。地址转换是通过段表实现的。 分页：分段允许进程的物理空间是非连续的，分页同样支持该功能，不过，分页避免了外部碎片，而分段则不能避免。分页的基本方法是将物理内存分为大小固定的块，称为帧（frame），同样，其将逻辑内存也分为大小相同的块，称为页（page）。通过页表实现地址转换。 转换表缓冲区（TLB）：大多数计算机允许页表非常大，通常需要将页表存放在内存中，并且将页表基地址寄存器指向页表。这样带来的问题是访问一个字节的数据需要两次内存访问。为此，可以设置 TLB，其实际上是一个高速缓存。TLB 和 页表协同工作： 共享页：不同进程之间可以共享公共代码，这也是分页的优点之一。 页表结构： 分层分页：为了解决单页表在大内存系统下的内存浪费问题。 哈希页表：采用哈希函数对虚拟地址页码进行哈希，并且在链表中比较页码。 倒置页表：通过查找&lt;pid, p&gt;条目，从而确定物理地址&lt;i, d&gt;，从而确定物理地址。该方法是根据虚拟地址查找的，可能需要花费大量时间。 第九章 虚拟内存管理请求调页（demand paging）：当程序被加载到内存中的时候，仅加载需要的页面。为了区分哪些页面已经加载到内存，需要提供标志位，当无效的页面被访问的时候，就会产生缺页错误。下图中 i 表示该页无效。 写时复制：比如采用了 fork 创建了一个子进程，两者会共享相同的物理内存页。在子进程修改相关数据时，才来进行复制。 页面置换：当所有的内存都已经分配完毕后，再次请求帧分配的时候，操作系统发现此时没有空闲帧，从而触发页面置换。采用脏位可以在牺牲页面没有被修改的情况下减少一次页面传输。 FIFO 页面置换：引用串表示依次访问的页面编号。 最优页面置换：置换最长时间不会被用到的页面。该算法难以实现，因为需要用到引用串的未来知识，主要用于比较研究。 最近最少使用（LRU）页面置换：当需要置换页面的时候，LRU 选择最长时间没有使用的页面。 基于计数的页面置换：最不经常使用和最经常使用页面置换算法。 内存映射文件：如果采用open,read,write来读取写入磁盘文件，每个文件访问都会经过系统调用和磁盘访问。采用虚拟内存技术，可以将文件 IO 作为常规内存进行访问。该方法会提升性能。 第十章 文件系统文件：操作系统对存储设备的物理属性加以抽象，定义出的逻辑存储单位。 文件操作：创建，读，写，文件定位，删除，截断。 访问方法：顺序访问，直接访问。 目录结构： 单级目录 两级目录 树形目录 无环图目录：树结构禁止共享文件或者目录，图目录允许共享 文件系统安装（mount）：操作系统需要直到设备的名称和安装点（附加文件系统在原来文件结构中的位置）。 第十二章 大容量存储结构大容量设备：磁盘，固态硬盘和磁带。磁盘结构如下： 磁盘调度： FCFS 调度 最短寻道时间（SSTF）调度：在移动磁头到其他位置以便处理其他请求之前，处理靠近当前磁头位置的所有请求。 扫描（SCAN）调度：磁臂从磁盘的一端开始，向另外一端移动，在移动每个柱面的时候，处理请求，之后反向进行该操作。 LOOK 调度：类似于 SCAN 调度，只不过不会移动到磁盘的一端，而是移动到一个方向的最远请求为止。 RAID 结构：磁盘冗余阵列技术，主要提供高可靠性和高数据传输率。 第十三章 IO系统IO 硬件和操作系统交互方式： 轮询 中断 直接内存访问（DMA）：主机将 DMA 命令块写入内存，CPU 将命令块地址写到 DMA 控制器，接着 CPU 继续其他工作，而 DMA 控制器继续操作内存总线，将地址放到总线，在没有 CPU 的帮助下执行传输。 IO 设备：块设备和字符设备。 缓冲区：是一块内存区域，用于保存在两个设备之间或在设备和应用程序之间传输的数据，提升 IO 系统性能。","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://blog.zsstrike.tech/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"《深入浅出MySQL》笔记","slug":"《深入浅出MySQL》笔记","date":"2020-12-20T02:42:23.000Z","updated":"2022-09-02T07:54:59.308Z","comments":true,"path":"2020/12/20/《深入浅出MySQL》笔记/","link":"","permalink":"http://blog.zsstrike.tech/2020/12/20/%E3%80%8A%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAMySQL%E3%80%8B%E7%AC%94%E8%AE%B0/","excerpt":"","text":"本文用于记录《深入浅出MySQL》里面的知识要点，以备再次查阅。 第二章 SQL基础MySQL使用入门： SQL语句分类：DDL，DML，DCL。 DDL：数据定义语言，对数据库内部的对象进行创建，删除，修改等操作。 123456789101112131415161718// 数据库CREATE DATABASE dbname;SHOW DATABASES;USE dbname;DROP DATABASE dbname;// 表CREATE TABLE tablename ( column_name1, type1 constraints, ... column_namen, typen constraints);DESC tablename; SHOW CREATE TABLE tablename;DROP TBALE database;ALTER TABLE tablename MODIFY [COLUMN] column_definition [FIRST | AFTER col_name];ALTER TABLE tablename ADD [COLUMN] column_difinition [FIRST | AFTER col_name];ALTER TABLE tablename DROP [COLUMN] col_name;ALTER TABLE tablename CHANGE [COLUMN] old_col_name column_definition;ALTER TABLE tablename RENAME new_tablename; DML：数据操作，主要包括表记录的增删查改。 1234567INSERT INTO tablename(field1, field2, ..., fieldn)VALUES(value1, value2, ..., valuen),(value1, value2, ..., valuen);UPDATE tablename SET field1=value1 [WHERE CONDITION];DELETE FROM tablename [WHERE CONDITION];SELECT * FROM tablename [WHERE CONDITION] [LIMIT offset_start, row_count]; 查询不重复记录：distinct 条件查询：WHERE CONDITION 排序和限制：ORDER BY 和 LIMIT offset_start, row_count 聚合：GROUP BY column_name HAVING condition 表连接：内连接，外连接（左连接，右连接） 子查询：可能需要 in，not in，exists 等 记录联合：UNION，UNION ALL DCL：DBA 用来管理系统中的对象权限时使用。 12GRANT SELECT, INSERT on dbname.* to &#x27;z1&#x27;@&#x27;localhost&#x27; identified by &#x27;123&#x27;;REVOKE INSERT on dbname.* from &#x27;z1&#x27;@&#x27;localhost&#x27;; 帮助的使用： 按照层次查看帮助：? contents 快速查阅帮助：? select 第三章 MySQL支持的数据类型数值类型： 类型 大小 范围（有符号） 范围（无符号） TINYINT 1 byte (-128，127) (0，255) SMALLINT 2 bytes (-32 768，32 767) (0，65 535) MEDIUMINT 3 bytes (-8 388 608，8 388 607) (0，16 777 215) INT或INTEGER 4 bytes (-2 147 483 648，2 147 483 647) (0，4 294 967 295) BIGINT 8 bytes (-9,223,372,036,854,775,808，9 223 372 036 854 775 807) (0，18 446 744 073 709 551 615) FLOAT 4 bytes (-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466 E+38) DOUBLE 8 bytes (-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) DECIMAL 采用二进制字符串存储，每四个字节存储9位数字 依赖于M和D的值 依赖于M和D的值 对于整形数据，MySQL还支持在类型名称后面的小括号内指定显示宽度。 日期时间类型： 类型 大小 ( bytes) 范围 格式 用途 DATE 3 1000-01-01&#x2F;9999-12-31 YYYY-MM-DD 日期值，压缩存储 TIME 3 ‘-838:59:59’&#x2F;‘838:59:59’ HH:MM:SS 时间值或持续时间，压缩存储 YEAR 1 1901&#x2F;2155 YYYY 年份值 DATETIME 8 1000-01-01 00:00:00&#x2F;9999-12-31 23:59:59 YYYY-MM-DD HH:MM:SS 混合日期和时间值，字面值存储 TIMESTAMP 4 1970-01-01 00:00:00&#x2F;2038 YYYYMMDD HHMMSS 混合日期和时间值，时间戳 字符串类型： 类型 大小 用途 CHAR 0-255 bytes 定长字符串 VARCHAR 0-65535 bytes 变长字符串 TINYBLOB 0-255 bytes 不超过 255 个字符的二进制字符串 TINYTEXT 0-255 bytes 短文本字符串 BLOB 0-65 535 bytes 二进制形式的长文本数据 TEXT 0-65 535 bytes 长文本数据 MEDIUMBLOB 0-16 777 215 bytes 二进制形式的中等长度文本数据 MEDIUMTEXT 0-16 777 215 bytes 中等长度文本数据 LONGBLOB 0-4 294 967 295 bytes 二进制形式的极大文本数据 LONGTEXT 0-4 294 967 295 bytes 极大文本数据 ENUM：值范围需要在创建表时通过枚举方式显式指定，对于插入不在 ENUM指定范围内的值时，并没有返回警告，而是插入了 enum 定义中的第一个值 SET：从允许值集合中选择任意1个或多个元素进行组合来赋值 第四章 MySQL中的运算符算术运算符： 运算符 作用 + 加法 - 减法 * 乘法 &#x2F; 或 DIV 除法 % 或 MOD 取余 在除法运算和模运算中，如果除数为0，将是非法除数，返回结果为NULL。 比较运算符： 运算符 含义 备注 &#x3D; 等于 &lt;&gt;, !&#x3D; 不等于 &gt; 大于 &lt; 小于 &lt;&#x3D; 小于等于 &gt;&#x3D; 大于等于 BETWEEN 在两值之间 &gt;&#x3D;min&amp;&amp;&lt;&#x3D;max NOT BETWEEN 不在两值之间 IN 在集合中 NOT IN 不在集合中 &lt;&#x3D;&gt; 严格比较两个NULL值是否相等 两个操作码均为NULL时，其所得值为1；而当一个操作码为NULL时，其所得值为0 LIKE 模糊匹配 REGEXP 或 RLIKE 正则式匹配 IS NULL 为空 IS NOT NULL 不为空 逻辑运算符： 运算符号 作用 NOT 或 ! 逻辑非 AND 逻辑与 OR 逻辑或 XOR 逻辑异或 位运算符： 运算符号 作用 &amp; 按位与 | 按位或 ^ 按位异或 ! 取反 &lt;&lt; 左移 &gt;&gt; 右移 第五章 常用函数字符串函数： 编号 函数名 作用 1 LEFT(s,n) 返回字符串s前n个字符 2 RIGHT(s,n) 返回字符串s后n个字符 3 LENGTH(s) 返回字符串s的长度 4 LOCATE(s1,s2) 从字符串 s2 中获取 子串s1 的开始位置 5 LOWER(s) 大写转小写 6 UPPER(s) 小写转大写 7 LTRIM(s) 去掉字符串s左面的空格 8 RTRIM(s) 去掉字符串s右面的空格 9 TRIM(s) 去掉字符串s两边的空格 10 ASCII(s) 返回字符串s的第一个字符的 ASCII 码 11 CONCAT(s1,s2…sn) 字符串 s1,s2 等多个字符串合并为一个字符串 12 FIND_IN_SET(s1,s2) 返回在字符串s2中与s1匹配的字符串的位置(多句话) 13 FORMAT(x,n) 可以将数字 x 进行格式化 “#,###.##”, 将 x 保留到小数点后 n 位，最后一位四舍五入 14 INSERT(s1,x,len,s2) 字符串 s2 替换 s1 的 x 位置开始长度为 len 的字符串 15 SUBSTR(s, start, length) 从字符串 s 的 start 位置截取长度为 length 的子字符串 16 POSITION(s1 IN s) 从字符串 s 中获取 s1 的开始位置 17 REPEAT(s,n) 将字符串 s 重复 n 次 18 REVERSE(s) 将字符串s的顺序反过来 19 STRCMP(s1,s2) 比较字符串 s1 和 s2，如果 s1 与 s2 相等返回 0 ，如果 s1&gt;s2 返回 1，如果 s1&lt;s2 返回 -1（比较的是字符串首字母的 ASCII 码） 20 REPLACE (s1,s2,s3) 替换字符串；将s1中的s2内容替换为s3 数值函数： 编号 函数名 作用 1 ABS(x) 返回x的绝对值 2 AVG(expression) 返回一个表达式的平均值，expression 是一个字段 3 CEIL(x)&#x2F;CEILING(x) 返回大于或等于 x 的最小整数 4 FLOOR(x) 返回小于或等于 x 的最大整数 5 EXP(x) 返回 e 的 x 次方 6 GREATEST(expr1, expr2, expr3, …) 返回列表中的最大值 7 LEAST(expr1, expr2, expr3, …) 返回列表中的最小值 8 LN 返回数字的自然对数 9 LOG(x) 返回自然对数(以 e 为底的对数) 10 MAX(expression) 返回字段 expression 中的最大值 11 MIN(expression) 返回字段 expression 中的最大值 12 POW(x,y)&#x2F;POWER(x,y) 返回 x 的 y 次方 13 RAND() 返回 0 到 1 的随机数 14 ROUND(x) 返回离 x 最近的整数 15 SIGN(x) 返回 x 的符号，x 是负数、0、正数分别返回 -1、0 和 1 16 SQRT(x) 返回x的平方根 17 SUM(expression) 返回指定字段的总和 18 TRUNCATE(x,y) 返回数值 x 保留到小数点后 y 位的值（与 ROUND 最大的区别是不会进行四舍五入） 日期和时间函数： 编号 函数名 作用 1 CURDATE()&#x2F;CURRENT_DATE() 返回当前日期 2 CURRENT_TIME()&#x2F;CURTIME() 返回当前时间 3 CURRENT_TIMESTAMP() 返回当前日期和时间 4 ADDDATE(d,n) 计算起始日期 d 加上 n 天的日期 5 ADDTIME(t,n) 时间 t 加上 n 秒的时间 6 DATE() 从日期或日期时间表达式中提取日期值 7 DAY(d) 返回日期值 d 的日期部分 8 DATEDIFF(d1,d2) 计算日期 d1-&gt;d2 之间相隔的天数 9 DATE_FORMAT 按表达式 f的要求显示日期 d 10 DAYNAME(d) 返回日期 d 是星期几，如 Monday,Tuesday 11 DAYOFMONTH(d) 计算日期 d 是本月的第几天 12 DAYOFWEEK(d) 日期 d 今天是星期几，1 星期日，2 星期一，以此类推 13 DAYOFYEAR(d) 计算日期 d 是本年的第几天 14 UNIX_TIMESTAMP() 得到时间戳 15 FROM_UNIXTIME() 时间戳转日期 16 NOW() 返回当前的日期和时间 17 STR_TO_DATE() 将日期格式的字符转换成指定格式的日期 18 DATE_FORMAT() 将日期转换成字符(支持：- . &#x2F;分割年月日) 流程函数： 函数 功能 IF(cond, t, f) 如果 cond 为真，返回 t，否则返回 f CASE cond WHEN value1 THEN result … END 多重选择 其他常用函数： 函数 功能 DATABASE() 返回当前数据库名 VERSION() 返回数据库版本 USER() 返回当前登录用户名 INET_ATON(IP) 返回 IP 代表的 num INET_NTOA(num) 返回 num 代表的 IP PASSWORD() 返回加密版本 MD5() 返回 MD5 的值 第六章 图形化工具的使用MySQL Workbench： SQL 开发 数据建模 服务器管理 MySQL Utilities phpMyAdmin： 数据库管理 数据库对象管理 权限管理 导入导出数据 第七章 存储引擎（表类型）的选择存储引擎概述：根据不同领域的需要选择合适的存储引擎，可以更好地提高数据库的效率。在诸多的引擎中，支持事务安全的只有 InnoDB 和 BDB。默认的存储引擎可以通过 default-table-type 配置。使用 SHOW ENGINES 可以查看当前数据库支持的引擎。 各种存储引擎的特性： MyISAM：不支持事务、也不支持外键，其优势是访问的速度快，对事务完整性没有要求或者以 SELECT、INSERT 为主的应用基本上都可以使用这个引擎创建表。 InnoDB：提供了具有提交、回滚和崩溃恢复能力的事务安全。但是对比MyISAM的存储引擎，InnoDB写的处理效率差一些，并且会占用更多的磁盘空间以保留数据和索引。 MEMORY：使用存在于内存中的内容来创建表。每个MEMORY 表只实际对应一个磁盘文件，格式是.frm。MEMORY 类型的表访问非常地快，因为它的数据是放在内存中的，并且默认使用 HASH 索引，但是一旦服务关闭，表中的数据就会丢失掉。 MERGE：是一组 MyISAM 表的组合，这些 MyISAM 表必须结构完全相同，MERGE 表本身并没有数据，对 MERGE 类型的表可以进行查询、更新、删除操作，这些操作实际上是对内部 MyISAM 表进行的。 TokuDB：第三方引擎，是一个高性能、支持事务处理的MySQL和MariaDB的存储引擎，具有高扩展性、高压缩率、高效的写入性能，支持大多数在线DDL操作。 第八章 选择合适的数据类型CHAR 与 VARCHAR：下表是它们之间的对比，最后一行只适用于 MySQL 运行在非严格模式下面。 CHAR 长度固定，处理速度快，但是浪费空间存储。对于 MyISAM 和 MEMORY 来说，首选 CHAR，而对于 InnoDB 来说，建议使用VARCHAR类型。 TEXT 与 BLOB：在保存较大文本时，通常会选择使用TEXT或者BLOB。二者之间的主要差别是BLOB能用来保存二进制数据，比如照片；而TEXT只能保存字符数据，比如一篇文章或者日记。 BLOB 和 TEXT 值会造成性能问题，在执行删除操作之后，会在数据表中留下很大的空洞，可以定期使用 OPTIMIZE TABLE 来进行碎片整理。 使用合成索引来提高大文本字段的查询性能。合成索引就是根据大文本字段的内容建立一个散列值，并把这个值存储在单独的数据列中，接下来就可以通过检索散列值找到数据行了。注意只能用于精确匹配。 在不必要的时候避免检索大型的 BLOB 或 TEXT 值。 把 BLOB 或 TEXT 列分离到单独的表中，以减少主表的碎片 浮点数与定点数：浮点数不是精确的，定点数更加精确。float，doble 都是浮点数，decimal 则是定点数。 日期类型选择：根据实际需要选择能够满足应用的最小存储的日期类型。如果记录的日期需要让不同时区的用户使用，那么最好使用 TIMESTAMP，因为日期类型中只有它能够和实际时区相对应。 第九章 字符集常用字符集比较： 选择字符集标准： 如果应用要处理各种各样的文字，首选 utf-8 如果应用中涉及已有数据的导入，就要充分考虑数据库字符集对已有数据的兼容性 如果数据库只需要支持一般中文，数据量很大，性能要求也很高，那就应该选择双字节定长编码的中文字符集，比如 GBK 如果数据库需要做大量的字符运算，如比较、排序等，那么选择定长字符集可能更好 MySQL 支持的字符集简介：查看所有可用的字符集的命令是 show character set，MySQL 的字符集包含字符集（CHARACTER）和校对规则（COLLATION）两个概念。其中字符集用来定义 MySQL 存储字符串的方式，校对规则用来定义比较字符串的方式。校对规则命名约定：以其相关的字符集名开始，通常包括一个语言名，并且以_ci（大小写不敏感）、_cs（大小写敏感）或_bin（比较是基于字符编码的值而与language无关）结束。 MySQL 字符集设置：有4个级别的默认设置：服务器级、数据库级、表级和字段级。 服务器字符集：可以在 my.cnf 中配置character-set-server来设置。 数据库字符集：可以在创建数据库的时候指定，也可以在创建完数据库后通过“alter database”命令进行修改。后者并不能修改之前已经插入的数据的字符集。 表字符集：可以在创建表的时候指定，可以通过 alter table 命令进行修改，同样，如果表中已有记录，修改字符集对原有的记录并没有影响，不会按照新的字符集进行存放。 列字符集：可以定义列级别的字符集和校对规则，主要是针对相同的表不同字段需要使用不同的字符集的情况。 字符集的修改步骤：如果原来的数据库中已经存在数据，那么通过 alter database 或者 alter tablename 的方式并不能修改之前已经插入的数据的字符集。最好先使用 mysqldump 导出表定义，然后手动修改数据集将SET NAMES character，最后再次导入数据。 第十章 索引的设计和使用索引概述：索引用于快速找出在某个列中有一特定值的行，对相关列使用索引能提高 SELECT 操作性能的最佳途径，MySQL 支持前缀索引，还支持全文索引。 创建索引： 123CREATE [UNIQUE|FULLTEXT|SPATIAL] INDEX index_name[USING index_type]ON tbl_name (index_col_name,. .); 删除索引： 1DROP INDEX index_name ON tbl_name; 索引设计原则： 最适合索引的列是出现在 WHERE 子句中的列，或连接子句中指定的列 使用唯一索引 使用短索引 不过度使用索引 BTREE 索引与 HASH 索引： 使用 HASH 索引的时候，只能用于 &#x3D; 或者 &lt;&gt; 操作符比较，MySQL 不能确定两个值之间大约有多少行数据 对于 BTREE 索引，当使用 &gt;、&lt;、&gt;&#x3D;、&lt;&#x3D;、BETWEEN、!&#x3D; 或者 &lt;&gt;，或者LIKE ‘pattern’ 操作符时，都可以使用相关列上的索引 第十一章 视图视图：一种虚拟存在的表，对于使用视图的用户来说透明，视图相对于表的优点有：简单，安全和数据独立。 视图操作： 12345678910111213// 创建视图CREATE [OR REPLACE] VIEW view_name [(column_list)]AS select_statement[WITH [CASCADED | LOCAL] CHECK OPTION]// 修改视图ALTER VIEW view_name [(column_list)]AS select_statement[WITH [CASCADED | LOCAL] CHECK OPTION]// 删除视图DROP VIEW [IF EXISTS] view_name [, view_name] . .[RESTRICT | CASCADE]// 查看视图SHOW TABLES;SHOW CREATE VIEW view_name; WITH [CASCADED | LOCAL] CHECK OPTION决定了是否允许更新数据使记录不再满足视图的条件，其中LOCAL只要满足本视图的条件就可以更新，而CASCADED则必须满足所有针对该视图的所有视图的条件才可以更新。 第十二章 存储过程和函数存储过程和函数：它们都是一段 SQL 语句的集合，不同之处在于函数必须有返回值，并且其参数只能是 IN 类型的，合理使用它们可以减少数据传输量，但是在服务器上进行大量的运算也会占用服务器的 CPU，需要综合考虑。 存储过程和函数的相关操作： 1234567891011121314151617181920// 创建CREATE PROCUDURE p_name ([proc_parameter[,...]])[characteristic ..] routine_bodyCREATE FUNCTION f_name ([func_parameter[,. .]])RETURNS type[characteristic ..] routine_body// 修改ALTER &#123;PROCEDURE | FUNCTION&#125; sp_name [characteristic . .]// 调用CALL sp_name([parameter[,...]])// 删除DROP &#123;PROCEDURE | FUNCTION&#125; [IF EXISTS] sp_name// 查看SHOW &#123;PROCEDURE | FUNCTION&#125; STATUS [LIKE &#x27;pattern&#x27;] 通常，routine_body包含多条语句，为了不出现错误，我们可以使用DELIMITER $$命令将语句的结束符从“;”修改成其他符号（$$）。 characteristic特征值说明如下： LANGUAGE SQL：说明 BODY 是使用 SQL 语言编写的 [NOT] DETERMINISTIC：DETERMINISTIC确定的,即每次输入一样输出也一样的程序，NOT DETERMINISTIC非确定的，默认是非确定的 { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA }：提供额外信息给服务器 SQL SECURITY { DEFINER | INVOKER }：可以用来指定子程序该用创建子程序者的许可来执行，还是使用调用者的许可来执行。默认值是DEFINER 变量的使用： 123456// 定义DECLARE var_name[,. .] type [DEFAULT value]// 赋值SET var_name = expr [, var_name = expr] ..SELECT col_name[,. .] INTO var_name[,. .] table_expr 条件的使用： 12345678910111213// 定义DECLARE condition_name CONDITION FOR condition_valuecondition_value: SQLSTATE [VALUE] sqlstate_value | mysql_error_code// 条件处理DECLARE handler_type HANDLER FOR condition_value[,...] sp_statementhandler_type: CONTINUE | EXIT | UNDOcondition_value: SQLSTATE [VALUE] sqlstate_value| condition_name| SQLWARNING| NOT FOUND| SQLEXCEPTION| mysql_error_code 光标使用： 1234567// 声明DECLARE cursor_name CURSOR FOR select_statement// OPEN -&gt; FETCH -&gt; CLOSEOPEN cursor_nameFETCH cursor_name INTO var_name[, var_name]..CLOSE cursor_name 流程控制： 123456789101112131415161718192021222324252627282930// IFIF condition THEN statement_list[ELSEIF condition THEN statement_list] ...[ELSE statement_list]END IF// CASECASE case_valueWHEN when_value THEN statement_list[WHEN when_value THEN statement_list] ...[ELSE statement_list]END CASE// LOOP，通常结合 LEAVE 使用，LEAVE 作用类似于 BREAK[begin_label:] LOOPstatement_listEND LOOP [end_label]// ITERATE：跳过当前循环的剩下的语句，直接进入下一轮循环，类似 CONTINUE// REPEAT[begin_label:] REPEATstatement_listUNTIL conditionEND REPEAT [end_label]// WHILE[begin_label:] WHILE condition DOstatement_listEND WHILE [end_label] 事件调度器：可以在某个时间点触发操作，或者每隔一段时间执行固定代码： 12345678910// 时间点CREATE EVENT myeventON SCHEDULE AT CURRENT_TIMESTAMP + INTERVAL 1 HOURDOUPDATE myschema.mytable SET mycol = mycol + 1;// 时间间隔CREATE EVENT myeventON SCHEDULE EVERY 5 SECONDDOUPDATE myschema.mytable SET mycol = mycol + 1; 第十三章 触发器触发器操作： 1234567// 创建CREATE TRIGGER trigger_name [BEFORE | AFTER] [INSERT | DELETE | UPDATE]ON table_name FOR EACH ROW trigger_stmt// 删除DROP TRIGGER [schema_name.]trigger_name// 查看show triggers 触发器使用：在触发器中，使用别名 OLD 和 NEW 来引用发生变化的记录内容。另外，触发器存在如下限制： 触发程序不能调用将数据返回客户端的存储程序 不能在触发器中使用以显式或隐式方式开始或结束事务的语句 第十四章 事务控制和锁定语句锁定级别：MySQL 支持对 MyISAM 和 MEMORY 存储引擎的表进行表级锁定，对 BDB 存储引擎的表进行页级锁定，对 InnoDB 存储引擎的表进行行级锁定。 表锁定：当所需要的锁已经被其他线程获取，此时该线程会进行等待，其操作如下： 12345LOCK TABLEStbl_name &#123;READ [LOCAL] | [LOW_PRIORITY] WRITE&#125;[, tbl_name &#123;READ [LOCAL] | [LOW_PRIORITY] WRITE&#125;] ...UNLOCK TABLES 事务控制：默认情况，MySQL 是自动提交的，CHAIN 会立即启动一个新事务，并且和刚才的事务具有相同的隔离级别，RELEASE 则会断开和客户端的连接。另外，所有的 DDL 语句都是不能回滚的。在事务中可以通过定义 SAVEPOINT，指定回滚事务的一个部分，但是不能指定提交事务的一个部分。 1234START TRANSACTION | BEGIN [WORK]COMMIT [WORK] [AND [NO] CHAIN] [[NO] RELEASE]ROLLBACK [WORK] [AND [NO] CHAIN] [[NO] RELEASE]SET AUTOCOMMIT = &#123;0 | 1&#125; 分布式事务：分布式事务通常涉及到一个事务管理器和多个资源管理器，采用两阶段提交。 第十五章 SQL中的安全问题SQL 注入：利用数据库的外部接口将用户数据插入到实际的 SQL 语言中，从而达到入侵的目的。常见的语句：SELECT * FROM user WHERE username=&#39;$username&#39; AND password= &#39;$password&#39;;，此时对 username 赋值为angel&#39; or &#39;1=1，angel&#39;/*或angel&#39;#都会导致注入成功，前者使用逻辑，后者使用注释。 应对方式： PrepareStatement + Bind-Variable：通过转义用户输入的参数防护 使用应用程序提供的转换函数：如mysql_real_escape_string() 自定义：正则校验，特殊字符转义等 第十六章 SQL Mode及其相关问题SQL Mode 简介：SQL Mode 通常用来解决以下问题： 设置不同的 SQL Mode，可以设置不同程度的数据校验，保证准确性 通过设置为 ANSI 模式，便于迁移 在数据迁移之前，改变 SQL Mode，可以便于数据迁移 SQL Mode 功能： 校验日期数据合法性 MOD(X, 0) 在 TRADITIONAL 模式下会直接产生错误 启用 NO_BACKSLASH_ESCAPE 使得反斜线成为普通字符 启用 PIPES_AS_CONCAT 模式，将|视作字符串的链接操作符 常见的 SQL Mode： SQL Mode 在迁移中使用方式：可以通过组合不同的 sql_mode 来构成适合于其他数据库的数据格式，这样就可以使得导出的数据更容易导入到对应的数据库。 第十七章 MySQL分区概述：分区有利于管理非常大的表，采用分而治之的思想，将表分成一系列的分区，分区对于应用来说是完全透明的，具体而言，使用分区的好处有： 和单个磁盘相比，能存储更多的数据 优化查询 通过删除分区直接删除相关的数据 跨多个磁盘，能获得更大的吞吐量 分区类型：无论那种分区，都只能使用主键或者唯一键 RANGE 分区：给予一个给定的连续区间范围，将数据分配到不同分区。使用VALUES LESS THAN划定范围。 LIST 分区：类似 RANGE 分区，不过 LIST 对应的是枚举值。使用VALUES IN划定分区。 COLUMNS 分区：支持分区的键的数据类型更广，并且支持多列分区（多列排序）。又分为RANGE COLUMNS和LIST COLUMNS。 HASH 分区：给予给定的分区个数，将数据分区。主要用于分散热点读，确保负载均衡。使用PARTITION BY HASH(expr) PARTITIONS num进行分区，底层采用的是 MOD 算法。常规的 HASH 算法挺不错，但是需要增加分区或者合并分区的时候，问题就出现了，即需要重新 MOD 计算。为此，可以使用线性 HASH 分区，其优点在于存在分区维护的时候，MySQL 能够处理得更加迅速。 KEY 分区：类似于 HASH 分区，只不过 HASH 分区允许使用用户自定义的表达式，而 Key 分区不允许使用用户自定义的表达式，需要使用 MySQL 服务器提供的 HASH 函数。 子分区：指对每个分区的再次分割，使用SUBPARTITION BY语句实现。 分区管理： RANGE &amp; LIST 分区管理： 123456// 删除ALTER TABLE tbl_name DROP PARTITION p_name;// 添加ALTER TABLE tbl_name ADD PARTITION (patition_stmt);// 重新组织ALTER TABLE tbl_name REORGANIZE PARTITION p_name into (patition_stmt); HASH &amp; KEY 分区管理： 1234// 合并ALTER TABLE tbl_name COALESCE PARTITION p_num;// 增加ALTER TABLE tbl_name ADD PARTITIONS p_num; 第十八章 SQL优化优化 SQL 语句的一般步骤： 通过使用SHOW [SESSION | GLOBAL] STATUS命令了解各种 SQL 的执行频率 定位执行效率较低的 SQL 语句：使用--log-slow-queries=[file_name]启动，或者使用show processlist查看进程列表 通过 EXPLANIN 分析低效 SQL 的执行计划 通过 show profile分析 SQL 通过trace分析优化器如何选择执行计划 确定问题并且采取相应的优化措施 索引问题： 索引的存储分类：索引是在存储引擎层中实现的，MySQL 暂时提供以下四种类型的索引： 使用索引的场景： 匹配全值 匹配值的范围查询 匹配最左前缀：比如在 col1 + col2 + col3 字段上的联合索引能够被包含 col1、(col1 + col2)、(col1 + col2 + col3)的等值查询利用到 仅仅对索引进行查询 匹配列前缀：仅仅使用索引中的第一列,并且只包含索引第一列的开头一部分进行查找 能够实现索引匹配部分精确而其他部分进行范围匹配 如果列名是索引,那么使用 column_name is null 就会使用索引 .存在索引但不能使用索引的典型场景： 以%开头的 LIKE 查询不能够利用 B-Tree 索引 数据类型出现隐式转换的时候也不会使用索引 复合索引的情况下，不满足最左原则 用 or 分割开的条件，如果 or 前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到 查看索引使用情况：show status like ‘Handler_read%’; 简单的优化方法： 定期分析表和检查表：ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name 和 CHECK TABLE tbl_name [, tbl_name] … [option] 定期优化表：OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name。 常用的 SQL 优化： 大批量数据插入：使用 load 命令，通过简单设置可提高导入速度： 12345678// MyISAM 存储引擎的表ALTER TABLE tbl_name DISABLE KEYS;loading the dataALTER TABLE tbl_name ENABLE KEYS;// InnoDB 存储引擎的表SET UNIQUE_CHECKS=0loading the dataSET UNIQUE_CHECKS=1 优化 INSERT 语句：同一个客户插入很多行，尽量使用多个值表的 INSERT 语句 优化 ORDER BY 语句：MySQL 排序方式有使用有序索引，第二种是对返回数据排序（FileSort）。对于 FIleSort 算法，MySQL 有两种方式：两次扫描算法和一次扫描算法，通过增加 max_length_for_sort_data 的大小使得尽可能使用一次扫描算法。 优化 GROUP BY 语句：默认情况下，MySQL 对 GROUP BY 字段进行排序，如果想要不排序，可以追加使用 ORDER BY NULL 禁止排序。 优化嵌套查询：有些情况下，子查询可以被更有效率的 JOIN 替代 优化 OR 条件：对于含有 OR 的查询子句，如果要利用索引，则 OR 之间的每个条件列都必须用到索引；如果没有索引，则应该考虑增加索引。 优化分页查询：考虑分页场景limit 1000,20，此时 MySQL 排序出前 1020 条记录后仅仅需要返回第 1001-1020 条记录，而前 1000 条记录则会被抛弃。优化方案有： 在索引上完成排序分页的操作，最后根据主键关联回原表查询所需要的其他列内容。 把 LIMIIT 查询转换成某个位置的查询，如根据上次的查询的最大 id 计算下一次的最大的值。 常用 SQL 技巧： 正则表达式的使用 使用 RAND() 提取随机行：select * from category order by rand() 使用 BIT GROUP FUNCTION 做统计 使用外键需要注意的问题：在 MySQL 中，InnoDB 存储系统支持对外键约束条件的检查，而对于其他类型存储引擎的表则没有这种检查 第十九章 优化数据库对象优化表的数据类型：在设计表的时候需要考虑字段的长度留有一定的冗余，但是不推荐让很多字段留有大量的冗余，这会造成磁盘空间的浪费，可以使用PROCEDURE ANALYSE()对表进行分析。 通过拆分提高表的访问效率： 垂直拆分：一个表中的某些列常用，某些列不常用的情况，缺点是查询所有数据的时候需要联合数据 水平拆分：表很大，分割后可以降低查询时需要读的数据和索引的页数；表中的数据本来就具有某些独立性，如时间段，缺点在于查询所有数据的时候需要联合数据 逆规范化：数据的规范化程度并不是越高越好，规范化程度越高，产生的关系就越多，从而导致表之间的连接操作越频繁，而导致性能下降。常用的方法有增加冗余列、增加派生列、重新组表和分割表。 使用中间表提高统计查询速度：如查询最近一周的消费情况，就可以在原来的消费表上建立起来，优点在于中间表复制源表部分数据，并且与源表相“隔离”，另外，中间表上可以灵活地添加索引或增加临时用的新字段。 第二十章 锁问题MySQL 锁概述：MySQL 提供以下三类级别的锁： 表级锁：如 MyISAM 和 MEMORY 引擎，开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低 行级锁：如 InnoDB 引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高 页面锁：如 BDB 引擎，开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 MyISAM 表锁： 查询表级锁争用情况：检查 table_locks_waited 和 table_locks_immediate 状态变量 表级锁模式：表共享读锁和表独占写锁 如何加表锁： 隐式：执行查询语句时，自动加读锁；执行更新操作时，自动加写锁 显式：LOCK TABLES tbl_name [READ | WRITE] [LOCAL]，LOCAL 参数允许用户在表尾并发插入记录，另外需要注意别名问题 并发插入：存在系统变量 concurrent_insert，用于控制并发插入行为： 设置为 0 时，不允许并发插入。 设置为 1 时，如果 MyISAM 表中没有空洞(即表的中间没有被删除的行)，MyISAM允许在一个进程读表的同时，另一个进程从表尾插入记录。默认项 设置为 2 时，无论 MyISAM 表中有没有空洞，都允许在表尾并发插入记录。 锁调度：如果同时存在一个写进程和一个读进程获取相同项的锁，MySQL 会优先将锁给写进程。可以通过调整low-priority-updates变量改变调度行为；还有一个动态调度，设置max_write_lock_count，当一个表的写锁达到这个值后，MySQL 就暂时将写请求的优先级降低 InnoDB 锁问题：InnoDB 相较于 MyISAM 最大的不同点是支持事务和采用了行级锁。 背景：事务和 ACID 属性，并发事务处理带来的问题，事务隔离级别 查询行锁使用情况：检查 InnoDB_row_lock 状态变量 锁模式：行锁有共享锁和排他锁，表锁（意向锁）有 IS 和 IX 锁，意向锁是自动加的，对于更新操作，会加上 X 锁，对于普通的 SELECT 语句，则不会加任何锁，但是可以通过SELECT ... LOCK IN SHARE MODE获取 S 锁，通过SELECT ... FOR UPDATE获取 X 锁 行锁实现方式：一般分为以下三种： Record Lock：对索引项加锁。 Gap lock：对索引项之间的“间隙”、第一条记录前的“间隙”或最后一条记录后的“间隙”加锁。 Next-key lock：前两种的组合，对记录及其前面的间隙加锁。 其实现方式会导致： 在不通过索引条件查询时，InnoDB 会锁定表中的所有记录 虽然是访问不同行的记录，但是如果是使用相同的索引键，是会出现锁冲突的 当表有多个索引的时候，不同的事务可以使用不同的索引锁定不同的行，不论是使用主键索引、唯一索引或普通索引，InnoDB 都会使用行锁来对数据加锁 Next-Key 锁：对于键值在条件范围内但并不存在的记录，叫做“间隙(GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的 Next-Key 锁。 什么时候使用表锁：事务需要更新大部分或全部数据，表又比较大；事务涉及多个表，比较复杂，很可能引起死锁，造成大量事务回滚。 死锁：MyISAM 是 deadlock free 的，这是因为 MyISAM 一次获取需要的所有锁，而在InnoDB中，锁是逐步获得的。可以使用顺序加锁，申请大粒度锁等情况规避死锁。 第二十一章 优化MySQL ServerMySQL 体系结构概览：体系结构图如下： 主要包含有以下几类线程： master thread：主要负责将脏缓存页刷新到数据文件，执行 purge操作，触发检查点，合并插入缓冲区等 insert buffer thread：主要负责插入缓冲区的合并操作 read thread write thread log thread purge thread lock thread MySQL 内存管理及优化： MyISAM 内存优化：使用 key buffer 缓存索引块，对于数据块，则依赖于 IO 缓存 key_buffer_size 使用多个索引缓存：MySQL 通过 session 之间共享 key buffer 提高使用效率，但是不能消除其竞争 调整中点插入策略：对 LRU 的改进 调整 read_buffer_size 和 read_rnd_buffer_size InnoDB 内存优化：用一块内存区做 IO 缓存池，该缓存池不仅用来缓存 InnoDB 的索引块，而且也用来缓存InnoDB的数据块，缓存池逻辑上由 free list、flush list 和 LRU list 组成。InnoDB 使用的 LRU 算法是类似两级队列的方法 innodb_buffer_pool_size 的设置 调整 old sublist 大小 调整 innodb_old_blocks_time 的设置：确定从 old sublist 到 young sublist 的时间 调整缓存池数量 innodb_buffer_pool_instances 控制 innodb buffer 刷新，延长数据缓存时间 InnoDB doublewrite：原因是 MySQL 的数据页大小（一般是 16KB）与操作系统的 IO 数据页大小（一般是 4KB）不一致，无法保证 InnoDB 缓存页被完整、一致地刷新到磁盘。原理是用系统表空间中的一块连续磁盘空间（100个连续数据页，大小为 2MB）作为 doublewrite buffer，当进行脏页刷新时，首先将脏页的副本写到系统表空间的 doublewrite buffer 中，然后调用 fsync 刷新操作系统 IO 缓存，确保副本被真正写入磁盘。 调整排序缓存大小 sort_buffer_size 和连接缓存大小 join_buffer_size InnoDB log 机制及优化：采用 redo 日志，优化方法如下 innodb_flush_log_at_trx_commit 的设置 设置 log file size，控制检查点 调整 innodb_log_buffer_size 调整 MySQL 并发相关的参数： 调整 max connections，提高并发连接 调整 back_log：积压请求栈大小 调整 table_open_cache：控制所有 SQL 执行线程可打开表缓存的数量 调整 thread cache size innod block wait timeout 的设置 第二十二章 磁盘IO问题使用磁盘阵列：RAID 将数据分布到若干物理磁盘上，确保了数据存储的可靠性，同时提供并发读写的能力，常见的级别如下： 虚拟文件卷或软 RAID：相较于单个磁盘，性能有所改善 使用 Symbolic Links 分布 IO：默认情况下，数据库名和表名对应的就是文件系统的目录名和文件名，但是这样不利于多磁盘并发读写的能力，可以使用符号链接将不同的数据库指向不同的物理磁盘，达到分布磁盘 IO 的目的。 禁止操作系统更新文件的 atime 属性：LINUX 系统下，每次读取一个文件，操作系统就会将读操作的时间写回到磁盘上，这可能会影响 IO 性能。 使用裸设备存放 InnoDB 的共享表空间：对于 MyISAM，数据文件的读写完全依赖于操作系统，但是对于 InnoDB 来说，其自己实现了数据缓存机制，操作系统的缓存系统可能对其有反作用，可将数据放倒 Raw Device 上。 调整 IO 调度算法：传统硬盘读取数据分为将磁头移动到磁盘表面正确位置，将磁盘旋转，使得正确的数据移动到磁头下面，继续旋转，直到所有数据读取完。Linux 实现了四种 IO 调度算法： NOOP：不对 I&#x2F;O请求排序，除了合并请求也不会进行其他任何优化 最后期限（Deadline）算法：维护了一个拥有合并和排序功能的请求队列之外，额外维护了两个队列，分别是读请求队列和写请求队列，它们都是带有超时的FIFO队列。超时的请求会被先处理。 预期算法（Anticipatory）：和 Deadline 算法类似，不过当其处理完一个I&#x2F;O请求之后并不会直接返回处理下一个请求，而是等待片刻（默认 6ms），等待期间如果有新来的相邻扇区的请求，会直接处理新来的请求，当等待时间结束后，调度才返回处理下一个队列请求。 完全公平队列：把 I&#x2F;O请求按照进程分别放入进程对应的队列中，公平是针对进程而言的。 第二十三章 应用优化使用连接池：建立连接的代价较大，使用连接池可以减去建立新连接的开销。 减少对 MySQL 的访问： 避免对同一数据做重复索引 使用查询缓存：查询缓存（Query Cache）会保存 SELECT 查询的文本和相应的结果，每次更新就会清空缓存，适用于更新不频繁的表 增加 CACHE 层：如在用户端上建立一个二级数据库，将访问频率高的放在这个库上面 负载均衡： 利用 MySQL 复制分流查询操作：一个主服务器承担更新操作，而多台从服务器承担查询操作，主从之间通过复制实现数据的同步 采用分布式数据库架构 第二十五章 MySQL中的常用工具mysql（客户端连接工具）： 连接选项：-u，-p，-h，-P（端口） 客户端字符集选项：-default-character-set myisampack（MyISAM 表压缩工具）：可以使用很高的压缩率来对 MyISAM 存储引擎的表进行压缩，使得压缩后的表占用比压缩前小得多的磁盘空间。但是压缩后的表也将成为一个只读表，不能进行DML操作。 mysqladmin（MySQL 管理工具）：可以用它来检查服务器的配置和当前的状态、创建并删除数据库等。它的功能和mysql客户端非常类似，主要区别在于它更侧重于一些管理方面的功能，比如关闭数据库。 mysqlbinlog（日志管理工具）：由于服务器生成的二进制日志文件以二进制格式保存，所以如果想要检查这些文件的文本格式，就会用到 mysqlbinlog 日志管理工具。 mysqlcheck（MyISAM 表维护工具）：可以检查和修复 MyISAM 表，还可以优化和分析表。其集成了mysql工具中check、repair、analyze、optimize的功能。 mysqldump（数据导出工具）：用来备份数据库或在不同数据库之间进行数据迁移。 mysqlimport（数据导入工具）：客户端数据导入工具，用来导入 mysqldump 加 -T 选项后导出的文本文件。 mysqlshow（数据库对象查看工具）：用来很快地查找存在哪些数据库、数据库中的表、表中的列或索引。 第二十六章 MySQL日志错误日志：记录了当mysqld启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。 二进制日志：记录了所有的 DDL（数据定义语言）语句和 DML（数据操纵语言）语句，但是不包括数据查询语句。此日志对于灾难时的数据恢复起着极其重要的作用。格式有： STATEMENT：每一条对数据造成修改的SQL语句都会记录在日志中，这种格式的优点是日志记录清晰易读、日志量少，对I&#x2F;O影响较小。缺点是在某些情况下slave的日志复制会出错。 ROW：将每一行的变更记录到日志中，而不是记录SQL语句，优点是会记录每一行数据的变化细节，不会出现某些情况下无法复制的情况。缺点是日志量大，对I&#x2F;O影响较大。 MIXED：能尽量利用两种模式的优点，而避开它们的缺点。 查询日志：查询日志记录了客户端的所有语句，而二进制日志不包含只查询数据的语句。 慢查询日志：记录了所有执行时间超过参数 long_query_time 设置值并且扫描记录数不小于min_examined_row_limit 的所有 SQL 语句的日志。 日志查询分析工具：mysqlsla，myprofi 和 mysql-explain-slow-log 等。 第二十七章 备份与恢复备份恢复策略： 备份的表的存储引擎是事务型还是非事务型 全备份和增量备份 定期备份 使用复制方法来异地备份，但是复制对于数据库的误操作无能为力 逻辑备份和恢复：逻辑备份的最大优点是对于各种存储引擎都可以用同样的方法来备份；而物理备份则不同，不同的存储引擎有着不同的备份方法。 备份：将数据库中的数据备份为一个文本文件，可以被查看和编辑，可使用 mysqldump 工具实现 完全恢复：mysql –uroot –p dbname &lt; bakfile，还需要执行日志重做：mysqlbinlog binlog-file | mysql -u root –p*** 基于时间点恢复：某个时间点发生了误操作，我们只需要不完全恢复即可： 12shell&gt;mysqlbinlog --stop-date=&quot;2005-04-20 9:59:59&quot; /var/log/mysql/bin.123456 | mysql -u root –pmypwdshell&gt;mysqlbinlog --start-date=&quot;2005-04-20 10:01:00&quot; /var/log/mysql/bin.123456| mysql-u root -pmypwd \\ 基于位置恢复： 12shell&gt;mysqlbinlog --stop-position=&quot;368312&quot; /var/log/mysql/bin.123456 | mysql -u root -pmypwdshell&gt;mysqlbinlog --start-position=&quot;368315&quot; /var/log/mysql/bin.123456 | mysql -u root -pmypwd 物理备份和恢复： 冷备份：就是停掉数据库服务，cp 数据文件的方法。 热备份： MyISAM：本质是将要备份的表加读锁，然后再复制数据文件到备份目录 InnoDB：使用 ibbackup 或者 Xtrabackup 表的导入和导出： 导出： 使用 SELECT …INTO OUTFILE … 命令来导出数据 使用 mysqldump 导入： 使用 LOAD DATA INFILE… 命令，该命令加载数据最快 使用 mysqlimport 第二十八章 MySQL权限与安全MySQL 权限管理： 工作原理：首先对连接的用户进行身份认证，然后对通过的用户赋予对应权限 权限表：系统会用到 mysql 数据库下面的 user，host 和 db 这三个权限表 帐号管理： 12345678910// 创建帐号，赋予权限GRANT pri_type ON &#123;tbl_name | db_name.*&#125; TO user [IDENTIFIED BY [PASSWORD] &#x27;password&#x27;]]// 查看帐号权限show grants for user@host;// 撤销权限REVOKE pri_type ON &#123;tbl_name | db_name.*&#125; FROM user.// 修改账户密码SET PASSWORD FOR &#x27;jeffrey&#x27;@&#x27;%&#x27; = PASSWORD(&#x27;biscuit&#x27;);// 删除帐号DROP USER user MySQL 安全问题： 操作系统相关： 严格控制操作系统账号和权限 尽量避免以 root 权限运行 MySQL 防止 DNS 欺骗，最好使用 IP 地址而不是域名 数据库相关： 删除匿名帐号 给 root 帐号设置口令 只授予帐号必需的权限 除 root 外，任何用户不应有 mysql 库 user 表的存取权限 不要把 FILE、PROCESS 或 SUPER 权限授予管理员以外的账号 DROP TABLE 命令并不收回以前的相关访问授权 使用 SSL 其他安全设置选项： old-passwords：PASSWORD生成的密码是 16 位，在 4.1 之后，生成的函数值变为了 41 位 skip-grant-tables：不使用权限表 skip-network：适用于应用和数据库在一台机器上的情况 第三十章 MySQL常见问题和应用技巧忘记 MySQL 的 root 密码：首先手动 kill 掉 MySQL 进程，接着使用--skip-grant-tables选项重启登陆到 MySQL 服务，之后就可以更新密码，并且刷新权限表。 处理MyISAM存储引擎的表损坏： 使用 myisamchk 工具：myisamchk -r tablename 使用 SQL 命令：CHECK TABLE 和 REPAIR TABLE MyISAM 表超过 4GB 无法访问的问题：对数据文件的最大 size 进行扩充 磁盘目录空间不足问题：更改数据文件和索引文件的默认位置 DNS 反向解析问题：--skip-name-resolve mysql.sock 丢失后连接数据库：使用其他协议如--protocol=TCP|PIPE|SOCKET 第三十一章 MySQL复制MySQL 复制的优点： 主库出现问题，可以切换到从库提供服务 可以在从库上执行查询操作，降低主库的访问压力 可以在从库上执行备份，以避免备份期间影响主库的服务 复制概述：首先，MySQL 主库会在数据变更的时候将其记录在 Binlog 中，主库推送 binlog 中的事件到从库的中继日志 Relay Log，之后从库根据 Relay Log 重做数据变更操作，通过逻辑复制以此达到数据一致。复制流程如下： 复制中的各类文件：binlog 会把所有的数据修改操作以二进制的形式记录到文件中，binlog 支持三种格式： Statement：基于 SQL 语句级别的 Binlog，每条修改数据的 SQL 都会保存到 Binlog 里 Row：基于行级别，记录每一行数据的变化，数据量大 Mixed：混合 Statement 和 Row 模式 复制的三种常见架构： 一主多从复制架构：对实时性要求不是特别高的读请求通过负载均衡分布到多个从库上，降低主库的读取压力 多级复制架构：解决了一主多从场景下，主库的 I&#x2F;O 负载和网络压力，当然也有缺点：MySQL 的复制是异步复制，多级复制场景下主库的数据是经历两次复制才到达从库 双主复制：主库 Master1 和 Master2 互为主从，所有 Web Client 客户端的写请求都访问主库 Master1，而读请求可以选择访问主库 Master1 或 Master2 复制搭建过程： 异步复制： 半同步复制：为了保证主库上的每一个 Binlog 事务都能够被可靠的复制到从库上，主库在每次事务成功提交时，并不及时反馈给前端应用用户，而是等待其中一个从库也接收到 Binlog 事务并成功写入中继日志后，主库才返回Commit操作成功给客户端 第三十二章 MySQL ClusterMySQL Cluster 架构：节点类型可以分为三类：管理节点，SQL 节点和数据节点。前台应用一定的负载均衡算法将对数据库的访问分散到不同的 SQL 节点上，然后 SQL 节点对数据节点进行数据访问并从数据节点返回结果，最后 SQL 节点将收到的结果返给前台应用。 第三十三章 高可用架构MMM 架构：MMM（Master-Master replication manager for MySQL）是一套支持双主故障切换和双主日常管理的脚本程序。实现了故障切换的功能，另一方面其内部附加的工具脚本也可以实现多个 slaves 的 read 负载均衡。由于 MMM 无法完全地保证数据一致性，所以 MMM 适用于对数据的一致性要求不是很高，但是又想最大程度的的保证业务可用性的场景；对于那些对数据的一致性要求很高的业务，非常不建议采用 MMM 这种高可用性架构。 MHA 架构：MHA（Master High Availability）目前在 MySQL 高可用方面是一个相对成熟的解决方案，MHA能在最大程度上保证数据的一致性，以达到真正意义上的高可用。工作原理如下： 从宕机崩溃的 master 保存二进制日志事件 识别含有最新更新的 slave 应用差异的中继日志（relay log）到其他 slave 应用从 master 保存的二进制日志事件 提升一个 slave 为新 master 使其他的 slave 连接新的 master 进行复制","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.zsstrike.tech/tags/MySQL/"}]},{"title":"《Java并发编程实战》笔记","slug":"《Java并发编程实战》笔记","date":"2020-12-17T10:43:50.000Z","updated":"2022-05-16T07:41:46.165Z","comments":true,"path":"2020/12/17/《Java并发编程实战》笔记/","link":"","permalink":"http://blog.zsstrike.tech/2020/12/17/%E3%80%8AJava%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/","excerpt":"本文总结了《Java并发编程实战》中的关键点，可以用于查阅其中的知识点。","text":"本文总结了《Java并发编程实战》中的关键点，可以用于查阅其中的知识点。 第二章 线程安全性线程安全性：当多个类访问同一个类的时候，这个类始终都能表现出正确的行为，就称该线程是线程安全的。 原子性： 竞态条件（Race Condition）：当某个计算的正确性取决于多个线程的交替执行时序时，那么就会发生竞态条件。比如懒汉式单例模式中的 getInstance 方法，基于先检查后执行，由于需要检查 instance 是否为 null，再判断是否需要实例化，此时就存在竞态条件。 复合操作：指的是将一系列的操作合并成一个，使其满足原子性，比如 AtomicLong 里面的 incrementAndGet 方法。 加锁机制： 内置锁：使用关键字 synchronized 实现同步锁，修饰方法的时候锁就是方法调用所在的对象，静态的 synchronized 方法以 Class 对象为锁。 123synchronized (lock) &#123; // 访问或者修改由锁保护的共享对象&#125; 重入：当某个线程请求一个由其他线程持有的锁的时候，发出请求的线程会阻塞。但是如果一个线程试图获得一个已经由它自己持有的锁，那么这个请求就会成功。 第三章 对象的共享可见性： 失效数据：一个线程修改某个数据后，如果没有进行同步操作，另外一个线程再去读的话，可能就会读到之前的数据。 非原子的 64 位操作：Java 内存模型要求，变量的读取和写入都是原子操作，但是对于非 volatile 类型的 long 和 double 变量，JVM 允许将其分解为两个 32 位的操作。此时就可能发生失效数据的读取。 加锁与可见性：加锁的含义不仅在于互斥行为，还在于内存可见性，为了确保所有的线程能看到共享变量的最新值，所有执行读操作或者写操作的线程必须在同一个锁上同步。 volatile 变量：对变量的更新操作将会通知到其他线程。不建议过度使用 volatile 变量，因为volatile 变量只能保证可见性，不能确保原子性。 发布与逸出：发布指对象能够在当前作用域之外的代码中使用；逸出指的是当某个不应该被发布的对象被发布。不要在构造过程中使得 this 逸出，常见错误是在构造函数中启动一个线程。 线程封闭：避免同步的方式就是不共享数据，如果仅在单线程内访问数据，就不需要同步，这就是线程封闭。 Ad-hoc 线程封闭：维护线程封闭的职责完全由程序实现来承担。 栈封闭：是线程封闭的一种特例，在栈封闭中，只能通过局部变量才能访问到对象。 ThreadLocal 类：能使线程中的某个值与保存值的对象关联起来。ThreadLocal 提供 get 和 set 方法，这些方法为每个使用该变量的线程都存有一份独立副本，因此是线程独立的。通常用于防止对可变的单例变量或全局变量进行共享。 不变性：不可变对象一定是线程安全的。 final 域：用于构造不可变性对象，使用 final 修饰的域是不可更改的。另外，final 域可以确保初始化过程的安全性。 第四章 对象的组合设计线程安全的类：收集同步需求，以来状态的操作，状态的所有权。 实例封闭：封装简化了线程安全类的实现过程，当一个对象封装到另外一个对象中的时候，能够访问被封装对象的代码路径都是已知的，这样更适合对代码进行分析和加锁。 Java 监视器模式：使用私有锁对象而不是对象的内置锁的优点有，私有的锁对象可以将锁封装起来，但是客户端还是可以获取到共有方法来访问锁。 12345678910public class ProvateLock &#123; private final Object myLock = new Object(); Widget widget; void someMethod() &#123; synchronized (myLock) &#123; // 访问修改Widget的状态 &#125; &#125;&#125; 在现有的线程安全类中添加功能： 客户端加锁机制：对于使用某个对象 X 的客户端代码，使用 X 本身用于保护其状态的锁来保护这段客户端代码。 组合：使用组合方法构建对象，同时在上层再次加锁，实现同步。 第五章 基础构建模块同步容器类： 问题：同步容器类都是线程安全的，但是在某些情况需要额外的客户端加锁实现复合操作。 迭代器与 ConcurrentModificationException：如果在迭代期间对迭代对象进行了修改，可能就会抛出该异常。可以使用加锁来解决该问题，但是可能会带来验证 的性能问题。如果不想在迭代期间对对象进行加锁操作，可以先克隆容器，并在副本上迭代。 隐藏迭代器：比如打印一个 set 的时候就隐式用到了迭代器。 并发容器： ConcurrentHashMap：同步容器类在执行期间都持有一个锁，而并发容器类则使用了一种不同的加锁策略：使用粒度更细的加锁机制实现最大程度的共享，称为分段锁。该策略能够在并发编程的环境中实现更大的吞吐量。另外，并发容器类提供的迭代器不会抛出 ConcurrentModificationException，因此不需要在迭代过程中对容器加锁。由于他们返回的迭代器具有弱一致性，也即可以容忍并发的修改，当创建迭代器会遍历已有的元素，并可以（不保证）在迭代器构造后将修改反映给容器。 额外的原子 Map 操作：由于并发容器不支持加锁，因此我们不能基于加锁来实现复合操作。但是，一些复合原子操作已经内置提供：putifAbsent，remove和replace。 CopyOnWriteArrayList：用于替代 List，提供更好的并发性能，并且迭代器件不需要对容器加锁或者复制。每次修改容器的时候都会复制底层数组，需要一定开销。 阻塞队列和生产者-消费者模式： 阻塞队列：提供了可阻塞的 put 和 take 方法，以及支持定时的 offer 和 poll 方法。如果队列满了，那么 put 方法阻塞直到有空间可用。阻塞队列提供了 offer 方法，如果数据项不能添加到队列中，将返回失败状态，客户端可以根据此来调整生产者的数量。类库有 LinkedBlockingQueue 和 ArrayBlockingQueue。 串行线程封闭：对于可变对象，生产者-消费者和阻塞队列一起，促进了穿行线程封闭，从而将对象所有权从生产者交付给消费者。 双端队列：ArrayDeque 和 LinkedBlockingDeque。双端队列可用于另外一种工作模式，工作密取。在该模式下，每个消费者有自己的双端队列，如果一个消费者完成了自己双端队列中的全部工作，那么它可以从其他消费者双端队列末尾秘密获取工作。 阻塞方法与中断方法：线程可能会被阻塞，阻塞原因有：等待 IO 操作，等待一个锁，等待从 sleep 中醒来。阻塞的线程只有得到外部某个事件发生的时候，才能脱离阻塞，回到Runnable 状态。而中断是一种协作机制，一个线程不能强制要求其他线程停止正在执行的操作而去执行其他的操作。 同步工具类： 闭锁（Latch）：闭锁的作用相当于一扇门：在闭锁到达结束状态之前，这扇门一直关闭，并且没有任何线程能通过，当到达结束状态时，这扇门会打开并且允许所有线程通过。一旦到达结束状态后，就再也不会改变状态。在 Java 中可以使用 CountDownLatch。 FutureTask：也可用作闭锁。其状态有三种：等待执行，正在运行，运行完成。Future.get 的行为取决于任务的状态，如果任务完成，立即返回结果，否则将阻塞直到任务完成。 信号量（Semaphore）：计数信号量用来控制访问某个特定资源的操作数量，或者同时执行某个指定操作的数量。可以通过 acquire 和 release 来进行获取和释放信号量的操作。 栅栏（Barrier）：栅栏类似闭锁，它能阻塞一组线程直到某个时间发生。栅栏和闭锁的关键区别在于，所有线程必须同事到达栅栏位置，才能继续执行。闭锁用于等待事件，栅栏则用于等待其他线程。CyclicBarrier 可以使一定数量的参与方反复在栅栏处汇集。当线程达到栅栏的时候，调用 await 方法，这个方法将阻塞直到所有线程都到达栅栏位置。如果所有线程都到达，那么栅栏将会打开，此时所有线程将被释放，而栅栏被重置以便下次使用。如果对 await 的调用超时，那么栅栏就被认为是打破了，所有阻塞的 await 调用将终止并且抛出 BrokenBarrierException。 第六章 任务执行在线程中执行任务： 串行地执行任务：每次只会执行一个任务，但是执行的性能低下。 显式地为任务创建线程：通过为每个请求提供一个新的线程提供服务，实现更高的响应性。 无限制创建线程的不足：线程生命周期开销高，资源消耗，稳定性。 Executor 框架：Java 中，任务执行的主要抽象是 Executor，而不是 Thread。Executor 基于生产者-消费者模式。其接口定义： 123public interface Executor &#123; void execute(Runnable command);&#125; 线程池：指的是管理一组同构工作线程的资源池。通过重用现有的线程而不是创建新的线程来处理新的请求，可以减少新的线程的创建和销毁的开销。通常需要配置一个合适大小的线程池，使得提高处理器的效率和防止过多线程竞争资源使得内存耗尽。在 Java 中可以通过调用静态工厂方法来创建一个线程池：newFixedThreadPool，newCachedThreadPool，newSingleThreadPool，newScheduledThreadPool。 Executor 生命周期：添加了 ExecutorService 接口，其中包含了3种状态：运行，关闭和已终止。shutdown 方法将会执行平缓的关闭过程：不再接受新的任务，同时等待已经提交的任务执行完成。而 shutdownNow 方法则执行粗暴的关闭过程：它将尝试取消所有运行中的任务，同时不再启动队列中尚未开始的任务。 延迟任务和周期管理：Timer 类负责管理延迟任务以及周期任务。Timer 在执行所有的定时任务的时候只会创建一个线程。如果某个任务的执行时间过长，那么将会破坏其他 TimerTask 的定时精确性。基于以上原因，建议使用 ScheduledThreadPoolExecutor。 找出可利用的并行性： 携带结果的任务 Callable 和 Future：Runnable 是一种有很大局限的抽象，虽然 run 能写入到日志文件或者某个共享的数据结构，但是它不能返回一个值或者抛出一个异常。Callable 则是一种更好的抽象，他认为主入口点将返回一个值，并可能抛出一个异常。而 Future 则表示一个任务的生命周期，并且提供相应的方法来判断是否完成，以及获取任务的结果等。 12345678910public interface Callable&lt;V&gt; &#123; V call() throws Exception;&#125;public interface Future&lt;V&gt; &#123; boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException, CancellationException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, CancellationException, TimeoutException;&#125; CompletionService：将 Executor 与 BlockingQueue 的功能结合在一起，通过将一组 Callable 任务提交给它来执行，然后使用 take 和 poll 等方法来获得已经完成的结果，这些结果会被封装成 Future。ExecutorCompletionService 实现了 CompletionService。 为任务设置时限：可以通过 Future.get 来支持该需求。 第七章 取消与关闭任务取消： 中断：线程中断是一种协作机制，每个线程都有一个 boolean 类型的中断状态。当这个线程被被中断的时候，其状态设置为 true。对于中断操作的正确理解：它不会真正地中断一个正在运行的线程，而只是发出中断请求，然后由线程在下一个合适的时刻中断自己。 中断策略：合理的中断策略应该是某种形式的线程级取消操作或者是服务级取消操作。如果需要恢复中断状态： 1Thread.currentThread().interrupt(); 响应中断：当调用可中断的阻塞函数时，如 Thread.sleep，有两种策略用于处理 InterruptException。其一是传递异常，其二是恢复中断状态。 通过 Future 实现取消：ExecutorService.submit 将返回一个 Future 来描述任务。Future 拥有一个 cancel 方法，该方法带有一个 boolean 类型的参数 mayInterruptIfRunning。 采用 newTaskFor 来封装非标准的取消：newTaskFor 是一个工厂方法，它将创建 Future 代表任务，通过定制表示任务的 Future 可以改表 Future.cancel 的行为。 停止基于线程的服务： 关闭 ExecutorService：使用 shutdown 或者 shutdownNow。 毒丸对象：另外一种关闭生产者-消费者的方法就是使用毒丸对象：毒丸是指一个放在队列上的对象，当消费者得到这个对象的时候，立刻停止执行。 shutdownNow 的局限性：使用该方法的时候，它将会取消所有正在执行的任务，并且返回所有已经提交但尚未开始的任务。然而，我们并不知道那些任务已经开始但是尚未正常结束。 处理非正常的线程中止：导致线程提前死亡的原因主要就是 RuntimeException。如果没有捕获该异常，程序就会在控制台打印栈信息，然后退出执行。在 Thread 中提供了 UncaughtExceptionHandler，它能检测出某个线程由于未捕获的异常而终结的情况。 JVM 关闭： 关闭钩子：在正常的关闭中，JVM 首先调用所有已注册过的关闭钩子（Shutdown Hook）。JVM 不保证关闭钩子的调用顺序。 守护线程：守护线程不会阻碍 JVM 的关闭。应该尽量少使用守护线程。 终结期：在回收器释放对象之前，会调用它们的 finalize 方法，从而保证一些持久化的资源被释放。最好不要使用 finalize 方法进行资源回收。 第八章 线程池的使用在任务与执行策略之间的隐性耦合： 线程饥饿死锁：如果两个线程相互依赖对方的执行结果，那么就会发生饥饿死锁。 运行时间较长的任务：如果任务的执行时间较长，那么即使不出现死锁，线程池的响应性也会变得很糟糕。可以限定任务等待资源的事件，而不是无限制的等待。如果等待超时，那么需要中止任务或者将任务重新放回队列中。 设置线程池的大小：配置合适的线程池的大小既不会造成资源的浪费，也不会产生线程频繁切换的代价。 配置 ThreadPoolExecutor：可以使用它的通用构造函数来自定义： 1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; ... &#125; 线程的创建和销毁：线程池的基本大小，最大大小以及存活时间等因素共同负责线程的创建与销毁。 管理任务队列：ThreadPoolExecutor 允许提供一个 BlockingQueue 来保存等待执行的任务。基本的任务排队方式有三种：无界队列，有界队列和同步移交。 饱和策略：当有界队列被填满后，饱和策略开始发挥作用。如中止，抛弃，抛弃最旧的。调用者运行策略既不会抛弃任务，也不会抛出异常，而是将某些任务会退到调用者，从而降低新任务的流量。 线程工厂：每当线程池需要创建一个线程的时候，都是通过线程工厂方法来完成的。通过实现 ThreadFactory 接口，可以进行定制化的工作。 扩展 ThreadPoolExecutor：可以在子类中改写 beforeExecute，afterExecute 和 terminated 方法来实现定制。 第十章 避免活跃性危险死锁： 锁顺序死锁：两个线程试图以不同的顺序获得相同的锁。通过固定顺序获得锁，可以消除该问题。 动态的锁顺序死锁：考虑 transfer(from, to, amount)，如果存在transfer(A, B, 10) 和 transfer(B, A, 10) ，那么就可能发生死锁。 开放调用：如果在调用某个方法的时候不需要持有锁，那么称其为开放调用。 资源死锁：当多个线程相互持有彼此正在等待的锁而又不释放自己已经持有的锁的时候，就会发生死锁。 死锁的避免和检测： 支持定时的锁：可以使用 Lock 类的定时 tryLock 功能来代替内置锁机制。当使用内置锁的时候，只要没有获得锁就会一直等待下去，而显式锁则可以指明一个超时时限。 通过线程转储信息来分析死锁：线程转储信息中包含了加锁信息，例如每个线程持有了哪些锁，在那些栈帧中获得了这些锁，以及被阻塞的线程正在等待哪一个锁。 其他活跃性危险： 饥饿：当线程无法访问它所需要的资源而不能继续执行的时候，就会发生饥饿。引发饥饿的最常见资源就是 CPU 时钟周期。 糟糕的响应性：不良的锁管理也会导致糟糕的响应性。如果某个线程长时间占用锁（容器的迭代），其他想访问该容器的线程就必须等待很长时间。 活锁：该问题尽管不会阻塞线程，但也不能继续执行，因为线程将不断重复执行相同的操作，而且总会失败。可以通过引入随机性来解决该问题。 第十一章 性能与可伸缩性对性能的思考： 性能与可伸缩性：性能可以通过服务时间，延迟时间，吞吐率，效率等指标衡量。可伸缩性指的是当增加计算资源时，程序的吞吐量或者处理能力相应增加。 Amdahl 定律 线程引入的开销： 上下文切换 内存同步：同步可能使用特殊指令，即内存栅栏，该指令可以刷新缓存，使得缓存无效，从而使得各个线程都能看到最新的值。内存栅栏会抑制一些编译器的优化操作。 阻塞：当在锁上发生竞争的时候，竞争失败的线程就会阻塞。JVM 实现阻塞的行为有自旋等待，或者通过操作系统挂起。 减少锁的竞争： 缩小锁的范围 减少锁的粒度 锁分段：如 ConcurrentHashMap 一些替代独占锁的方法：使用并发容器，ReadWriteLock，不可变对象以及原子变量 第十二章 并发程序的测试正确性测试： 基本的单元测试 对阻塞操作的测试 安全性的测试 资源管理的测试 性能测试： 增加计时功能 多种算法比较 响应性衡量 避免性能测试的陷阱： 垃圾回收：垃圾回收的执行时序是无法预测的 动态编译 对代码路径的不真实采样 不真实的竞争程度 无用代码消除 其他的测试方法： 代码审查 静态分析工具 分析与检测工具 第十三章 显式锁Lock 与 ReentrantLock：Lock 接口提供了一种无条件的，可轮询的，定时的以及可中断的锁获取操作。ReentrantLock 则实现了 Lock 接口。 12345678public interface Lock &#123; void lock(); void lockInterruptibly() throws InterruptedException; boolean tryLock(); boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException; void unlock(); Condition newCondition();&#125; 轮询锁与定时锁：由 tryLock 方法实现，同时具有完善的错误恢复机制，使用这两种锁可以避免死锁的发生。 可中断的锁获取操作：lockInterruptibly 方法能够在获得锁的同时保持对中断的响应。 非块结构的加锁：内置锁中，锁的获取和释放操作都是基于代码块的。而分段锁技术则不是块结构的锁。 性能考虑因素：在 Java5 中，当线程数增大的时候，内置锁的性能急剧下降，而 ReentrantLock 的性能下降更加平缓。在 Java6 中，两者的可伸缩性基本相同。 公平性：ReentrantLock 的构造函数中提供了两种公平性的选择：创建一个非公平的锁（默认）或者一个公平的锁。在公平的锁上，线程将按照他们发出请求的顺序来获得锁，但是在非公平的锁上，则允许插队。大多数的情况下，非公平锁的性能高于公平锁的性能。 在 synchronized 和 ReentrantLock 之间进行选择：ReentrantLock 在加锁和内存上提供的语义与内置锁相同，另外，它还实现了其他功能，如定时的锁等待，公平性以及非块结构的加锁。内置锁则更加简洁，同时能在线程转储中给出哪些栈帧获得了哪些锁。 读写锁：ReentrantLock 实现了一种标准的互斥锁，互斥通常是一种过硬的加锁规则，因此限制了并发性。可以使用读写锁来改善： 1234public interface ReadWriteLock &#123; Lock readLock(); Lock writeLock();&#125; 在读写锁的加锁策略中，允许多个操作同时执行，但每次最多只允许一个写操作。ReentrantReadWriteLock 实现了上述接口，提供可重入的语义，同时构造的时候可以选择是否公平锁。 第十四章 构建自定义的同步工具状态依赖性的管理：如 BlockingQueue 中的 put 和 take 操作的前提分别时队列不为空或者满的状态，当前提没有满足的时候，可以抛出异常，或者保持阻塞直到条件被满足。 条件队列：使得一组线程（等待线程集合）能够通过某种方式来等待特定的条件变成真。Object 中的 wait，notify 和 notifyAll 方法就构成了内部条件队列的 API。Object.wait 会自动释放锁，并且请求操作系统挂起当前线程。当被挂起的线程醒来的时候，它将在返回之前重新获取锁。 使用条件队列： 条件谓词：指的是操作正常执行的前提条件，如队列不为空 过早唤醒：wait 方法的返回并不意味着线程正在等待的条件谓词已经变成真的了。因为可能被其他线程通过 notifyAll 唤醒，但是它的条件为此可能并未变为真的，此时就需要再次进行条件判断。 丢失的信号：也是一种活跃性故障。指的是线程必须等待一个已经为真的条件，但在开始等待之前没有检查条件谓词。 通知：在 put 方法成功执行后，将会调用 notifyAll，向任何等待“不为空”条件的线程发出通知。只使用 notify 可能会造成信号丢失的情况。 显式的 Condition 对象：正如 Lock 是一种广义的内置锁，Condition 也是一种广义的内置条件队列。 123456789public interface Condition &#123; void await() throws InterruptedException; boolean await(long time, TimeUnit unit) throws InterruptedException; long awaitNanos(long nanosTimeout) throws InterruptedException; void awaitUninterruptibly(); boolean awaitUntil(Date deadline) throws InterruptedException; void signal(); void signalAll();&#125; 内置锁的缺陷在于每个内置锁都只能有一个相关联的条件队列。而 Condition 和 Lock 一起使用就可以消除该问题。和内置条件队列不同的是，对于每个 Lock，可以有任意数量的 Condition 对象。在 Condition 中相应的方法是 await，signal 和 signalAll。 Synchronizer 剖析：在 ReentrantLock 和 Semaphore 两个接口存在许多共同点，如都可以用作阀门，即每次只允许一定数量的线程通过；都支持可中断；都支持公平和非公平的队列操作。事实上，它们的实现都使用了一个共同的基类，AbstractQueuedSynchronizer（AQS）。AQS 是一个用于构建锁和同步器的框架，CountDownLatch，ReentrantReadWriteLock 和 FutureTask 都是基于 AQS 实现。 同步容器中的 AQS： ReentrantLock：支持独占，实现了 tryAcquire，tryRelease 和 isHeldExclusively。将同步状态用于保存锁获取操作的次数，还维护一些 owner 的变量保存当前所有者线程的标识符。 Semaphore 和 CountDownLatch：前者将同步状态用于保存当前可用许可的数量，后者保存当前的计数值。 FutureTask：同步状态用来保存任务的状态。 第十五章 原子变量与非阻塞同步机制锁的劣势：重量级的同步方式，使用 volatile 变量可以同步，但是不支持原子操作，另外，当一个线程正在等待锁的时候，不能做任何有用的事情。 原子变量类： 原子变量是一种更好的 volatile：原子变量不但支持同步操作，还提供部分原子操作支持。 锁与原子变量的性能比较：在高度竞争的情况下，锁的性能超过原子变量的性能；而在适度竞争情况下，原子变量的性能超过锁的性能。 非阻塞算法：如果在某个算法中，一个线程的失败挥着挂起不会造成其他线程的失败或挂起，俺么该算法就是非阻塞算法。 非阻塞的栈 非阻塞的链表 原子的域更新器：compareAndSet 保证了操作的原子性 第十六章 Java 内存模型内存模型： 在共享内存的多处理器体系结构中，每个处理器有自己的缓存，并且定期的与主内存进行协调。在需要进行内存同步的时候，就可以执行内存栅栏指令，来保证数据的一致性。JVM 通过在合适的位置上插入内存栅栏来屏蔽 JMM 与底层平台内存模型的差异。 重排序 Java 内存模型：如果两个操作之间缺乏 Happens-Before 关系，那么 JVM 就可以对他们进行任意重排序。 Happens-Before 的规则包括： 程序顺序规则 监视器锁规则 volatile 变量规则 线程启动规则 线程结束规则 中断规则 终结期规则 传递性","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.zsstrike.tech/tags/Java/"}]},{"title":"《Effective Java》笔记","slug":"《Effective-Java》笔记","date":"2020-12-06T10:04:57.000Z","updated":"2022-05-16T07:41:46.126Z","comments":true,"path":"2020/12/06/《Effective-Java》笔记/","link":"","permalink":"http://blog.zsstrike.tech/2020/12/06/%E3%80%8AEffective-Java%E3%80%8B%E7%AC%94%E8%AE%B0/","excerpt":"本文是《Effective Java》第三版的读书笔记。","text":"本文是《Effective Java》第三版的读书笔记。 第二章 创建和销毁对象 考虑使用静态工厂方法代替构造函数：获取一个类的实例的传统方式是使用类提供的公开构造函数，另外一种方法是类提供公开静态工厂方法，用于返回实例。使用静态工厂方法优点： 静态工厂方法有确切名称，便于阅读 静态工厂方法不需要在每次调用时创建新对象 可以通过静态工厂方法获取返回类型的任何子类的对象，提供灵活性 返回对象的类可以随调用的不同而变化，作为输入参数的函数，声明的返回类型的任何子类型都是允许的 当编写包含方法的类时，返回对象的类不需要存在，如JDBC 静态工厂方法缺点： 没有公共或受保护构造函数的类不能被子类化 程序员很难找到它们，下面是一些静态工厂方法的常用名称： from：一种类型转换方法，接收单个参数并且返回相应实例 of：一个聚合方法，接受多个参数返回一个实例 valueOf：替代from和of但是更加冗长的方法 instance或getInstance：返回一个实例，该实例由参数描述，但具有不同的值（可能会缓存） create或newInstance：该方法保证每个调用都返回一个新实例 getType：类似于 getInstance，但如果工厂方法位于不同的类中，则使用此方法 newType：与 newInstance 类似，但是如果工厂方法在不同的类中使用 type：一个用来替代 getType 和 newType 的比较简单的方式 当构造函数有多个参数的时候，考虑使用构造器：静态工厂和构造函数都有一个局限，就是不能对大量可选参数做很好的扩展。当我们的可选参数个数大于4个时，往往需要重载很多个构造函数，会降低代码的可维护性。另外一种选择是JavaBean模式，但是JavaBean可能在构建的过程中处于不一致状态。此时我们可以使用构造器来生成所需对象。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// Builder Patternpublic class NutritionFacts &#123; private final int servingSize; private final int servings; private final int calories; private final int fat; private final int sodium; private final int carbohydrate; public static class Builder &#123; // Required parameters private final int servingSize; private final int servings; // Optional parameters - initialized to default values private int calories = 0; private int fat = 0; private int sodium = 0; private int carbohydrate = 0; public Builder(int servingSize, int servings) &#123; this.servingSize = servingSize; this.servings = servings; &#125; public Builder calories(int val) &#123; calories = val; return this; &#125; public Builder fat(int val) &#123; fat = val; return this; &#125; public Builder sodium(int val) &#123; sodium = val; return this; &#125; public Builder carbohydrate(int val) &#123; carbohydrate = val; return this; &#125; public NutritionFacts build() &#123; return new NutritionFacts(this); &#125; &#125; private NutritionFacts(Builder builder) &#123; servingSize = builder.servingSize; servings = builder.servings; calories = builder.calories; fat = builder.fat; sodium = builder.sodium; carbohydrate = builder.carbohydrate; &#125;&#125; 这样我们在生成代码的时候就可以通过链式调用来生成我们的对象实例。构造器模式很灵活，一个构造器可以构造多个对象。但是构造器的缺点就是为了创建一个对象，必须首先创建它的构造器。 使用私有构造函数或枚举类型实施单例模式：实现单例模式的第一种方法： 123456// Singleton with public final fieldpublic class Elvis &#123; public static final Elvis INSTANCE = new Elvis(); private Elvis() &#123; ... &#125; public void leaveTheBuilding() &#123; ... &#125;&#125; 上述代码可以防止用户来自己创建Elvis实例。但是拥有特殊权限的客户端可以借助AccessibleObject.setAccessible 方法利用反射调用私有构造函数，如果需要防范这种问题，需要修改构造器，使其在请求创建第二个实例的时候抛出异常即可。另外一种方法： 1234567// Singleton with static factorypublic class Elvis &#123; private static final Elvis INSTANCE = new Elvis(); private Elvis() &#123; ... &#125; public static Elvis getInstance() &#123; return INSTANCE; &#125; public void leaveTheBuilding() &#123; ... &#125;&#125; 静态工厂方法的一个优点是，它可以在不更改 API 的情况下决定类是否是单例，如为每个线程返回一个单例；第二个优点是，如果应用程序需要的话，可以编写泛型的单例工厂。实现单例的第三种方法： 12345// Enum singleton - the preferred approachpublic enum Elvis &#123; INSTANCE; public void leaveTheBuilding() &#123; ... &#125;&#125; 这种方法类似于 public 字段方法，但是它更简洁，默认提供了序列化机制，提供了对多个实例化的严格保证，即使面对复杂的序列化或反射攻击也是如此。 用私有构造函数实现不可实例化：对于一个工具类库，如Arrays，实例化这些类是没有意义的。试图通过使类抽象来实施不可实例化是行不通的。因为可以对类进行子类化，并实例化子类。有一个简单的习惯用法来确保不可实例化。只有当类不包含显式构造函数时，才会生成默认构造函数，因此可以通过包含私有构造函数使类不可实例化： 1234567// Noninstantiable utility classpublic class UtilityClass &#123; // Suppress default constructor for noninstantiability private UtilityClass() &#123; throw new AssertionError(); &#125; ... // Remainder omitted&#125; 因为显式构造函数是私有的，所以在类之外是不可访问的。AssertionError 不是严格要求的，但是它提供了保障，以防构造函数意外地被调用。 依赖注入优于硬连接资源：尽量将类依赖的资源在创建新实例时将资源传递给构造函数，从而实现依赖注入。 123456789// Dependency injection provides flexibility and testabilitypublic class SpellChecker &#123; private final Lexicon dictionary; public SpellChecker(Lexicon dictionary) &#123; this.dictionary = Objects.requireNonNull(dictionary); &#125; public boolean isValid(String word) &#123; ... &#125; public List&lt;String&gt; suggestions(String typo) &#123; ... &#125;&#125; 另外，这种模式的一个有用变体是将资源工厂传递给构造函数。Java 8 中引入的 Supplier&lt;T&gt; 非常适合表示工厂。尽管依赖注入极大地提高了灵活性和可测试性，但它可能会使大型项目变得混乱，这些项目通常包含数千个依赖项。 避免创建不必要的对象：作为一个不该做的极端例子，请考虑下面的语句： 1String s = new String(&quot;bikini&quot;); // DON&#x27;T DO THIS! 该语句每次执行时都会创建一个新的 String 实例，而这些对象创建都不是必需的。String 构造函数的参数 (&quot;bikini&quot;) 本身就是一个 String 实例，在功能上与构造函数创建的所有对象相同。另外，有些对象的创建代价很高，如果你需要重复地使用这样一个「昂贵的对象」，那么最好将其缓存以供复用： 1234567// Reusing expensive object for improved performancepublic class RomanNumerals &#123; private static final Pattern ROMAN = Pattern.compile(&quot;^(?=.)M*(C[MD]|D?C&#123;0,3&#125;)&quot; + &quot;(X[CL]|L?X&#123;0,3&#125;)(I[XV]|V?I&#123;0,3&#125;)$&quot;); static boolean isRomanNumeral(String s) &#123; return ROMAN.matcher(s).matches(); &#125;&#125; 另外，还需要注意基本类型优于包装类，需要提防意外的自动自动装箱。 排除过时的对象引用：考虑一个栈的pop操作： 12345public Object pop() &#123; if (size == 0) throw new EmptyStackException(); return elements[--size];&#125; 上述代码没有明显的问题，但是存在内存泄露的隐患：如果堆栈增长，然后收缩，那么从堆栈中弹出的对象将不会被垃圾收集，即使使用堆栈的程序不再引用它们。改进方式： 1234567public Object pop() &#123; if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result;&#125; 一般来说，一个类管理它自己的内存时，程序员应该警惕内存泄漏。当释放一个元素时，该元素中包含的任何对象引用都应该被置为 null。 另一个常见的内存泄漏源是缓存。一旦将对象引用放入缓存中，就很容易忘记它就在那里，并且在它变得无关紧要之后很久仍将它留在缓存中。如果你非常幸运地实现了一个缓存，只要缓存外有对其键的引用，那么就将缓存表示为 WeakHashMap，当条目过时后，条目将被自动删除。 内存泄漏的第三个常见来源是侦听器和其他回调。 如果你实现了一个 API，其中客户端注册回调，但不显式取消它们，除非你采取一些行动，否则它们将累积。 避免使用终结器和清除器：终结器是不可预测的，通常是危险的，也是不必要的。清除器的危险比终结器小，但仍然不可预测、缓慢，而且通常是不必要的。终结器和清除器的一个缺点是不能保证它们会被立即执行，另外一个缺点是它们可能会使的即将要被清理的对象死而复生。终结器和清除器可以充当一个安全网，以防资源的所有者忽略调用它的 close 方法。 使用try-with-resources优于try-finally：从历史上看，try-finally 语句是确保正确关闭资源的最佳方法，即使在出现异常或返回时也是如此。但是当存在两个资源的时候，可能就需要嵌套的调用了，这会导致代码不易阅读。最好的方法就是使用try-with-resources： 123456789// try-with-resources on multiple resources - short and sweetstatic void copy(String src, String dst) throws IOException &#123; try (InputStream in = new FileInputStream(src);OutputStream out = new FileOutputStream(dst)) &#123; byte[] buf = new byte[BUFFER_SIZE]; int n; while ((n = in.read(buf)) &gt;= 0) out.write(buf, 0, n); &#125;&#125; 第三章 对象的通用方法 覆盖 equals 方法时应该遵守的约定：当满足下面的条件的时候，不应该覆盖equals方法： 类的每个实例本质上是唯一的 该类不需要提供逻辑相等测试 超类已经覆盖了equals，超类行为适合于这个类 类是私有的或者包私有的，并且你确信它的 equals 方法永远不会被调用 equals方法实现了等价关系：反身性，对称性，传递性，一致性，最后还需要满足非无效性：即o.equals(null)返回false。为了搞笑实现equals方法，需要： 使用 &#x3D;&#x3D; 运算符检查参数是否是对该对象的引用 使用 instanceof 运算符检查参数是否具有正确的类型 将参数转换为正确的类型 对于类中的每个「重要」字段，检查参数的字段是否与该对象的相应字段匹配 是否满足等价关系 当覆盖 equals 方法的时候，总要覆盖 hashCode 方法：由于相等的对象必须具有相等的散列码，如果PhoneNumber没有实现hashCode方法的话： 123Map&lt;PhoneNumber, String&gt; m = new HashMap&lt;&gt;();m.put(new PhoneNumber(707, 867, 5309), &quot;Jenny&quot;);// m.get(new PhoneNumber(707, 867,5309)) == null 第三行的结果将是null，而不是&quot;Jenny&quot;。实现hashCode方法的一个简单方法步骤： 声明一个名为 result 的 int 变量，并将其初始化为对象中第一个重要字段的散列码 c 对象中剩余的重要字段 f，执行以下操作： 为字段计算一个整数散列码 c：如果字段是基本数据类型，计算 Type.hashCode(f)，其中 type 是与 f 类型对应的包装类。如果字段是对象引用，并且该类的 equals 方法通过递归调用 equals 方法来比较字段，则递归调用字段上的 hashCode 方法。如果字段是一个数组，则将其每个重要元素都视为一个单独的字段。也就是说，通过递归地应用这些规则计算每个重要元素的散列码，并将每个步骤 2.b 的值组合起来。如果数组中没有重要元素，则使用常量，最好不是 0。如果所有元素都很重要，那么使用 Arrays.hashCode。 将步骤 2.a 中计算的散列码 c 合并到 result 变量 返回result 一个简单的demo： 12345678// Typical hashCode method@Overridepublic int hashCode() &#123; int result = Short.hashCode(areaCode); result = 31 * result + Short.hashCode(prefix); result = 31 * result + Short.hashCode(lineNum); return result;&#125; 始终覆盖 toString 方法：虽然Object提供了默认的toString方法，但是它返回的字符串通常不是用户希望看到的。提供一个好的 toString 实现（能）使类更易于使用，使用该类的系统（也）更易于调试。当实际使用时，toString 方法应该返回对象中包含的所有有用信息。 明智地覆盖 clone 方法：Cloneable 接口的目的是作为 mixin 接口，用于让类来宣称它们允许克隆。不幸的是，它没有达到这个目的。它的主要缺点是缺少 clone 方法，并且 Object 类的 clone 方法是受保护的。它决定了 Object 类受保护的 clone 实现的行为：如果一个类实现了 Cloneable 接口，Object 类的 clone 方法则返回该类实例的逐字段拷贝；否则它会抛出 CloneNotSupportedException。默认提供的clone方法执行的是浅拷贝，如果需要深拷贝，就需要自己覆盖clone方法，实现该功能。 考虑实现 Comparable 接口：与本章讨论的其他方法不同，compareTo 方法不是在 Object 中声明的。相反，它是 Comparable 接口中的唯一方法。通过让类实现 Comparable，就可与依赖于此接口的所有通用算法和集合实现进行互操作。如果一个类有多个重要的字段，此时就需要用户来指定对应的比较顺序。在 Java 8 中，Comparator 接口配备了一组比较器构造方法，可以流畅地构造比较器。然后可以使用这些比较器来实现 Comparator 接口所要求的 compareTo 方法。 12345678// Comparable with comparator construction methodsprivate static final Comparator&lt;PhoneNumber&gt; COMPARATOR = comparingInt((PhoneNumber pn) -&gt; pn.areaCode) .thenComparingInt(pn -&gt; pn.prefix) .thenComparingInt(pn -&gt; pn.lineNum);public int compareTo(PhoneNumber pn) &#123; return COMPARATOR.compare(this, pn);&#125; 第四章 类和接口 尽量减少类和成员的可访问性：隐藏内部数据和其他实现细节用于实现信息封装，可以解耦组成系统的组件。通用方法是让每个类或者成员尽可能不可访问。对于顶级（非嵌套）类和接口，只有两个可能的访问级别：包私有和公共。如果一个方法覆盖了超类方法，那么它在子类中的访问级别就不能比超类更严格。公共类的实例字段很少采用 public 修饰，因为带有公共可变字段的类通常不是线程安全的。请注意，非零长度的数组总是可变的，因此对于类来说，拥有一个公共静态 final 数组字段或返回该字段的访问器是错误的。如果一个类具有这样的字段或访问器，客户端将能够修改数组的内容。对于 Java 9，作为模块系统的一部分，还引入了另外两个隐式访问级别。模块是包的分组单位，就像包是类的分组单位一样。模块可以通过模块声明中的导出声明显式地导出它的一些包。 在公共类中，使用访问器方法，而不是公共字段：如果类可以在包之外访问，那么提供访问器方法来保持更改类内部表示的灵活性。但是，如果一个类是包级私有的或者是私有嵌套类，那么公开它的数据字段并没有什么本质上的错误。无论是在类定义还是在使用它的客户端代码中，这种方法产生的视觉混乱都比访问方法少。虽然公共类直接公开字段从来都不是一个好主意，但是如果字段是不可变的，那么危害就会小一些。 减少可变性：不可变类就是一个实例不能被修改的类。要使类不可变，请遵循以下 5 条规则： 不要提供修改对象状态的方法 确保类不能被扩展 所有字段用 final 修饰 所有字段设为私有 确保对任何可变组件的独占访问 不可变对象提供的好处： 不可变对象本质上是线程安全的 不可变对象可以很好的作为其他对象的构建模块 不可变对象自带提供故障原子性。他们的状态从未改变，所以不可能出现暂时的不一致。 不可变类的主要缺点是每个不同的值都需要一个单独的对象。 优先选择复合而不是继承：在包中使用继承是安全的，其中子类和超类实现由相同的程序员控制。在对专为扩展而设计和文档化的类时使用继承也是安全的。然而，对普通的具体类进行跨包边界的继承是危险的。与方法调用不同，继承破坏了封装。换句话说，子类的功能正确与否依赖于它的超类的实现细节。子类脆弱的一个原因是他们的超类可以在后续版本中获得新的方法。有一种方法可以避免上述所有问题。与其扩展现有类，不如为新类提供一个引用现有类实例的私有字段。这种设计称为复合，因为现有的类是新类的一个组件。只有在子类确实是超类的子类型的情况下，继承才合适。换句话说，只有当两个类之间存在「is-a」关系时，类 B 才应该扩展类 A。 继承要设计良好并且具有文档，否则禁止使用：首先，类必须精确地在文档中记录覆盖任何方法的效果。换句话说，类必须在文档中记录它对可覆盖方法的自用性。对于每个公共或受保护的方法，文档必须指出方法调用的可覆盖方法、调用顺序以及每次调用的结果如何影响后续处理过程。但是，这是否违背了一个格言：好的 API 文档应该描述一个给定的方法做什么，而不是如何做？是的，它确实违背了！这是继承违反封装这一事实的不幸结果。要为一个类编制文档，使其能够安全地子类化，你必须描述实现细节，否则这些细节应该是未指定的。为了允许继承，类必须遵守更多的限制。构造函数不能直接或间接调用可重写的方法。 如果你违反了这个规则，程序就会失败。超类构造函数在子类构造函数之前运行，因此在子类构造函数运行之前将调用子类中的覆盖方法。如果重写方法依赖于子类构造函数执行的任何初始化，则该方法的行为将不像预期的那样。 接口优于抽象类：Java 有两种机制来定义允许多种实现的类型：接口和抽象类。由于 Java 8 中引入了接口的默认方法，这两种机制都允许你为一些实例方法提供实现。一个主要区别是，一个类要实现抽象类定义的类型，该类必须是抽象类的子类。因为 Java 只允许单一继承，这种限制对抽象类而言严重制约了它们作为类型定义的使用。使用接口的优点： 可以很容易地对现有类进行改造，以实现新的接口 接口是定义 mixin（混合类型）的理想工具 接口允许构造非层次化类型框架 为后代设计接口：在 Java 8 之前，在不破坏现有实现的情况下向接口添加方法是不可能的。如果在接口中添加新方法，通常导致现有的实现出现编译时错误，提示缺少该方法。在 Java 8 中，添加了默认的方法构造，目的是允许向现有接口添加方法。除非必要，否则应该避免使用默认方法向现有接口添加新方法，在这种情况下，你应该仔细考虑现有接口实现是否可能被默认方法破坏。尽管默认方法现在已经是 Java 平台的一部分，但是谨慎地设计接口仍然是非常重要的。虽然默认方法使向现有接口添加方法成为可能，但这样做存在很大风险。 如果一个接口包含一个小缺陷，它可能会永远影响它的使用者；如果接口有严重缺陷，它可能会毁掉包含它的 API。 接口只用于定义类型：当一个类实现了一个接口时，这个接口作为一种类型，可以用来引用类的实例。不满足上述条件的一种接口是所谓的常量接口。如果你想导出常量，有几个合理的选择。如果这些常量与现有的类或接口紧密绑定，则应该将它们添加到类或接口。例如，所有数值包装类，比如 Integer 和 Double，都导出 MIN_VALUE 和 MAX_VALUE 常量。如果将这些常量看作枚举类型的成员，那么应该使用 enum 类型导出它们。否则，你应该使用不可实例化的工具类导出常量。 类层次结构优于带标签的类：有时候，你可能会遇到这样一个类，它的实例有两种或两种以上的样式，并且包含一个标签字段来表示实例的样式。这样的标签类有许多缺点。它们充斥着样板代码，包括 enum 声明、标签字段和 switch 语句。标签类冗长、容易出错和低效。面向对象的语言提供了一个更好的选择来定义能够表示多种类型对象的单一数据类型：子类型。标签类只是类层次结构的简易模仿。 静态成员类优于非静态成员类：有四种嵌套类：静态成员类、非静态成员类、匿名类和局部类。除了第一种，所有的类都被称为内部类。静态成员类是最简单的嵌套类。最好把它看做是一个普通的类，只是碰巧在另一个类中声明而已，并且可以访问外部类的所有成员，甚至那些声明为 private 的成员。静态成员类的一个常见用法是作为公有的辅助类。从语法上讲，静态成员类和非静态成员类之间的唯一区别是静态成员类在其声明中具有修饰符 static。如果声明的成员类不需要访问外部的实例，那么应始终在声明中添加 static 修饰符，使其成为静态的而不是非静态的成员类。匿名类的适用性有很多限制。你不能实例化它们，除非在声明它们的时候。在 lambda 表达式被添加到 Java 之前，匿名类是动态创建小型函数对象和进程对象的首选方法，但 lambda 表达式现在是首选方法。局部类是四种嵌套类中最不常用的。局部类几乎可以在任何能够声明局部变量的地方使用，并且遵守相同的作用域规则。局部类具有与其他嵌套类相同的属性。 源文件仅限有单个顶层类：虽然 Java 编译器允许你在单个源文件中定义多个顶层类，但这样做没有任何好处，而且存在重大风险。这种风险源于这样一个事实：在源文件中定义多个顶层类使得为一个类提供多个定义成为可能。 第五章 泛型 不要使用原始类型：声明中具有一个或多个类型参数的类或接口就是泛型类或泛型接口，每个泛型都定义了一个原始类型，它是没有任何相关类型参数的泛型的名称。例如，List&lt;E&gt; 对应的原始类型是 List。原始类型的行为就好像所有泛型信息都从类型声明中删除了一样。它们的存在主要是为了与之前的泛型代码兼容。当从集合中检索元素时，编译器会为你执行不可见的强制类型转换，并确保它们不会失败。使用原始类型（没有类型参数的泛型）是合法的，但是你永远不应该这样做。如果使用原始类型，就会失去泛型的安全性和表现力。考虑如下程序： 1234567891011// Fails at runtime - unsafeAdd method uses a raw type (List)!public static void main(String[] args) &#123; List&lt;String&gt; strings = new ArrayList&lt;&gt;(); unsafeAdd(strings, Integer.valueOf(42)); String s = strings.get(0); // Has compiler-generated cast&#125;private static void unsafeAdd(List list, Object o) &#123; list.add(o);&#125; 该程序可以编译，但因为它使用原始类型 List，所以你会得到一个警告： 1234Test.java:10: warning: [unchecked] unchecked call to add(E) as amember of the raw type Listlist.add(o);^ 实际上，如果你运行程序，当程序试图将调用 strings.get(0) 的结果强制转换为字符串时，你会得到一个 ClassCastException。这是一个由编译器生成的强制类型转换，它通常都能成功，但在本例中，我们忽略了编译器的警告，并为此付出了代价。 如果将 unsafeAdd 声明中的原始类型 List 替换为参数化类型 List，并尝试重新编译程序，你会发现它不再编译，而是发出错误消息： 1234Test.java:5: error: incompatible types: List&lt;String&gt; cannot beconverted to List&lt;Object&gt;unsafeAdd(strings, Integer.valueOf(42));^ 对于元素类型未知且无关紧要的集合，你可能会尝试使用原始类型。这种方法是可行的，但是它使用的是原始类型，这是很危险的。安全的替代方法是使用无界通配符类型。如果你想使用泛型，但不知道或不关心实际的类型参数是什么，那么可以使用问号代替。 12// Uses unbounded wildcard type - typesafe and flexiblestatic int numElementsInCommon(Set&lt;?&gt; s1, Set&lt;?&gt; s2) &#123; ... &#125; 对于不应该使用原始类型的规则，有一些小的例外。必须在类字面量中使用原始类型。换句话说，List.class，String[].class 和 int.class 都是合法的，但是 List.class 和 List.class 不是。第二个例外是 instanceof 运算符。由于泛型信息在运行时被删除，因此在不是无界通配符类型之外的参数化类型上使用 instanceof 操作符是非法的。使用无界通配符类型代替原始类型不会以任何方式影响 instanceof 运算符的行为。在这种情况下，尖括号和问号只是多余的。下面的例子是使用通用类型 instanceof 运算符的首选方法： 12345// Legitimate use of raw type - instanceof operatorif (o instanceof Set) &#123; // Raw type Set&lt;?&gt; s = (Set&lt;?&gt;) o; // Wildcard type ...&#125; 总之，使用原始类型可能会在运行时导致异常，所以不要轻易使用它们。它们仅用于与引入泛型之前的遗留代码进行兼容和互操作。快速回顾一下，Set 是一个参数化类型，表示可以包含任何类型的对象的集合，Set 是一个通配符类型，表示只能包含某种未知类型的对象的集合，Set 是一个原始类型，它选择了泛型系统。前两个是安全的，后一个就不安全了。 消除 unchecked 警告：使用泛型获得的经验越多，得到的警告就越少，但是不要期望新编写的代码能够完全正确地编译。力求消除所有 unchecked 警告。 如果你消除了所有警告，你就可以确信你的代码是类型安全的，这是一件非常好的事情。如果不能消除警告，但是可以证明引发警告的代码是类型安全的，那么（并且只有在那时）使用 SuppressWarnings(“unchecked”) 注解来抑制警告。SuppressWarnings 注解可以用于任何声明中，从单个局部变量声明到整个类。总是在尽可能小的范围上使用 SuppressWarnings 注解。 每次使用SuppressWarnings(“unchecked”) 注解时，要添加一条注释，说明这样做是安全的。 list 优于数组：数组与泛型有两个重要区别。首先，数组是协变的。这个听起来很吓人的单词的意思很简单，如果 Sub 是 Super 的一个子类型，那么数组类型 Sub[] 就是数组类型 Super[] 的一个子类型。数组和泛型之间的第二个主要区别：数组是具体化的。这意味着数组在运行时知道并强制执行他们的元素类型。相比之下，泛型是通过擦除来实现的。 由于这些基本差异，数组和泛型不能很好地混合。例如，创建泛型、参数化类型或类型参数的数组是非法的。因此，这些数组创建表达式都不是合法的：new List[]、new List[]、new E[]。所有这些都会在编译时导致泛型数组创建错误。为了更具体，请考虑以下代码片段： 123456// Why generic array creation is illegal - won&#x27;t compile!List&lt;String&gt;[] stringLists = new List&lt;String&gt;[1]; // (1)List&lt;Integer&gt; intList = List.of(42); // (2)Object[] objects = stringLists; // (3)objects[0] = intList; // (4)String s = stringLists[0].get(0); // (5) 假设创建泛型数组的第 1 行是合法的。第 2 行创建并初始化一个包含单个元素的 List。第 3 行将 List 数组存储到 Object 类型的数组变量中，这是合法的，因为数组是协变的。第 4 行将 List 存储到 Object 类型的数组的唯一元素中，这是成功的，因为泛型是由擦除实现的：List 实例的运行时类型是 List，List[] 实例的运行时类型是 List[]，因此这个赋值不会生成 ArrayStoreException。现在我们有麻烦了。我们将一个 List 实例存储到一个数组中，该数组声明只保存 List 实例。在第 5 行，我们从这个数组的唯一列表中检索唯一元素。编译器自动将检索到的元素转换为 String 类型，但它是一个 Integer 类型的元素，因此我们在运行时得到一个 ClassCastException。为了防止这种情况发生，第 1 行（创建泛型数组）必须生成编译时错误。 当你在转换为数组类型时遇到泛型数组创建错误或 unchecked 强制转换警告时，通常最好的解决方案是使用集合类型 List，而不是数组类型 E[]。 总之，数组和泛型有非常不同的类型规则。数组是协变的、具体化的；泛型是不变的和可被擦除的。因此，数组提供了运行时类型安全，而不是编译时类型安全，对于泛型反之亦然。一般来说，数组和泛型不能很好地混合。如果你发现将它们混合在一起并得到编译时错误或警告，那么你的第一个反应该是将数组替换为 list。 优先使用泛型：考虑一个泛型栈结构： 123public Stack() &#123; elements = new E[DEFAULT_INITIAL_CAPACITY];&#125; 通常至少会得到一个错误或警告，这个类也不例外。幸运的是，这个类只生成一个错误： 123Stack.java:8: generic array creationelements = new E[DEFAULT_INITIAL_CAPACITY];^ 每当你编写由数组支持的泛型时，就会出现这个问题。有两种合理的方法来解决它。第一个解决方案直接绕过了创建泛型数组的禁令：创建对象数组并将其强制转换为泛型数组类型。现在，编译器将发出一个警告来代替错误。这种用法是合法的，但（一般而言）它不是类型安全的： 1234Stack.java:8: warning: [unchecked] unchecked castfound: Object[], required: E[]elements = (E[]) new Object[DEFAULT_INITIAL_CAPACITY];^ 消除 Stack 中泛型数组创建错误的第二种方法是将字段元素的类型从 E[] 更改为 Object[]。如果你这样做，你会得到一个不同的错误： 1234Stack.java:19: incompatible typesfound: Object, required: EE result = elements[--size];^ 通过将从数组中检索到的元素转换为 E，可以将此错误转换为警告，但你将得到警告： 1234Stack.java:19: warning: [unchecked] unchecked castfound: Object, required: EE result = (E) elements[--size];^ 消除泛型数组创建的两种技术都有其追随者。第一个更容易读：数组声明为 E[] 类型，这清楚地表明它只包含 E 的实例。它也更简洁：在一个典型的泛型类中，从数组中读取代码中的许多点；第一种技术只需要一次转换（在创建数组的地方），而第二种技术在每次读取数组元素时都需要单独的转换。因此，第一种技术是可取的，在实践中更常用。 泛型比需要在客户端代码中转换的类型更安全、更容易使用。 优先使用泛型方法：允许类型参数被包含该类型参数本身的表达式限制，尽管这种情况比较少见。这就是所谓的递归类型限定。递归类型边界的一个常见用法是与 Comparable 接口相关联，后者定义了类型的自然顺序： 123public interface Comparable&lt;T&gt; &#123; int compareTo(T o);&#125; 许多方法采用实现 Comparable 的元素集合，在其中进行搜索，计算其最小值或最大值，等等。要做到这些，需要集合中的每个元素与集合中的每个其他元素相比较，换句话说，就是列表中的元素相互比较。 12// Using a recursive type bound to express mutual comparabilitypublic static &lt;E extends Comparable&lt;E&gt;&gt; E max(Collection&lt;E&gt; c); 类型限定 &lt;E extends Comparable&lt;E&gt;&gt; 可以被理解为「可以与自身进行比较的任何类型 E」，这或多或少与相互可比性的概念相对应。 使用有界通配符增加 API 的灵活性：假设我们想添加一个方法，该方法接受一系列元素并将它们全部推入堆栈。这是第一次尝试： 12345// pushAll method without wildcard type - deficient!public void pushAll(Iterable&lt;E&gt; src) &#123; for (E e : src) push(e);&#125; 该方法能够正确编译，但并不完全令人满意。下面的代码将会产生错误： 123Stack&lt;Number&gt; numberStack = new Stack&lt;&gt;();Iterable&lt;Integer&gt; integers = ... ;numberStack.pushAll(integers); 错误信息： 1234StackTest.java:7: error: incompatible types: Iterable&lt;Integer&gt;cannot be converted to Iterable&lt;Number&gt; numberStack.pushAll(integers); ^ Java 提供了一种特殊的参数化类型，有界通配符类型来处理这种情况。pushAll 的输入参数的类型不应该是「E 的 Iterable 接口」，而应该是「E 的某个子类型的 Iterable 接口」，并且有一个通配符类型，它的确切含义是：Iterable&lt;? extends E&gt;： 12345// Wildcard type for a parameter that serves as an E producerpublic void pushAll(Iterable&lt;? extends E&gt; src) &#123; for (E e : src) push(e);&#125; popAll方法的代码如下： 12345// popAll method without wildcard type - deficient!public void popAll(Collection&lt;E&gt; dst) &#123; while (!isEmpty()) dst.add(pop());&#125; 同样，如果目标集合的元素类型与堆栈的元素类型完全匹配，那么这种方法可以很好地编译。但这也不是完全令人满意： 123Stack&lt;Number&gt; numberStack = new Stack&lt;Number&gt;();Collection&lt;Object&gt; objects = ... ;numberStack.popAll(objects); 同样，有一个通配符类型，它的确切含义是：Collection&lt;? super E&gt;。 12345// Wildcard type for parameter that serves as an E consumerpublic void popAll(Collection&lt;? super E&gt; dst) &#123; while (!isEmpty()) dst.add(pop());&#125; 总而言之，PECS 表示生产者应使用 extends，消费者应使用 super。 明智地合用泛型和可变参数：泛型和可变参数不能很好的结合起来。考虑如下代码： 12345678// Mixing generics and varargs can violate type safety!// 泛型和可变参数混合使用可能违反类型安全原则！static void dangerous(List&lt;String&gt;... stringLists) &#123; List&lt;Integer&gt; intList = List.of(42); Object[] objects = stringLists; objects[0] = intList; // Heap pollution String s = stringLists[0].get(0); // ClassCastException&#125; 此方法没有显式的强制类型转换，但在使用一个或多个参数调用时抛出 ClassCastException。它的最后一行有一个由编译器生成的隐式强制转换。此转换失败，表明类型安全性受到了影响，并且在泛型可变参数数组中存储值是不安全的。为什么使用泛型可变参数声明方法是合法的，而显式创建泛型数组是非法的？答案是，带有泛型或参数化类型的可变参数的方法在实际开发中非常有用，因此语言设计人员选择忍受这种不一致性。事实上，Java 库导出了几个这样的方法，包括 Arrays.asList(T… a)、Collections.addAll(Collection&lt;? super T&gt; c, T… elements) 以及 EnumSet.of(E first, E… rest)。 在 Java 7 中添加了 SafeVarargs 注释，以允许使用泛型可变参数的方法的作者自动抑制客户端警告。本质上，SafeVarargs 注释构成了方法作者的一个承诺，即该方法是类型安全的。 可变参数方法和泛型不能很好地交互，因为可变参数工具是构建在数组之上的漏洞抽象，并且数组具有与泛型不同的类型规则。虽然泛型可变参数不是类型安全的，但它们是合法的。如果选择使用泛型（或参数化）可变参数编写方法，首先要确保该方法是类型安全的，然后使用 @SafeVarargs 对其进行注释。 考虑类型安全的异构容器： 123456789101112// Typesafe heterogeneous container pattern - implementationpublic class Favorites &#123; private Map&lt;Class&lt;?&gt;, Object&gt; favorites = new HashMap&lt;&gt;(); public &lt;T&gt; void putFavorite(Class&lt;T&gt; type, T instance) &#123; favorites.put(Objects.requireNonNull(type), instance); &#125; public &lt;T&gt; T getFavorite(Class&lt;T&gt; type) &#123; return type.cast(favorites.get(type)); &#125;&#125; 这里发生了一些微妙的事情。每个 Favorites 实例都由一个名为 favorites 的私有 Map&lt;Class&lt;?&gt;, Object&gt; 支持。你可能认为由于通配符类型是无界的，所以无法将任何内容放入此映射中，但事实恰恰相反。需要注意的是，通配符类型是嵌套的：通配符类型不是 Map 的类型，而是键的类型。这意味着每个键都可以有不同的参数化类型：一个可以是 Class&lt;String&gt;，下一个是 Class&lt;Integer&gt;，等等。这就是异构的原理。 接下来要注意的是 favorites 的值类型仅仅是 Object。换句话说，Map 不保证键和值之间的类型关系，即每个值都是其键所表示的类型。实际上，Java 的类型系统还没有强大到足以表达这一点。但是我们知道这是事实，当需要检索一个 favorite 时，我们会利用它。 putFavorite 的实现很简单：它只是将从给定 Class 对象到给定 Favorites 实例的放入 favorites 中。如前所述，这将丢弃键和值之间的「类型关联」；将无法确定值是键的实例。但这没关系，因为 getFavorites 方法可以重新建立这个关联。 getFavorite 的实现比 putFavorite 的实现更复杂。首先，它从 favorites 中获取与给定 Class 对象对应的值。这是正确的对象引用返回，但它有错误的编译时类型：它是 Object（favorites 的值类型），我们需要返回一个 T。因此，getFavorite 的实现通过使用 Class 的 cast 方法，将对象引用类型动态转化为所代表的 Class 对象。 总之，以集合的 API 为例的泛型在正常使用时将每个容器的类型参数限制为固定数量。你可以通过将类型参数放置在键上而不是容器上来绕过这个限制。你可以使用 Class 对象作为此类类型安全异构容器的键。以这种方式使用的 Class 对象称为类型标记。还可以使用自定义键类型。例如，可以使用 DatabaseRow 类型表示数据库行（容器），并使用泛型类型 Column&lt;T&gt; 作为它的键。 第六章 枚举和注解 用枚举类型代替 int 常量：在枚举类型被添加到 JAVA 之前，表示枚举类型的一种常见模式是声明一组 int 的常量，这种技术称为 int 枚举模式，它有许多缺点。它没有提供任何类型安全性，并且几乎不具备表现力。如果你传递一个苹果给方法，希望得到一个橘子，使用 &#x3D;&#x3D; 操作符比较苹果和橘子时编译器并不会提示错误，或更糟的情况： 12// Tasty citrus flavored applesauce!int i = (APPLE_FUJI - ORANGE_TEMPLE) / APPLE_PIPPIN; 使用 String 常量代替 int 常量。这种称为 String 枚举模式的变体甚至更不可取。虽然它确实为常量提供了可打印的字符串，但是它可能会导致不知情的用户将字符串常量硬编码到客户端代码中，而不是使用字段名。使用枚举可以解决上述问题。 从表面上看，Java 枚举类型可能与其他语言（如 C、c++ 和 c#）的枚举类型类似，但不能只看表象。Java 的枚举类型是成熟的类，比其他语言中的枚举类型功能强大得多，在其他语言中的枚举本质上是 int 值。除了纠正 int 枚举的不足之外，枚举类型还允许添加任意方法和字段并实现任意接口。 编写一个富枚举类型很容易，如上述的 Planet。要将数据与枚举常量关联，可声明实例字段并编写一个构造函数，该构造函数接受数据并将其存储在字段中。 枚举本质上是不可变的，因此所有字段都应该是 final。字段可以是公共的，但是最好将它们设置为私有并提供公共访问器。 有一种更好的方法可以将不同的行为与每个枚举常量关联起来，这些方法称为特定常量方法实现： 12345678// Enum type with constant-specific method implementationspublic enum Operation &#123; PLUS &#123;public double apply(double x, double y)&#123;return x + y;&#125;&#125;, MINUS &#123;public double apply(double x, double y)&#123;return x - y;&#125;&#125;, TIMES &#123;public double apply(double x, double y)&#123;return x * y;&#125;&#125;, DIVIDE&#123;public double apply(double x, double y)&#123;return x / y;&#125;&#125;; public abstract double apply(double x, double y);&#125; 使用实例字段替代序数：所有枚举都有一个 ordinal 方法，该方法返回枚举类型中每个枚举常数的数值位置。 123456// Abuse of ordinal to derive an associated value - DON&#x27;T DO THISpublic enum Ensemble &#123; SOLO, DUET, TRIO, QUARTET, QUINTET,SEXTET, SEPTET, OCTET, NONET, DECTET; public int numberOfMusicians() &#123; return ordinal() + 1; &#125;&#125; 虽然这个枚举可以工作，但维护却是噩梦。如果常量被重新排序，numberOfMusicians 方法将被破坏。有一个简单的解决方案：不要从枚举的序数派生与枚举关联的值；而是将其存储在实例字段中： 123456789public enum Ensemble &#123; SOLO(1), DUET(2), TRIO(3), QUARTET(4), QUINTET(5),SEXTET(6), SEPTET(7), OCTET(8), DOUBLE_QUARTET(8),NONET(9), DECTET(10),TRIPLE_QUARTET(12); private final int numberOfMusicians; Ensemble(int size) &#123; this.numberOfMusicians = size; &#125; public int numberOfMusicians() &#123; return numberOfMusicians; &#125;&#125; 枚举规范对 ordinal 方法的评价是这样的：「大多数程序员都不会去使用这个方法。它是为基于枚举的通用数据结构（如 EnumSet 和 EnumMap）而设计的」。除非你使用这个数据结构编写代码，否则最好完全避免使用这个方法。 用 EnumSet 替代位字段：位字段模式如下： 123456789// Bit field enumeration constants - OBSOLETE!public class Text &#123; public static final int STYLE_BOLD = 1 &lt;&lt; 0; // 1 public static final int STYLE_ITALIC = 1 &lt;&lt; 1; // 2 public static final int STYLE_UNDERLINE = 1 &lt;&lt; 2; // 4 public static final int STYLE_STRIKETHROUGH = 1 &lt;&lt; 3; // 8 // Parameter is bitwise OR of zero or more STYLE_ constants public void applyStyles(int styles) &#123; ... &#125;&#125; 允许你使用位运算的 OR 操作将几个常量组合成一个 Set： 1text.applyStyles(STYLE_BOLD | STYLE_ITALIC); 位字段表示方式允许使用位运算高效地执行 Set 操作，如并集和交集。但是位字段具有 int 枚举常量所有缺点，甚至更多。当位字段被打印为数字时，它比简单的 int 枚举常量更难理解。没有一种简单的方法可以遍历由位字段表示的所有元素。 当之前的示例修改为使用枚举和 EnumSet 而不是位字段时。它更短，更清晰，更安全： 123456// EnumSet - a modern replacement for bit fieldspublic class Text &#123; public enum Style &#123; BOLD, ITALIC, UNDERLINE, STRIKETHROUGH &#125; // Any Set could be passed in, but EnumSet is clearly best public void applyStyles(Set&lt;Style&gt; styles) &#123; ... &#125;&#125; 下面是将 EnumSet 实例传递给 applyStyles 方法的客户端代码： 1text.applyStyles(EnumSet.of(Style.BOLD, Style.ITALIC)); EnumSet 类结合了位字段的简洁性和性能。EnumSet 的一个真正的缺点是，从 Java 9 开始，它不能创建不可变的 EnumSet。但是，可以用 Collections.unmodifiableSet 包装 EnumSet，实现不可变性，但简洁性和性能将受到影响。 使用 EnumMap 替换序数索引：如果想要使用 Enum 里面的美居元素来对一组对象进行分组的话，请不要使用序数索引： 12// Using ordinal() to index into an array - DON&#x27;T DO THIS!Set&lt;Plant&gt;[] plantsByLifeCycle =(Set&lt;Plant&gt;[]) new Set[Plant.LifeCycle.values().length]; 这样带来的问题是不便于维护。Java 提供了一种简单的方式实现该目的，EnumMap： 12// Using an EnumMap to associate data with an enumMap&lt;Plant.LifeCycle, Set&lt;Plant&gt;&gt; plantsByLifeCycle =new EnumMap&lt;&gt;(Plant.LifeCycle.class); 这个程序比原来的版本更短，更清晰，更安全，速度也差不多。没有不安全的转换；不需要手动标记输出，因为 Map 的键是能转换为可打印字符串的枚举；在计算数组索引时不可能出错。 使用接口模拟可扩展枚举：利用枚举类型可以实现任意接口这一事实，为 opcode 类型定义一个接口，并为接口的标准实现定义一个枚举： 123456789101112131415161718192021222324252627282930// Emulated extensible enum using an interfacepublic interface Operation &#123; double apply(double x, double y);&#125;public enum BasicOperation implements Operation &#123; PLUS(&quot;+&quot;) &#123; public double apply(double x, double y) &#123; return x + y; &#125; &#125;, MINUS(&quot;-&quot;) &#123; public double apply(double x, double y) &#123; return x - y; &#125; &#125;, TIMES(&quot;*&quot;) &#123; public double apply(double x, double y) &#123; return x * y; &#125; &#125;, DIVIDE(&quot;/&quot;) &#123; public double apply(double x, double y) &#123; return x / y; &#125; &#125;; private final String symbol; BasicOperation(String symbol) &#123; this.symbol = symbol; &#125; @Override public String toString() &#123; return symbol; &#125;&#125; 注解优于命名模式：使用命名模式来标明某些程序元素需要工具或框架特殊处理的方式是很常见的，例如，在版本 4 之前，JUnit 测试框架要求其用户通过以字符 test 开头的名称来指定测试方法。命名模式有几个问题： 首先，排版错误会导致没有提示的失败 无法确保只在相应的程序元素上使用它们 它们没有提供将参数值与程序元素关联的好方法 假设我们声明了一个 Test 注解，那么我们需要相应的工具来解析这些注解： 12345678910111213141516171819202122232425// Program to process marker annotationsimport java.lang.reflect.*;public class RunTests &#123; public static void main(String[] args) throws Exception &#123; int tests = 0; int passed = 0; Class&lt;?&gt; testClass = Class.forName(args[0]); for (Method m : testClass.getDeclaredMethods()) &#123; if (m.isAnnotationPresent(Test.class)) &#123; tests++; try &#123; m.invoke(null); passed++; &#125; catch (InvocationTargetException wrappedExc) &#123; Throwable exc = wrappedExc.getCause(); System.out.println(m + &quot; failed: &quot; + exc); &#125; catch (Exception exc) &#123; System.out.println(&quot;Invalid @Test: &quot; + m); &#125; &#125; &#125; System.out.printf(&quot;Passed: %d, Failed: %d%n&quot;,passed, tests - passed); &#125;&#125; 使用注解很简洁，同时也防止了使用命名模式所带来的一系列的问题。 坚持使用 @Override 注解：在每个方法声明上都使用 @Override 注解来覆盖超类型声明，那么编译器可以帮助你减少受到有害错误的影响，如错误将重写实现为重载。在具体类中，可以不对覆盖抽象方法声明的方法使用该注解。 使用标记接口定义类型：标记接口是一种不包含任何方法声明的接口，它只是指定一个类，该类实现了具有某些属性的接口。例如 Serializable 接口。与标记注解相比，标记接口有两个优点。首先，标记接口定义的类型由标记类的实例实现；标记注解不会。标记接口相对于标记注解的另一个优点是可以更精确地定位它们。相对于标记接口，标记注解的主要优势是它们可以是其他注解功能的一部分。 第七章 Lambda表达式和流 lambda 表达式优于匿名类：在历史上，带有单个抽象方法的接口被用作函数类型。它们的实例（称为函数对象）表示函数或操作。自从 JDK 1.1 在 1997 年发布以来，创建函数对象的主要方法就是匿名类： 123456// Anonymous class instance as a function object - obsolete!Collections.sort(words, new Comparator&lt;String&gt;() &#123; public int compare(String s1, String s2) &#123; return Integer.compare(s1.length(), s2.length()); &#125;&#125;); 在 Java 8 中官方化了一个概念，即具有单个抽象方法的接口是特殊的，应该得到特殊处理。这些接口现在被称为函数式接口，允许使用 lambda 表达式创建这些接口的实例： 12// Lambda expression as function object (replaces anonymous class)Collections.sort(words,(s1, s2) -&gt; Integer.compare(s1.length(), s2.length())); 一般来说，省略lambda中参数的类型，除非编译器不能自动推断出来。另外，在lambda表达式中this关键字指向的是外部的类的实例，但是匿名类指的是匿名类自己。 方法引用优于 lambda 表达式：lambda 表达式与匿名类相比，主要优势是更简洁。Java 提供了一种方法来生成比 lambda 表达式更简洁的函数对象：方法引用。 1map.merge(key, 1, Integer::sum); 而是用lambda代码如下： 1map.merge(key, 1, (count, incr) -&gt; count + incr); 方法引用通常为 lambda 表达式提供了一种更简洁的选择。如果方法引用更短、更清晰，则使用它们；如果没有，仍然使用 lambda 表达式。 优先使用标准函数式接口：现在 Java 已经有了 lambda 表达式，编写 API 的最佳实践已经发生了很大的变化。java.util.function 包提供了大量的标准函数接口供你使用。如果一个标准的函数式接口可以完成这项工作，那么你通常应该优先使用它，而不是使用专门构建的函数式接口。六个基本的函数式接口总结如下： Interface Function Signature Example UnaryOperator T apply(T t) String::toLowerCase BinaryOperator T apply(T t1, T t2) BigInteger::add Predicate boolean test(T t) Collection::isEmpty Function R apply(T t) Arrays::asList Supplier T get() Instant::now Consumer void accept(T t) System.out::println 还有 6 个基本接口的 3 个变体，用于操作基本类型 int、long 和 double。Function 接口还有 9 个额外的变体，在结果类型为基本数据类型时使用。 明智地使用流：在 Java 8 中添加了流 API，以简化序列或并行执行批量操作的任务。这个 API 提供了两个关键的抽象：流（表示有限或无限的数据元素序列）和流管道（表示对这些元素的多阶段计算）。流管道的计算是惰性的：直到调用 Terminal 操作时才开始计算，并且对完成 Terminal 操作不需要的数据元素永远不会计算。这种惰性的求值机制使得处理无限流成为可能。流 API 非常通用，实际上任何计算都可以使用流来执行，但这并不意味着你就应该这样做。如果使用得当，流可以使程序更短、更清晰；如果使用不当，它们会使程序难以读取和维护。由于 Java 不支持基本字符流： 1&quot;Hello world!&quot;.chars().forEach(System.out::print); 你可能希望它打印 Hello world!，但如果运行它，你会发现它打印 721011081081113211911111410810033。这是因为 “Hello world!”.chars() 返回的流元素不是 char 值，而是 int 值，因此调用了 print 的 int 重载。强制转换可以解决这个问题： 1&quot;Hello world!&quot;.chars().forEach(x -&gt; System.out.print((char) x)); 有些事情你可以对代码块做，而你不能对函数对象（通常是 lambda 表达式或方法引用）做 从代码块中，可以读取或修改作用域中的任何局部变量；在 lambda 表达式中，只能读取 final 或有效的 final 变量，不能修改任何局部变量。 从代码块中，可以从封闭方法返回、中断或继续封闭循环，或抛出声明要抛出的任何已检查异常；在 lambda 表达式中，你不能做这些事情。 相反，流使做一些事情变得非常容易： 元素序列的一致变换 过滤元素序列 使用单个操作组合元素序列 将元素序列累积到一个集合中，可能是按某个公共属性对它们进行分组 在元素序列中搜索满足某些条件的元素 在流中使用无副作用的函数：你可能会看到如下使用流的代码片段，它用于构建文本文件中单词的频率表： 1234567// Uses the streams API but not the paradigm--Don&#x27;t do this!Map&lt;String, Long&gt; freq = new HashMap&lt;&gt;();try (Stream&lt;String&gt; words = new Scanner(file).tokens()) &#123; words.forEach(word -&gt; &#123; freq.merge(word.toLowerCase(), 1L, Long::sum); &#125;);&#125; 简单地说，它根本不是流代码，而是伪装成流代码的迭代代码。它没有从流 API 中获得任何好处，而且它（稍微）比相应的迭代代码更长、更难于阅读和更难以维护。这个问题源于这样一个事实：这段代码在一个 Terminal 操作中（forEach）执行它的所有工作，使用一个会改变外部状态的 lambda 表达式（频率表）。改进后的代码： 12345// Proper use of streams to initialize a frequency tableMap&lt;String, Long&gt; freq;try (Stream&lt;String&gt; words = new Scanner(file).tokens()) &#123; freq = words.collect(groupingBy(String::toLowerCase, counting()));&#125; 这个代码片段与前面的代码片段做了相同的事情，但是正确地使用了流 API。它更短更清晰。 将流的元素收集到一个真正的 Collection 中的 collector 非常简单。这样的 collector 有三种：toList()、toSet() 和 toCollection(collectionFactory)。它们分别返回 List、Set 和程序员指定的集合类型。 12345// Pipeline to get a top-ten list of words from a frequency tableList&lt;String&gt; topTen = freq.keySet().stream() .sorted(comparing(freq::get).reversed()) .limit(10) .collect(toList()); 另外还有 groupingBy 和 join。 优先选择 Collection 而不是流作为返回类型：在编写返回元素序列的方法时，有些用户可能希望将它们作为流处理，而有些用户可能希望对它们进行迭代。如果可以返回集合，那么就这样做。如果你已经在一个集合中拥有了元素，或者序列中的元素数量足够小，可以创建一个新的元素，那么返回一个标准集合，例如 ArrayList 。否则，请考虑像对 power 集那样实现自定义集合。如果返回集合不可行，则返回流或 iterable，以看起来更自然的方式返回。 谨慎使用并行流：在主流语言中，Java 一直走在提供简化并发编程任务工具的前列。当 Java 在 1996 年发布时，它内置了对线程的支持，支持同步和 wait&#x2F;notify。Java 5 引入了 java.util.concurrent。具有并发集合和执行器框架的并发库。Java 7 引入了 fork-join 包，这是一个用于并行分解的高性能框架。Java 8 引入了流，它可以通过对 parallel 方法的一次调用来并行化。 并行性带来的性能提升在 ArrayList、HashMap、HashSet 和 ConcurrentHashMap 实例上的流效果最好；int 数组和 long 数组也在其中。 这些数据结构的共同之处在于，它们都可以被精确且廉价地分割成任意大小的子程序，这使得在并行线程之间划分工作变得很容易。 并行化流不仅会导致糟糕的性能，包括活动失败；它会导致不正确的结果和不可预知的行为（安全故障）。 如果管道使用映射器、过滤器和其他程序员提供的函数对象，而这些对象没有遵守其规范，则并行化管道可能导致安全故障。流规范对这些功能对象提出了严格的要求。例如，传递给流的 reduce 操作的累加器和组合器函数必须是关联的、不干扰的和无状态的。 第八章 方法 检查参数的有效性：大多数方法和构造函数都对传递给它们的参数值有一些限制。例如，索引值必须是非负的，对象引用必须是非空的。如果一个无效的参数值被传递给一个方法，如果该方法在执行之前会检查它的参数，那么这个过程将迅速失败，并引发适当的异常。如果方法未能检查其参数，可能会发生以下几件事。该方法可能会在处理过程中出现令人困惑的异常而失败。更糟的是，该方法可以正常返回，但会静默计算错误的结果。在 Java 7 中添加的 Objects.requireNonNull 方法非常灵活和方便，因此不再需要手动执行空检查。在 Java 9 中，范围检查功能被添加到 java.util.Objects 中。这个功能由三个方法组成：checkFromIndexSize、checkFromToIndex 和 checkIndex。非公共方法可以使用断言检查它们的参数。 在执行方法的计算任务之前，应该显式地检查方法的参数，这条规则也有例外。一个重要的例外是有效性检查成本较高或不切实际，或者检查是在计算过程中隐式执行了。例如，考虑一个为对象 List 排序的方法，比如 Collections.sort(List)。List 中的所有对象必须相互比较。在对 List 排序的过程中，List 中的每个对象都会与列表中的其他对象进行比较。如果对象不能相互比较，将抛出 ClassCastException，这正是 sort 方法应该做的。因此，没有必要预先检查列表中的元素是否具有可比性。 在需要时制作防御性副本：即使使用一种安全的语言，如果你不付出一些努力，也无法与其他类隔离。你必须进行防御性的设计，并假定你的类的客户端会尽最大努力破坏它的不变量。 随着人们越来越多地尝试破坏系统的安全性，这个观点越来越正确。考虑这样的一个类： 123456789101112131415161718192021// Broken &quot;immutable&quot; time period classpublic final class Period &#123; private final Date start; private final Date end; public Period(Date start, Date end) &#123; if (start.compareTo(end) &gt; 0) throw new IllegalArgumentException(start + &quot; after &quot; + end); this.start = start; this.end = end; &#125; public Date start() &#123; return start; &#125; public Date end() &#123; return end; &#125; ... // Remainder omitted&#125; 乍一看，这个类似乎是不可变的，并且要求一个时间段的开始时间不能在结束时间之后。然而，利用 Date 是可变的这一事实很容易绕过这个约束： 12345// Attack the internals of a Period instanceDate start = new Date();Date end = new Date();Period p = new Period(start, end);end.setYear(78); // Modifies internals of p! 为了防止这样的攻击，可以选择制作防御性副本： 12345678910111213141516// Repaired constructor - makes defensive copies of parameterspublic Period(Date start, Date end) &#123; this.start = new Date(start.getTime()); this.end = new Date(end.getTime()); if (this.start.compareTo(this.end) &gt; 0) throw new IllegalArgumentException(this.start + &quot; after &quot; + this.end);&#125;// Repaired accessors - make defensive copies of internal fieldspublic Date start() &#123; return new Date(start.getTime());&#125;public Date end() &#123; return new Date(end.getTime());&#125; 防御性复制可能会带来性能损失，最好的方法是使用不可变类，比如Instant（或 Local-DateTime 或 ZonedDateTime）来代替 Date，Date 已过时，不应在新代码中使用。 仔细设计方法签名： 仔细选择方法名称。 不要提供过于便利的方法 避免长参数列表 对于参数类型，优先选择接口而不是类 双元素枚举类型优于 boolean 参数 明智地使用重载：考虑下面的代码： 123456789101112131415161718192021// Broken! - What does this program print?public class CollectionClassifier &#123; public static String classify(Set&lt;?&gt; s) &#123; return &quot;Set&quot;; &#125; public static String classify(List&lt;?&gt; lst) &#123; return &quot;List&quot;; &#125; public static String classify(Collection&lt;?&gt; c) &#123; return &quot;Unknown Collection&quot;; &#125; public static void main(String[] args) &#123; Collection&lt;?&gt;[] collections = &#123; new HashSet&lt;String&gt;(),new ArrayList&lt;BigInteger&gt;(),new HashMap&lt;String, String&gt;().values() &#125;; for (Collection&lt;?&gt; c : collections) System.out.println(classify(c)); &#125; 你可能期望这个程序打印 Set，然后是 List 和 Unknown Collection，但是它没有这样做。它打印 Unknown Collection 三次。为什么会这样？因为 classify 方法被重载，并且 在编译时就决定了要调用哪个重载。 这个程序的行为违反常规，因为重载方法的选择是静态的，而覆盖方法的选择是动态的。 在运行时根据调用方法的对象的运行时类型选择覆盖方法的正确版本。 123456789101112131415161718192021class Wine &#123; String name() &#123; return &quot;wine&quot;; &#125;&#125;class SparklingWine extends Wine &#123; @Override String name() &#123; return &quot;sparkling wine&quot;; &#125;&#125;class Champagne extends SparklingWine &#123; @Override String name() &#123; return &quot;champagne&quot;; &#125;&#125;public class Overriding &#123; public static void main(String[] args) &#123; List&lt;Wine&gt; wineList = List.of(new Wine(), new SparklingWine(), new Champagne()); for (Wine wine : wineList) System.out.println(wine.name()); &#125;&#125; 正如你所期望的，这个程序打印出 wine、sparkling 和 champagne，即使实例的编译时类型是循环每次迭代中的 wine。 应该避免混淆重载的用法。安全、保守的策略是永远不导出具有相同数量参数的两个重载。这些限制并不十分繁琐，因为你总是可以为方法提供不同的名称，而不是重载它们。 明智地使用可变参数：可变参数方法接受指定类型的零个或多个参数。可变参数方法首先创建一个数组，其大小是在调用点上传递的参数数量，然后将参数值放入数组，最后将数组传递给方法。在性能关键的情况下使用可变参数时要小心。每次调用可变参数方法都会导致数组分配和初始化。如果你已经从经验上确定你负担不起这个成本，但是你仍需要可变参数的灵活性，那么有一种模式可以让你鱼与熊掌兼得。假设你已经确定对方法 95% 的调用只需要三个或更少的参数。可以声明该方法的 5 个重载，每个重载 0 到 3 个普通参数，当参数数量超过 3 个时引入可变参数： 12345public void foo() &#123; &#125;public void foo(int a1) &#123; &#125;public void foo(int a1, int a2) &#123; &#125;public void foo(int a1, int a2, int a3) &#123; &#125;public void foo(int a1, int a2, int a3, int... rest) &#123; &#125; 返回空集合或数组，而不是 null：在几乎每次使用返回 null 来代替空集合或数组的方法时，都需要使用这种权宜之计。它很容易出错，因为编写客户端的程序员可能忘记编写特殊情况的代码来处理 null 返回。这样的错误可能会被忽略多年，因为这样的方法通常返回一个或多个对象。此外，在空容器中返回 null 会使返回容器的方法的实现复杂化。数组的情况与集合的情况相同。永远不要返回 null，而应该返回零长度的数组。永远不要用 null 来代替空数组或集合。它使你的 API 更难以使用，更容易出错，并且没有性能优势。 明智地返回 Optional：在 Java 8 之前，在编写在某些情况下无法返回值的方法时，可以采用两种方法。要么抛出异常，要么返回 null（假设返回类型是对象引用类型）。这两种方法都不完美。应该为异常条件保留异常，并且抛出异常代价高昂，因为在创建异常时捕获整个堆栈跟踪。返回 null 没有这些缺点，但是它有自己的缺点。如果方法返回 null，客户端必须包含特殊情况代码来处理 null 返回的可能性，除非程序员能够证明 null 返回是不可能的。如果客户端忽略检查 null 返回并将 null 返回值存储在某个数据结构中，那么 NullPointerException 可能会在将来的某个时间，在代码中的某个与该问题无关的位置产生。 在 Java 8 中，还有第三种方法来编写可能无法返回值的方法。Optional&lt;T&gt; 类表示一个不可变的容器，它可以包含一个非空的 T 引用，也可以什么都不包含。不包含任何内容的 Optional 被称为空。一个值被认为存在于一个非空的 Optional 中。Optional 的本质上是一个不可变的集合，它最多可以容纳一个元素。具备 Optional 返回值的方法比抛出异常的方法更灵活、更容易使用，并且比返回 null 的方法更不容易出错。 如果你发现自己编写的方法不能总是返回确定值，并且你认为该方法的用户在每次调用时应该考虑这种可能性，那么你可能应该让方法返回一个 Optional。但是，你应该意识到，返回 Optional 会带来实际的性能后果；对于性能关键的方法，最好返回 null 或抛出异常。最后，除了作为返回值之外，你几乎不应该以任何其他方式使用 Optional。 为所有公开的 API 元素编写文档注释：如果 API 要可用，就必须对其进行文档化。传统上，API 文档是手工生成的，保持与代码的同步是一件苦差事。Java 编程环境使用 Javadoc 实用程序简化了这一任务。Javadoc 使用特殊格式的文档注释（通常称为文档注释）从源代码自动生成 API 文档。 要正确地编写 API 文档，必须在每个公开的类、接口、构造函数、方法和字段声明之前加上文档注释。 如果一个类是可序列化的，还应该记录它的序列化形式。方法的文档注释应该简洁地描述方法与其客户端之间的约定。 第九章 通用程序设计 将局部变量的作用域最小化：通过最小化局部变量的范围，可以提高代码的可读性和可维护性，并降低出错的可能性。在做循环操作的时候，尽量使用for而不是while。下面是使用局部变量的方法技巧： 将局部变量的作用域最小化，最具说服力的方式就是在第一次使用它的地方声明。 每个局部变量声明都应该包含一个初始化表达式。 最小化局部变量范围的最后一种技术是保持方法小而集中。 for-each 循环优于传统的 for 循环：通常我们使用迭代器来遍历集合，使用数组索引遍历数组，但是他们的缺点就是需要自己维护迭代器和索引变量，最好的方法是使用for-each循环。但是，下列情况不适合for-each循环： 破坏性过滤，如果需要遍历一个集合并删除选定元素，则需要使用显式的迭代器，以便调用其 remove 方法。 转换，如果需要遍历一个 List 或数组并替换其中部分或全部元素的值，那么需要 List 迭代器或数组索引来替换元素的值。 并行迭代，如果需要并行遍历多个集合，那么需要显式地控制迭代器或索引变量，以便所有迭代器或索引变量都可以同步执行。 了解并使用库：假设你想要生成 0 到某个上界之间的随机整数： 12345// Common but deeply flawed!static Random rnd = new Random();static int random(int n) &#123; return Math.abs(rnd.nextInt()) % n;&#125; 有三个缺点：首先，如果 n 是小的平方数，随机数序列会在相当短的时间内重复。第二个缺陷是，如果 n 不是 2 的幂，那么平均而言，一些数字将比其他数字更频繁地返回。第三个缺陷是，在极少数情况下会返回超出指定范围的数字，这是灾难性的结果。 从 Java 7 开始，就不应该再使用 Random。在大多数情况下，选择的随机数生成器现在是 ThreadLocalRandom。使用这些库能生成更高的随机数，同时，你不必浪费时间为那些与你的工作无关的问题编写专门的解决方案。 若需要精确答案就应避免使用 float 和 double 类型：float 和 double 类型特别不适合进行货币计算，因为不可能将 0.1（或 10 的任意负次幂）精确地表示为 float 或 double。为了得到准确值，应该使用BigDecimal或者是long，int类型进行计算操作。 基本数据类型优于包装类：将 &#x3D;&#x3D; 操作符应用于包装类型几乎都是错误的，而在使用基本类型的时候则是正确的。在操作中混合使用基本类型和包装类型时，包装类型就会自动拆箱，如果包装类是null，可能就会导致NullPointerException。 什么时候该使用包装类型？第一个是作为集合中的元素、键和值。在参数化类型和方法中，必须使用包装类型作为类型参数，因为 Java 不允许使用基本类型。最后，在进行反射方法调用时，必须使用包装类型。 总之，只要有选择，就应该优先使用基本类型，而不是包装类型。基本类型更简单、更快。当你的程序使用 &#x3D;&#x3D; 操作符比较两个包装类型时，它会执行标识比较，这几乎肯定不是你想要的。 其他类型更合适时应避免使用字符串：字符串被设计用来表示文本，它们在这方面做得很好。下面是一些字符串不推荐使用的方式： 字符串是枚举类型的糟糕替代品 字符串是聚合类型的糟糕替代品 总之，当存在或可以编写更好的数据类型时，应避免将字符串用来表示对象。如果使用不当，字符串比其他类型更麻烦、灵活性更差、速度更慢、更容易出错。字符串经常被误用的类型包括基本类型、枚举和聚合类型。 当心字符串连接引起的性能问题：使用字符串串联运算符重复串联 n 个字符串需要 n 的平方级时间。这是字符串不可变这一事实导致的结果。当连接两个字符串时，将复制这两个字符串的内容。要获得能接受的性能，请使用 StringBuilder 代替 String。 通过接口引用对象：应该优先使用接口而不是类来引用对象。如果存在合适的接口类型，那么应该使用接口类型声明参数、返回值、变量和字段。惟一真正需要引用对象的类的时候是使用构造函数创建它的时候。如果你养成了使用接口作为类型的习惯，那么你的程序将更加灵活。如果没有合适的接口存在，那么用类引用对象是完全合适的。 接口优于反射：核心反射机制 java.lang.reflect 提供对任意类的编程访问。给定一个 Class 对象，你可以获得 Constructor、Method 和 Field 实例，分别代表了该 Class 实例所表示的类的构造器、方法和字段。使用反射是有代价的： 你失去了编译时类型检查的所有好处，包括异常检查 执行反射访问所需的代码既笨拙又冗长 性能降低，反射方法调用比普通方法调用慢得多 反射是一种功能强大的工具，对于某些复杂的系统编程任务是必需的，但是它有很多缺点。如果编写的程序必须在编译时处理未知的类，则应该尽可能只使用反射实例化对象，并使用在编译时已知的接口或超类访问对象。 明智地使用本地方法：Java 本地接口（JNI）允许 Java 程序调用本地方法，这些方法是用 C 或 C++ 等本地编程语言编写的。为了提高性能，很少建议使用本地方法。使用本地方法有严重的缺点。由于本地语言不安全，使用本地方法的应用程序不再能免受内存毁坏错误的影响同时很难进行调试。 明智地进行优化：不要过早地进行优化。为了获得良好的性能而改变 API 是一个非常糟糕的想法。同时，在每次尝试优化之前和之后测量性能。 遵守被广泛认可的命名约定： Identifier Type Example Package or module org.junit.jupiter.api, com.google.common.collect Class or Interface Stream, FutureTask, LinkedHashMap, HttpClient Method or Field remove, groupingBy, getCrc Constant Field MIN_VALUE, NEGATIVE_INFINITY Local Variable i, denom, houseNum Type Parameter T, E, K, V, X, R, U, V, T1, T2 第十章 异常 仅在确有异常条件下使用异常：异常只适用于确有异常的情况；它们不应该用于一般的控制流程。下列代码不应该使用： 12345678// Horrible abuse of exceptions. Don&#x27;t ever do this!try &#123; int i = 0; while(true) range[i++].climb(); &#125; catch (ArrayIndexOutOfBoundsException e) &#123;&#125; 我们完全可以使用for-each循环实现。 对可恢复情况使用 checked 异常，对编程错误使用运行时异常：Java 提供了三种可抛出项：checked 异常、运行时异常和错误。决定是使用 checked 异常还是 unchecked 异常的基本规则是：使用 checked 异常的情况是为了合理地期望调用者能够从中恢复。有两种 unchecked 的可抛出项：运行时异常和错误。它们在行为上是一样的：都是可抛出的，通常不需要也不应该被捕获。使用运行时异常来指示编程错误。 绝大多数运行时异常都表示操作违反了先决条件。 避免不必要地使用 checked 异常：在 API 中过度使用 checked 异常会变得不那么令人愉快。如果一个方法抛出 checked 异常，调用它的代码必须在一个或多个 catch 块中处理它们；或者通过声明抛出，让它们向外传播。无论哪种方式，它都给 API 的用户带来了负担。消除 checked 异常的最简单方法是返回所需结果类型的 Optional 对象（Item-55）。该方法只返回一个空的 Optional 对象，而不是抛出一个 checked 异常。 总之，如果谨慎使用，checked 异常可以提高程序的可靠性；当过度使用时，它们会使 API 难以使用。如果调用者不应从失败中恢复，则抛出 unchecked 异常。如果恢复是可能的，并且你希望强制调用者处理异常条件，那么首先考虑返回一个 Optional 对象。只有当在失败的情况下，提供的信息不充分时，你才应该抛出一个 checked 异常。 鼓励复用标准异常：使你的 API 更容易学习和使用，因为它符合程序员已经熟悉的既定约定。最常见的可复用异常： Exception Occasion for Use IllegalArgumentException Non-null parameter value is inappropriate（非空参数值不合适） IllegalStateException Object state is inappropriate for method invocation（对象状态不适用于方法调用） NullPointerException Parameter value is null where prohibited（禁止参数为空时仍传入 null） IndexOutOfBoundsException Index parameter value is out of range（索引参数值超出范围） ConcurrentModificationException Concurrent modification of an object has been detected where it is prohibited（在禁止并发修改对象的地方检测到该动作） UnsupportedOperationException Object does not support method（对象不支持该方法调用） 另外，不要直接复用 Exception、RuntimeException、Throwable 或 Error。应当将这些类视为抽象类。你不能对这些异常进行可靠的测试，因为它们是方法可能抛出的异常的超类。 抛出能用抽象解释的异常：当一个方法抛出一个与它所执行的任务没有明显关联的异常时，这是令人不安的。这种情况经常发生在由方法传播自低层抽象抛出的异常。为了避免这个问题，高层应该捕获低层异常，并确保抛出的异常可以用高层抽象解释。 这个习惯用法称为异常转换： 123456// Exception Translationtry &#123; ... // Use lower-level abstraction to do our bidding&#125; catch (LowerLevelException e) &#123; throw new HigherLevelException(...);&#125; 虽然异常转换优于底层异常的盲目传播，但它不应该被过度使用。在可能的情况下，处理低层异常的最佳方法是确保低层方法避免异常。 为每个方法记录会抛出的所有异常：始终单独声明 checked 异常，并使用 Javadoc 的 @throw 标记精确记录每次抛出异常的条件。使用 Javadoc 的 @throw 标记记录方法会抛出的每个异常，但是不要对 unchecked 异常使用 throws 关键字。如果一个类中的许多方法都因为相同的原因抛出异常，你可以在类的文档注释中记录异常， 而不是为每个方法单独记录异常。 异常详细消息中应包含捕获失败的信息：当程序由于未捕获异常而失败时，系统可以自动打印出异常的堆栈跟踪。堆栈跟踪包含异常的字符串表示，这是调用其 toString 方法的结果。这通常包括异常的类名及其详细信息。要捕获失败，异常的详细消息应该包含导致异常的所有参数和字段的值。因为许多人在诊断和修复软件问题的过程中可能会看到堆栈跟踪，所以不应包含密码、加密密钥等详细信息。 尽力保证故障原子性：在对象抛出异常之后，通常希望对象仍然处于定义良好的可用状态，即使在执行操作时发生了故障。对于 checked 异常尤其如此，调用者希望从异常中恢复。一般来说，失败的方法调用应该使对象处于调用之前的状态。 具有此属性的方法称为具备故障原子性。有几种方式可以达到这种效果： 最简单的方法是设计不可变对象 对计算进行排序，以便可能发生故障的部分都先于修改对象的部分发生 以对象的临时副本执行操作，并在操作完成后用临时副本替换对象的内容 编写恢复代码，拦截在操作过程中发生的故障，并使对象回滚到操作开始之前的状态 不要忽略异常：如果在方法调用的周围加上一条 try 语句，其 catch 块为空，可以很容易忽略异常，空 catch 块违背了异常的目的，它的存在是为了强制你处理异常情况。如果你选择忽略异常，catch 块应该包含一条注释，解释为什么这样做是合适的，并且应该将变量命名为 ignored。 第十一章 并发 对共享可变数据的同步访问：synchronized 关键字确保一次只有一个线程可以执行一个方法或块。没有同步，一个线程所做的的更改可能对其他线程不可见。同步不仅阻止线程察觉到处于不一致状态的对象，而且确保每个进入同步方法或块的线程都能察觉由同一把锁保护的所有已修改的效果。 12345678910111213141516// Broken! - How long would you expect this program to run?public class StopThread &#123; private static boolean stopRequested; public static void main(String[] args) throws InterruptedException &#123; Thread backgroundThread = new Thread(() -&gt; &#123; int i = 0; while (!stopRequested) i++; &#125;); backgroundThread.start(); TimeUnit.SECONDS.sleep(1); stopRequested = true; &#125;&#125; 在缺乏同步的情况下，无法保证后台线程何时（如果有的话）看到主线程所做的 stopRequested 值的更改。在缺乏同步的情况下，虚拟机可以很好地转换这段代码： 123456while (!stopRequested) i++;into this code:if (!stopRequested) while (true) i++; 这种优化称为提升，这正是 OpenJDK 服务器 VM 所做的。结果是活性失败：程序无法取得进展。 注意，写方法（requestStop）和读方法（stopRequested）都是同步的。仅同步写方法是不够的！除非读和写操作都同步，否则不能保证同步工作。虽然 volatile 修饰符不执行互斥，但它保证任何读取字段的线程都会看到最近写入的值。 123456// Broken - requires synchronization!private static volatile int nextSerialNumber = 0;public static int generateSerialNumber() &#123; return nextSerialNumber++;&#125; 问题在于增量运算符 (++) 不是原子性的。它对 nextSerialNumber 字段执行两个操作：首先读取值，然后返回一个新值，旧值再加 1。如果第二个线程在读取旧值和写入新值之间读取字段，则第二个线程将看到与第一个线程相同的值，并返回相同的序列号。可以将 synchronized 添加到方法声明中，另外，也使用 AtomicLong 类，它是 java.util.concurrent.atomic 的一部分。 总之，当多个线程共享可变数据时，每个读取或写入数据的线程都必须执行同步。 在缺乏同步的情况下，不能保证一个线程的更改对另一个线程可见。 避免过度同步：过度的同步可能导致性能下降、死锁甚至不确定行为。作为规则，你应该在同步区域内做尽可能少的工作。获取锁，检查共享数据，根据需要进行转换，然后删除锁。如果你必须执行一些耗时的活动，请设法将其移出同步区域。 Executor、task、流优于直接使用线程：java.util.concurrent 已经添加到 Java 中。这个包有一个 Executor 框架，它是一个灵活的基于接口的任务执行工具。对于小程序或负载较轻的服务器，Executors.newCachedThreadPool 通常是一个不错的选择，因为它不需要配置，而且通常「做正确的事情」。但是对于负载沉重的生产服务器来说，缓存的线程池不是一个好的选择！在缓存的线程池中，提交的任务不会排队，而是立即传递给线程执行。如果没有可用的线程，则创建一个新的线程。如果服务器负载过重，所有 CPU 都被充分利用，并且有更多的任务到达，就会创建更多的线程，这只会使情况变得更糟。因此，在负载沉重的生产服务器中，最好使用 Executors.newFixedThreadPool，它为你提供一个线程数量固定的池，或者直接使用 ThreadPoolExecutor 类来实现最大限度的控制。 并发实用工具优于 wait 和 notify：考虑到正确使用 wait 和 notify 的困难，你应该使用更高级别的并发实用工具。java.util.concurrent 中级别较高的实用工具可分为三类：Executor 框架，Item-80 简要介绍了该框架；并发集合；同步器。本条目简要介绍并发集合和同步器。 并发集合是标准集合接口，如 List、Queue 和 Map 的高性能并发实现。为了提供高并发性，这些实现在内部管理它们自己的同步。一些集合接口使用阻塞操作进行了扩展，这些操作将等待（或阻塞）成功执行。例如，BlockingQueue 扩展了 Queue 并添加了几个方法，包括 take，它从队列中删除并返回首个元素，如果队列为空，则等待。 同步器是允许线程彼此等待的对象，允许它们协调各自的活动。最常用的同步器是 CountDownLatch 和 Semaphore。较不常用的是 CyclicBarrier 和 Exchanger。最强大的同步器是 Phaser。 始终使用 wait 习惯用法，即循环来调用 wait 方法；永远不要在循环之外调用它。 循环用于在等待之前和之后测试条件。 文档应包含线程安全属性：类的线程安全的描述通常属于该类的文档注释，但是具有特殊线程安全属性的方法应该在它们自己的文档注释中描述这些属性。没有必要记录枚举类型的不变性。 明智地使用延迟初始化：延迟初始化是延迟字段的初始化，直到需要它的值。与大多数优化一样，延迟初始化的最佳建议是「除非需要，否则不要这样做」。在大多数情况下，常规初始化优于延迟初始化。 不要依赖线程调度器：任何依赖线程调度器来保证正确性或性能的程序都可能是不可移植的。如果线程没有做有用的工作，它们就不应该运行。线程优先级可以少量地用于提高已经工作的程序的服务质量，但绝不应该用于「修复」几乎不能工作的程序。 第十二章 序列化 Java 序列化的替代方案：序列化的一个根本问题是它的可攻击范围太大，且难以保护，而且问题还在不断增多：通过调用 ObjectInputStream 上的 readObject 方法反序列化对象图。这个方法本质上是一个神奇的构造函数，可以用来实例化类路径上几乎任何类型的对象，只要该类型实现 Serializable 接口。避免序列化利用的最好方法是永远不要反序列化任何东西。永远不要反序列化不可信的数据。 序列化是危险的，应该避免。如果你从头开始设计一个系统，可以使用跨平台的结构化数据，如 JSON 或 protobuf。不要反序列化不可信的数据。如果必须这样做，请使用对象反序列化过滤，但要注意，它不能保证阻止所有攻击。避免编写可序列化的类。 非常谨慎地实现 Serializable：使类的实例可序列化非常简单，只需实现 Serializable 接口即可。因为这很容易做到，所以有一个普遍的误解，认为序列化只需要程序员付出很少的努力。而事实上要复杂得多。虽然使类可序列化的即时代价可以忽略不计，但长期代价通常是巨大的： 一旦类的实现被发布，它就会降低更改该类实现的灵活性 增加了出现 bug 和安全漏洞的可能性 增加了与发布类的新版本相关的测试负担 为继承而设计的类（Item-19）很少情况适合实现 Serializable 接口，接口也很少情况适合扩展它。另外，内部类不应该实现 Serializable。 考虑使用自定义序列化形式：当对象的物理表示与其逻辑数据内容有很大差异时，使用默认的序列化形式有四个缺点： 它将导出的 API 永久地绑定到当前的内部实现 它会占用过多的空间 它会消耗过多的时间 它可能导致堆栈溢出 无论你是否使用默认的序列化形式，必须对对象序列化强制执行任何同步操作，就像对读取对象的整个状态的任何其他方法强制执行的那样。无论选择哪种序列化形式，都要在编写的每个可序列化类中声明显式的序列版本 UID。 这消除了序列版本 UID 成为不兼容性的潜在来源。这么做还能获得一个小的性能优势。如果没有提供序列版本 UID，则需要执行高开销的计算在运行时生成一个 UID。不要更改序列版本 UID，除非你想破坏与现有序列化所有实例的兼容性。 防御性地编写 readObject 方法：当对象被反序列化时，对任何客户端不能拥有的对象引用的字段进行防御性地复制至关重要。 因此，对于每个可序列化的不可变类，如果它包含了私有的可变组件，那么在它的 readObjec 方法中，必须要对这些组件进行防御性地复制： 12345678910// readObject method with defensive copying and validity checkingprivate void readObject(ObjectInputStream s) throws IOException, ClassNotFoundException &#123; s.defaultReadObject(); // Defensively copy our mutable components start = new Date(start.getTime()); end = new Date(end.getTime()); // Check that our invariants are satisfied if (start.compareTo(end) &gt; 0) throw new InvalidObjectException(start +&quot; after &quot;+ end);&#125; 下面是编写 readObject 方法的指导原则： 对象引用字段必须保持私有的的类，应防御性地复制该字段中的每个对象 检查任何不变量，如果检查失败，则抛出 InvalidObjectException 如果必须在反序列化后验证整个对象图，那么使用 ObjectInputValidation 接口 不要直接或间接地调用类中任何可被覆盖的方法 对于实例控制，枚举类型优于 readResolve：在可能的情况下，使用枚举类型强制实例控制不变量。如果这是不可能的，并且你需要一个既可序列化又实例控制的类，那么你必须提供一个 readResolve 方法，并确保该类的所有实例字段都是基本类型，或使用 transient 修饰。 考虑以序列化代理代替序列化实例：实现 Serializable 接口的决定增加了出现 bug 和安全问题的可能性，因为它允许使用一种超语言机制来创建实例，而不是使用普通的构造函数。然而，有一种技术可以大大降低这些风险。这种技术称为序列化代理模式。序列化代理模式相当简单。首先，设计一个私有静态嵌套类，它简洁地表示外围类实例的逻辑状态。这个嵌套类称为外围类的序列化代理。它应该有一个构造函数，其参数类型是外围类。这个构造函数只是从它的参数复制数据：它不需要做任何一致性检查或防御性复制。 序列化代理模式有两个限制。它与客户端可扩展的类不兼容；序列化代理模式所增强的功能和安全性并不是没有代价的。 当你发现必须在客户端不可扩展的类上编写 readObject 或 writeObject 方法时，请考虑序列化代理模式。要想稳健地将带有重要约束条件的对象序列化时，这种模式可能是最容易的方法。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.zsstrike.tech/tags/Java/"}]},{"title":"一致性算法","slug":"一致性算法","date":"2020-11-25T08:02:10.000Z","updated":"2022-05-16T07:41:46.271Z","comments":true,"path":"2020/11/25/一致性算法/","link":"","permalink":"http://blog.zsstrike.tech/2020/11/25/%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/","excerpt":"本文主要介绍分布式系统中的一致性算法，包括 Panxos，Raft 和 ZAB 算法。","text":"本文主要介绍分布式系统中的一致性算法，包括 Panxos，Raft 和 ZAB 算法。 一致性概念CAP 理论：对于一个分布式系统，不能同时满足以下三点： 一致性（Consistency）：在分布式系统中的所有数据备份，在同一时刻是否同样的值。 可用性（Availability）：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。 分区容错性（Partition Tolerance）：一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。容忍性就提高了。 一致性模型： 弱一致性：如果能容忍后续的部分或者全部访问不到，则是弱一致性。 最终一致性：如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。如 DNS，Gossip（Cassandra 通信协议）。 强一致性：对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。如 Raft，ZAB，Paxos。 问题：数据不能存在单点上，分布式系统对 fault tolerence 的一般解决方案是 state machine replication。其实我们今天讨论的准确的说，应该是 state machine replication的共识( consensus)算法。paxos其实是一个共识算法。系统的最终一致性，不仅需要达成共识，还会取决于 clientl的行为。 强一致性算法： 主从同步复制： Master 接受写请求 Master 复制日志到 slave Master 等待，直到所有 slave 返回成功信息 问题：一个节点失败，Master阻塞，导致整个集群不可用，保证了一致性，可用性却大大降低。 多数派：每次写都保证写入大于N&#x2F;2个节点，每次读保证从大于N&#x2F;2个节点中读。 问题：并发环境下，无法保证系统正确性，顺序很重要 Paxos：分为 Basic Paxos，Multi Paxos 和 Fast Paxos。 Basic Paxos： 角色分配： Client：请求发起者。像是民众 Proposer：接受 Client 请求，向集群提出提议，像是议员 Acceptor（Voter）：提议投票和接受者，只有形成法定人数（Quorum，一般为多数派）时，提议才会最终被接受。像是国会。 Learner：提议接受者，backup。像是记录员 阶段： Phase 1a：Prepare：proposer 提出一个提案，编号为 N，这个 N 大于之前提出提案的编号。 Phase 1b：Promise：如果 N 大于此 acceptor 之前接受的提案编号，则接受，否则拒绝。 Phase 2a：Accept：如果达到多数派，此时 proposer 发出accept 请求，请求包含编号N，以及提案内容。 Phase 2b：Accepted：如果 N 大于此 acceptor 之前接受的提案编号，则接受，否则拒绝。 问题：部分节点失败，但是达到了 Quoroms 问题：Proposer 失败 问题：活锁，指的是两个 Proposer 互相间隔发出提案，要求进行投票 上图中的提案编号应该依次是 1,2,3,4… 使用 Random Timeout 来解决上述问题。 其他问题：难以实现，效率低（2 轮 RPC） Multi Paxos： 新概念： Leader：唯一的 Proposer，所有请求都需要经过此 Leader 流程：首先执行 Leader 竞选（Basic Paxos 阶段1），选择之后直接执行 Basic Paxos 阶段2即可 简化：减少角色，Server 之一同时充当 Proposer 和 Acceptor 第一阶段也是进行 Leader 选举，然后再 Propose 提案。 强一致性算法Raft 算法： 三个问题： Leader Election：通过 Election Timeout 来转变为 Candidate，进行选举，选举成功后，Leader 会发送 heartbeat timeout，来表示自己存在与网络当中 问题：Leader 宕机了，剩余节点继续执行 Leader Election 问题：如果两个节点同时成为 Candidate，则通过 Random Timeout 来恢复 Log Replication：首先将 log entry 发送给 follower，之后如果获取到大多数投票后，就进行数据持久化，同时发送消息给客户端，最后发送信号给 follower，进行持久化 Safety：发生故障时或者网络发生分区后，如何进行数据恢复。 在下面的网络分区由于没有达到多数派，数据不会被持久化，但是上面的分区却可以进行数据持久化，这也是为什么一般集群中节点的数量是奇数的原因。当网络被修复之后，由于上面一部分的 Term 大于下面一部分，下面一部分就会更改 Leader，同事新的 Leader 会发送数据包进行数据持久化操作。 重新定义角色： Leader：整个集群只有一个 Leader Follower：只会接受来自 Leader 的请求 Candidate：准备竞选 Leader 的节点 原理动画解释：http://thesecretlivesofdata.com/raft/ 场景测试：https://raft.github.io ZAB：基本上和 Raft 相同，在一些名词的法上有些区别：如 ZAB 将某一个 leader 的周期称为 epoch,而 raft 则称之为term。实现上也有些许不同：如 raft 保证日志连续性，心跳方向为 leader 至 follower，而 ZAB 则相反。","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://blog.zsstrike.tech/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"深入理解JAVA虚拟机笔记","slug":"深入理解JAVA虚拟机笔记","date":"2020-11-12T10:59:12.000Z","updated":"2022-05-16T07:41:46.314Z","comments":true,"path":"2020/11/12/深入理解JAVA虚拟机笔记/","link":"","permalink":"http://blog.zsstrike.tech/2020/11/12/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JAVA%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%AC%94%E8%AE%B0/","excerpt":"本文主要整理由周志明编写的《深入理解Java虚拟机》第三版书籍的整理笔记。","text":"本文主要整理由周志明编写的《深入理解Java虚拟机》第三版书籍的整理笔记。 第二章 Java内存区域与内存溢出异常运行时数据区域： 程序计数器：通过改变其值来获取下一条需要执行的字节码指令。 虚拟机栈：每个方法执行的时候会创建一个栈帧，用于存储局部变量，方法出口等信息。 本地方法栈：同虚拟机栈，只不过本地方法栈是为本地方法服务的。 堆：几乎所有的对象实例都会在这里面分配。 方法区：用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。 运行时常量池：是方法区的一部分，常量池表，Class文件中描述信息会放在此处。 直接内存：在JDK 1.4中新加入了NIO（New Input&#x2F;Output）类，引入了一种基于通道（Channel）与缓冲区（Buffer）的I&#x2F;O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆里面的DirectByteBuffer对象作为这块内存的引用进行操作。 对象的创建：当Java虚拟机遇到一条字节码new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。在类加载检查通过后，接下来虚拟机将为新生对象分配内存。分配方式有指针碰撞和空闲列表两种方式。接下来，就需要执行构造函数了，也就是Class文件中的&lt;init&gt;()方法。 对象的内存布局：对象在堆里面的内存布局分为三部分：对象头，实例数据，对齐填充 对象头：第一类用于存储对象自身的运行时数据，如哈希码，GC分代年龄等，第二部分是类型指针，用于确定该对象是那个类的实例。 实例数据：从父类继承和该类中定义的数据。 对齐填充：用于保证对象是8字节对齐的。 对象的访问定位：主流的方式有两种，使用句柄或者使用直接指针，HotSpot虚拟机使用直接指针方式。 句柄：好处就是reference中存储的是稳定句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而referrence不用修改 直接指针：好处就是速度更快，它节省了一次指针定位的时间开销 第三章 垃圾收集器与内存分配策略引用计数算法：在对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加一；当引用失效时，计数器值就减一；任何时刻计数器为零的对象就是不可能再被使用的。该方法不能检测循环引用。 可达性分析算法：基本思路就是通过一系列称为“GC Roots”的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链”（Reference Chain），如果某个对象到GC Roots间没有任何引用链相连，或者用图论的话来说就是从GC Roots到这个对象不可达时，则证明此对象是不可能再被使用的。在Java技术体系中，GC Roots对象有： 虚拟机栈中引用的对象 在方法区中类静态属性引用的对象，常量引用的对象 同步锁持有的对象 引用类型：在JDK 1.2版之后，Java对引用的概念进行了扩充，有以下几类 强引用：最传统的“引用”的定义，是指在程序代码之中普遍存在的引用赋值 软引用：来描述一些还有用，但非必须的对象。只被软引用关联着的对象，在系统将要发生内存溢出异常前，会把这些对象列进回收范围之中进行第二次回收 弱引用：也是用来描述那些非必须对象，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生为止 虚引用：最弱的一种引用关系，一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例，为一个对象设置虚引用关联的唯一目的只是为了能在这个对象被收集器回收时收到一个系统通知 对象自我拯救：即使在可达性分析算法中判定为不可达的对象，这时候它们暂时还处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记，随后进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法。假如对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，那么虚拟机将这两种情况都视为“没有必要执行”。如果这个对象被判定为确有必要执行finalize()方法，那么该对象将会被放置在一个名为F-Queue的队列之中，并在稍后由一条由虚拟机自动建立的、低调度优先级的Finalizer线程去执行它们的finalize()方法。finalize()方法是对象逃脱死亡命运的最后一次机会，稍后收集器将对F-Queue中的对象进行第二次小规模的标记，如果对象要在finalize()中成功拯救自己，即只要重新与引用链上的任何一个对象建立关联即可，譬如把自己（this关键字）赋值给某个类变量或者对象的成员变量，那在第二次标记时它将被移出“即将回收”的集合。 回收方法区：在Java堆中，尤其是在新生代中，对常规应用进行一次垃圾收集通常可以回收70%至99%的内存空间，相比之下，方法区回收囿于苛刻的判定条件，其区域垃圾收集的回收成果往往远低于此。方法区的垃圾收集主要回收两部分内容：废弃的常量和不再使用的类型。 分代收集理论：建立在三个假说之上： 弱分代假说：绝大多数对象都是朝生夕灭的。 强分代假说：熬过越多次垃圾收集过程的对象就越难以消亡。 跨代引用假说：跨代引用相对于同代引用来说仅占极少数。 前两个假说表明如果一个区域中大多数对象都是朝生夕灭，难以熬过垃圾收集过程的话，那么把它们集中放在一起，每次回收时只关注如何保留少量存活而不是去标记那些大量将要被回收的对象，就能以较低代价回收到大量的空间；如果剩下的都是难以消亡的对象，那把它们集中放在一块，虚拟机便可以使用较低的频率来回收这个区域，这就同时兼顾了垃圾收集的时间开销和内存的空间有效利用；第三点表明我们就不应再为了少量的跨代引用去扫描整个老年代，也不必浪费空间专门记录每一个对象是否存在及存在哪些跨代引用。 标记-清除算法：首先标记出所有需要回收的对象，在标记完成后，统一回收掉所有被标记的对象。缺点：第一个是执行效率不稳定，如果Java堆中包含大量对象，而且其中大部分是需要被回收的，这时必须进行大量标记和清除的动作，导致标记和清除两个过程的执行效率都随对象数量增长而降低；第二个是内存空间的碎片化问题，标记、清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 标记-复制算法：将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。如果内存中多数对象都是存活的，这种算法将会产生大量的内存间复制的开销，但对于多数对象都是可回收的情况，算法需要复制的就是占少数的存活对象，而且每次都是针对整个半区进行内存回收，分配内存时也就不用考虑有空间碎片的复杂情况，只要移动堆顶指针，按顺序分配即可。这样实现简单，运行高效，不过其缺陷也显而易见，这种复制回收算法的代价是将可用内存缩小为了原来的一半，空间浪费未免太多了一点。 新生代存在朝生夕灭现象，存活者大概只有 10% 左右，内存空间比可以分为 8 ：1 标记-整理算法：其中的标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存。标记-清除算法与标记-整理算法的本质差异在于前者是一种非移动式的回收算法，而后者是移动式的。是否移动回收后的存活对象是一项优缺点并存的风险决策：在老年代这种每次回收都有大量对象存活区域，移动存活对象并更新所有引用这些对象的地方将会是一种极为负重的操作，而且这种对象移动操作必须全程暂停用户应用程序才能进行；而不移动对象的时候又存在空间碎片化问题。 经典垃圾收集器： Serial 收集器：是一个单线程工作的收集器，但它的“单线程”的意义并不仅仅是说明它只会使用一个处理器或一条收集线程去完成垃圾收集工作，更重要的是强调在它进行垃圾收集时，必须暂停其他所有工作线程，直到它收集结束。优点是简单，内存消耗低；缺点是需要暂停其他工作线程。 ParNew 收集器：实质上是Serial收集器的多线程并行版本。 Parallel Scavenge收集器：也是一款新生代收集器，它同样是基于标记-复制算法实现的收集器，也是能够并行收集的多线程收集器，它的特点在于它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量。 Serial Old收集器：Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法。 Parallel Old收集器：是Parallel Scavenge收集器的老年代版本，支持多线程并发收集，基于标记-整理算法实现。吞吐量优先收集器。 CMS（Concurrent Mark Sweep）收集器：是一种以获取最短回收停顿时间为目标的收集器。很大一部分Java应用基于 B&#x2F;S 实现，这类应用通常都会较为关注服务的响应速度，希望系统停顿时间尽可能短，以给用户带来良好的交互体验。CMS收集器就非常符合这类应用的需求。收集过程如下： 初始标记：记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快；Stop the World 并发标记：是从GC Roots的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行； 重新标记：则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录；Stop the World 并发清除：清理删除掉标记阶段判断的已经死亡的对象，由于不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发的。 优点：并发收集，低停顿；缺点：对处理器资源敏感（并发阶段会导致应用程序变慢，降低总吞吐量），无法处理“浮动垃圾”（并发清理阶段，用户线程是还在继续运行的，程序在运行自然就还会伴随有新的垃圾对象不断产生），基于标记清除，空间碎片化问题严重。 Garbage First收集器：简称 G1 收集器，在G1收集器出现之前的所有其他收集器，包括CMS在内，垃圾收集的目标范围要么是整个新生代（Minor GC），要么就是整个老年代（Major GC），再要么就是整个Java堆（Full GC）。而G1跳出了这个樊笼，它可以面向堆内存任何部分来组成回收集（Collection Set，一般简称CSet）进行回收，衡量标准不再是它属于哪个分代，而是哪块内存中存放的垃圾数量最多，回收收益最大，这就是G1收集器的Mixed GC模式。G1不再坚持固定大小以及固定数量的分代区域划分，而是把连续的Java堆划分为多个大小相等的独立区域（Region），每一个Region都可以根据需要，扮演新生代的Eden空间、Survivor空间，或者老年代空间。收集器能够对扮演不同角色的Region采用不同的策略去处理，从而获取更好的收集效果。G1收集器过程： 初始标记：仅仅只是标记一下GC Roots能直接关联到的对象，需要短暂停顿 并发标记：从GC Root开始对堆中对象进行可达性分析，递归扫描整个堆里的对象图，找出要回收的对象，这阶段耗时较长，但可与用户程序并发执行。 最终标记：对用户线程做另一个短暂的暂停，用于处理并发阶段结束后仍遗留下来的最后那少量的SATB记录。 筛选回收：负责更新Region的统计数据，对各个Region的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划，可以自由选择任意多个Region构成回收集，然后把决定回收的那一部分Region的存活对象复制到空的Region中，再清理掉整个旧Region的全部空间。这里的操作涉及存活对象的移动，是必须暂停用户线程，由多条收集器线程并行完成的。 低延迟垃圾收集器：HotSpot的垃圾收集器从Serial发展到CMS再到G1，经历了逾二十年时间，经过了数百上千万台服务器上的应用实践，已经被淬炼得相当成熟了，不过它们距离“完美”还是很遥远。衡量垃圾收集器的三项最重要的指标是：内存占用（Footprint）、吞吐量（Throughput）和延迟（Latency），三者共同构成了一个“不可能三角”。图3-14中浅色阶段表示必须挂起用户线程，深色表示收集器线程与用户线程是并发工作的。 Shenandoah收集器：Shenandoah作为第一款不由Oracle（包括以前的Sun）公司的虚拟机团队所领导开发的HotSpot垃圾收集器，不可避免地会受到一些来自“官方”的排挤。Shenandoah反而更像是G1的下一代继承者，它们两者有着相似的堆内存布局，在初始标记、并发标记等许多阶段的处理思路上都高度一致，甚至还直接共享了一部分实现代码。虽然Shenandoah也是使用基于Region的堆内存布局，同样有着用于存放大对象的Humongous Region，默认的回收策略也同样是优先处理回收价值最大的Region……但在管理堆内存方面，它与G1至少有三个明显的不同之处，最重要的当然是支持并发的整理算法，G1的回收阶段是可以多线程并行的，但却不能与用户线程并发；其次，Shenandoah（目前）是默认不使用分代收集的；Shenandoah摒弃了在G1中耗费大量内存和计算资源去维护的记忆集，改用名为“连接矩阵”（Connection Matrix）的全局数据结构来记录跨Region的引用关系，降低了处理跨代指针时的记忆集维护消耗。大致上可以分为九个阶段：初始标记，并发标记，最终标记，并发清理，并发回收，初始引用更新，最终引用更新，并发清理。 最重要三个阶段是并发标记、并发回收、并发引用更新。 ZCG收集器：Z Garbage Collector，是由Oracle公司研发的。ZGC和Shenandoah的目标是高度相似的，都希望在尽可能对吞吐量影响不太大的前提下[2]，实现在任意堆内存大小下都可以把垃圾收集的停顿时间限制在十毫秒以内的低延迟。ZGC也采用基于Region的堆内存布局，但与它们不同的是，ZGC的Region（在一些官方资料中将它称为Page或者ZPage，本章为行文一致继续称为Region）具有动态性——动态创建和销毁，以及动态的区域容量大小。Shenandoah使用转发指针和读屏障来实现并发整理，ZGC虽然同样用到了读屏障，但用的却是一条与Shenandoah完全不同，更加复杂精巧的解题思路：染色指针技术。分为四个阶段：并发标记，并发预备重分配，并发重分配，并发重映射。 Epsilon收集器：这是一款以不能够进行垃圾收集为“卖点”的垃圾收集器，要负责堆的管理与布局、对象的分配、与解释器的协作、与编译器的协作、与监控子系统协作等职责，其中至少堆的管理和对象的分配这部分功能是Java虚拟机能够正常运作的必要支持，是一个最小化功能的垃圾收集器也必须实现的内容。对弈比较小的应用有用武之地。 第四章 虚拟机性能监控、故障处理工具基础故障处理工具： jps：虚拟机进程状况工具，可以列出正在运行的虚拟机进程，并显示虚拟机执行主类（Main Class，main()函数所在的类）名称以及这些进程的本地虚拟机唯一ID（LVMID，Local Virtual Machine Identifier）。 jstat（JVM Statistics Monitoring Tool）：用于监视虚拟机各种运行状态信息的命令行工具，可以显示本地或者远程[1]虚拟机进程中的类加载、内存、垃圾收集、即时编译等运行时数据。 jinfo（Configuration Info for Java）：实时查看和调整虚拟机各项参数。 jmap（Memory Map for Java）：用于生成堆转储快照（一般称为heapdump或dump文件）。 jhat（JVM Heap Analysis Tool）：与jmap搭配使用，来分析jmap生成的堆转储快照。 jstack（Stack Trace for Java）：用于生成虚拟机当前时刻的线程快照（一般称为threaddump或者 javacore文件），线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的目的通常是定位线程出现长时间停顿的原因，如线程间死锁、死循环。 可视化故障处理工具： JHSDB：基于服务性代理的调试工具 JConsole：Java监视与管理控制台 VisualVM：多合-故障处理工具，是功能最强大的运行监视和故障处理程序之一 JMC（Java Mission Control）：可持续在线的监控工具 第六章 类文件结构平台无关性：字节码(Byte Code)文件是构成平台无关性的基石，Java 虚拟机只接受字节码文件，而不管这些文件是怎么的得到的，这就为其他语言可以运行在 Java 虚拟机上提供了基础。 Class类文件的结构： Class文件是一组以8个字节为基础单位的二进制流，并且按照 Big-Endian 来排列位数较大的数。 Class文件采用一种类似C语言的言结构体的伪结构来存储数据，这种伪结构中只有两种数据类型：“无符号数”和“表”。 无符号数属于基本的数据类型，以u1、u2、u4、u8来分别代表1个字节、2个字节、4个字节和8个字节的无符号数，无符号数可以用来描述数字、索引引用、数量值或者按照UTF-8编码构成字符串值。 表是由多个无符号数或者其他表作为数据项构成的复合数据类型，为了便于区分，所有表的命名都习惯性地以“_info”结尾。整个Class文件本质上也可以视作是一张表： 无论是无符号数还是表，当需要描述同一类型但数量不定的多个数据时，经常会使用一个前置的容量计数器加若干个连续的数据项的形式，这时候称这一系列连续的某一类型的数据为某一类型的“集合”。 魔数：每个Class文件的头4个字节被称为魔数（Magic Number），它的唯一作用是确定这个文件是否为一个能被虚拟机接受的Class文件。文件格式的制定者可以自由地选择魔数值，只要这个魔数值还没有被广泛采用过而且不会引起混淆。Class文件的魔数取得很有“浪漫气息”，值为0xCAFEBABE（咖啡宝贝？）。 Class 文件的版本：第5和第6个字节是次版本号（Minor Version），第7和第8个字节是主版本号（Major Version）。 常量池：常量池可以比喻为Class文件里的资源仓库，它是Class文件结构中与其他项目关联最多的数据，通常也是占用Class文件空间最大的数据项目之一。由于常量池中常量的数量是不固定的，所以在常量池的入口需要放置一项u2类型的数据，代表常量池容量计数值（constant_pool_count）。与Java中语言习惯不同，这个容量计数是从1而不是0开始的。这样做的目的在于，如果后面某些指向常量池的索引值的数据在特定情况下需要表达“不引用任何一个常量池项目”的含义，可以把索引值设置为0来表示。常量池中主要存放两大类常量：字面量（Literal）和符号引用（Symbolic References）。常量池中每一项常量都是一个表，截至JDK13，常量表中分别有17种不同类型的常量。这17类表都有一个共同的特点，表结构起始的第一位是个u1类型的标志位（tag，取值见表6-3中标志列），代表着当前常量属于哪种常量类型。17种常量类型： 访问标志：用于识别一些类或者接口层次的访问信息，包括：这个Class是类还是接口；是否定义为public类型；是否定义为abstract类型；如果是类的话，是否被声明为final； 类索引、父类索引与接口索引集合：类索引（this_class）和父类索引（super_class）都是一个u2类型的数据，而接口索引集合（interfaces）是一组u2类型的数据的集合，Class文件中由这三项数据来确定该类型的继承关系。 字段表集合：字段表（field_info）用于描述接口或者类中声明的变量。 字段修饰符放在access_flags项目中，跟随access_flags标志的是两项索引值：name_index descriptor_index。它们都是对常量池项的引用，分别代表着字段的简单名称以及字段和方法的描述符。 方法表集合：Class文件存储格式中对方法的描述与对字段的描述采用了几乎完全一致的方式，方法表的结构如同字段表一样，依次包括访问标志（access_flags）、名称索引（name_index）、描述符索引（descriptor_index）、属性表集合（attributes）几项，如表6-11所示。 属性表集合：属性表（attribute_info）在前面的讲解之中已经出现过数次，Class文件、字段表、方法表都可以携带自己的属性表集合，以描述某些场景专有的信息。部分属性表如下： 对于每一个属性，它的名称都要从常量池中引用一个CONSTANT_Utf8_info类型的常量来表示，而属性值的结构则是完全自定义的，只需要通过一个u4的长度属性去说明属性值所占用的位数即可。 字节码指令简介：由于Java虚拟机采用面向操作数栈而不是面向寄存器的架构，所以大多数指令都不包含操作数，只有一个操作码（一个字节），指令参数都存放在操作数栈中。由于Class文件格式放弃了编译后代码的操作数长度对齐，这就意味着虚拟机在处理那些超过一个字节的数据时，不得不在运行时从字节中重建出具体数据的结构；放弃了操作数长度对齐，就意味着可以省略掉大量的填充和间隔符号；用一个字节来代表操作码，也是为了尽可能获得短小精干的编译代码。 字节码和数据类型：大多数指令都包含其操作所对应的数据类型信息，如iload，fload。大部分指令都没有支持整数类型byte、char和short，甚至没有任何指令支持boolean类型。编译器会在编译期或运行期将byte和short类型的数据带符号扩展（Sign-Extend）为相应的int类型数据，将boolean和char类型数据零位扩展（Zero-Extend）为相应的int类型数据。 加载和存储指令：iload、iload_&lt;n&gt;、，istore、istore_&lt;n&gt;等 运算指令：算术指令用于对两个操作数栈上的值进行某种特定运算，并把结果重新存入到操作栈顶。分为两种：对整型数据进行运算的指令与对浮点型数据进行运算的指令。换句话说是不存在直接支持byte、short、char和boolean类型的算术指令，对于上述几种数据的运算，应使用操作int类型的指令代替。指令有：iadd、ladd、fadd、dadd等。Java虚拟机在进行浮点数运算时，所有的运算结果都必须舍入到适当的精度，非精确的结果必须舍入为可被表示的最接近的精确值；如果有两种可表示的形式与该值一样接近，那将优先选择最低有效位为零的；而在把浮点数转换为整数时，Java虚拟机使用IEEE 754标准中的向零舍入模式，这种模式的舍入结果会导致数字被截断，所有小数部分的有效字节都会被丢弃掉。 类型转换指令：Java虚拟机直接支持（即转换时无须显式的转换指令）宽化类型转换（即小范围类型向大范围类型的安全转换），处理窄化类型转换（Narrowing Numeric Conversion）时，就必须显式地使用转换指令来完成，这些转换指令包括i2b、i2c、i2s、l2i、f2i、f2l、d2i、d2l和d2f。在将int或long类型窄化转换为整数类型T的时候，转换过程仅仅是简单丢弃除最低位N字节以外的内容，N是类型T的数据类型长度，这将可能导致转换结果与输入值有不同的正负号。 对象创建和访问指令：对象创建后，就可以通过对象访问指令获取对象实例或者数组实例中的字段或者数组元素，这些指令包括： 创建类实例的指令：new 创建数组的指令：newarray、anewarray、multianewarray 访问类字段（static字段，或者称为类变量）和实例字段（非static字段，或者称为实例变量）的指令：getfield、putfield、getstatic、putstatic 把一个数组元素加载到操作数栈的指令：baload、caload、saload、iaload、laload、faload、daload、aaload 将一个操作数栈的值储存到数组元素中的指令：bastore、castore、sastore、iastore、fastore、dastore、aastore 取数组长度的指令：arraylength 检查类实例类型的指令：instanceof、checkcast 操作数栈管理指令：pop，dup，swap等 控制转移指令：ifeq，ret，if_icmpeq等 方法调用和返回指令： invokevirtual指令：用于调用对象的实例方法，根据对象的实际类型进行分派（虚方法分派），这也是Java语言中最常见的方法分派方式。 invokeinterface指令：用于调用接口方法，它会在运行时搜索一个实现了这个接口方法的对象，找出适合的方法进行调用。 invokespecial指令：用于调用一些需要特殊处理的实例方法，包括实例初始化方法、私有方法和父类方法。 invokestatic指令：用于调用类静态方法（static方法）。 invokedynamic指令：用于在运行时动态解析出调用点限定符所引用的方法。并执行该方法。前面四条调用指令的分派逻辑都固化在Java虚拟机内部，用户无法改变，而invokedynamic指令的分派逻辑是由用户所设定的引导方法决定的。 异常处理指令：在Java程序中显式抛出异常的操作（throw语句）都由athrow指令来实现。 同步指令：Java虚拟机可以支持方法级的同步和方法内部一段指令序列的同步，这两种同步结构都是使用管程（Monitor，更常见的是直接将它称为“锁”）来实现的。当方法调用时，调用指令将会检查方法的ACC_SYNCHRONIZED访问标志是否被设置，如果设置了，执行线程就要求先成功持有管程，然后才能执行方法，最后当方法完成（无论是正常完成还是非正常完成）时释放管程。同步一段指令集序列通常是由Java语言中的synchronized语句块来表示的，Java虚拟机的指令集中有monitorenter和monitorexit两条指令来支持synchronized关键字的语义。 公有设计，私有实现：《Java虚拟机规范》描绘了Java虚拟机应有的共同程序存储格式：Class文件格式以及字节码指令集。但一个优秀的虚拟机实现，在满足《Java虚拟机规范》的约束下对具体实现做出修改和优化也是完全可行的。虚拟机实现的方式主要有以下两种： 将输入的Java虚拟机代码在加载时或执行时翻译成另一种虚拟机的指令集； 将输入的Java虚拟机代码在加载时或执行时翻译成宿主机处理程序的本地指令集（即即时编译器代码生成技术） Class文件结构的发展：相对于语言、API以及Java技术体系中其他方面的变化，Class文件结构一直处于一个相对比较稳定的状态，Class文件的主体结构、字节码指令的语义和数量几乎没有出现过变动，所有对Class文件格式的改进，都集中在访问标志、属性表这些设计上原本就是可扩展的数据结构中添加新内容。 第七章 虚拟机类加载机制概述：Java虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这个过程被称作虚拟机的类加载机制。与那些在编译时需要进行连接的语言不同，在Java语言里面，类型的加载、连接和初始化过程都是在程序运行期间完成的，这种策略让Java语言进行提前编译会面临额外的困难，也会让类加载时稍微增加一些性能开销，但是却为Java应用提供了极高的扩展性和灵活性。 类加载的时机：一个类的整个生命周期如下： 加载、验证、准备、初始化和卸载这五个阶段的顺序是确定的，而解析阶段则不一定：它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定特性（也称为动态绑定或晚期绑定）。但是对于初始化阶段，《Java虚拟机规范》则是严格规定了有且只有六种情况必须立即对类进行“初始化”（而加载、验证、准备自然需要在此之前开始）： 遇到new、getstatic、putstatic或invokestatic这四条字节码指令时，如果类型没有进行过初始化，则需要先触发其初始化阶段。如使用new关键字，读取或设置一个类的静态字段，调用一个类的静态方法。 使用java.lang.reflect包的方法对类型进行反射调用的时候，如果类型没有进行过初始化，则需要先触发其初始化。 当初始化类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类。 如果一个java.lang.invoke.MethodHandle实例最后的解析结果为REF_getStatic、REF_putStatic、REF_invokeStatic、REF_newInvokeSpecial四种类型的方法句柄，并且这个方法句柄对应的类没有进行过初始化，则需要先触发其初始化。 当一个接口中定义了JDK 8新加入的默认方法（被default关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化。 类加载过程：加载、验证、准备、解析和初始化这五个阶段所执行的具体动作。 加载： 通过一个类的全限定名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。 验证：这一阶段的目的是确保Class文件的字节流中包含的信息符合《Java虚拟机规范》的全部约束要求，保证这些信息被当作代码运行后不会危害虚拟机自身的安全。 文件格式验证 元数据验证：这个类的父类是否继承了不允许被继承的类；如果这个类不是抽象类，是否实现了其父类或接口之中要求实现的所有方法 字节码验证：这阶段就要对类的方法体（Class文件中的Code属性）进行校验分析，如保证任何跳转指令都不会跳转到方法体以外的字节码指令上，保证方法体中的类型转换总是有效的等。 符号引用验证：验证该类是否缺少或者被禁止访问它依赖的某些外部类、方法、字段等资源。如符号引用中的类、字段、方法的可访问性（private、protected、public、&lt;package&gt;）是否可被当前类访问等。 准备：是正式为类中定义的变量（即静态变量，被static修饰的变量）分配内存并设置类变量初始值的阶段。需要注意的是如果是static int value = 123，准备阶段的初始值是0而不是123，因为这时尚未开始执行任何Java方法，而把value赋值为123的putstatic指令是程序被编译后，存放于类构造器&lt;clinit&gt;()方法之中的；但是如果是public static final int value = 123，那么准备阶段的值就是123。 解析：是Java虚拟机将常量池内的符号引用替换为直接引用的过程。 类或接口的解析：如果C不是一个数组类型，那虚拟机将会把代表N的全限定名传递给D的类加载器去加载这个类C。在加载过程中，由于元数据验证、字节码验证的需要，又可能触发其他相关类的加载动作，例如加载这个类的父类或实现的接口。一旦这个加载过程出现了任何异常，解析过程就将宣告失败。成功的话，那么C在虚拟机中实际上已经成为一个有效的类或接口了，但在解析完成前还要进行符号引用验证，确认D是否具备对C的访问权限。 字段解析 方法解析 接口方法解析 初始化：直到初始化阶段，Java虚拟机才真正开始执行类中编写的Java程序代码，将主导权移交给应用程序。初始化阶段就是执行类构造器&lt;clinit&gt;()方法的过程。&lt;clinit&gt;()并不是程序员在Java代码中直接编写的方法，它是Javac编译器的自动生成物。&lt;clinit&gt;()方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块（static{}块）中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序决定的，静态语句块中只能访问到定义在静态语句块之前的变量。Java虚拟机会保证在子类的&lt;clinit&gt;()方法执行前，父类的&lt;clinit&gt;()方法已经执行完毕。 类加载器：Java虚拟机设计团队有意把类加载阶段中的“通过一个类的全限定名来获取描述该类的二进制字节流”这个动作放到Java虚拟机外部去实现，以便让应用程序自己决定如何去获取所需的类。实现这个动作的代码被称为“类加载器”（Class Loader）。 类与类加载器：比较两个类是否“相等”，只有在这两个类是由同一个类加载器加载的前提下才有意义，否则，即使这两个类来源于同一个Class文件，被同一个Java虚拟机加载，只要加载它们的类加载器不同，那这两个类就必定不相等。 双亲委派模型： 三层类加载器： 启动类加载器：这个类加载器负责加载存放在&lt;JAVA_HOME&gt;\\lib目录，是Java虚拟机能够识别的（按照文件名识别，如rt.jar、tools.jar，名字不符合的类库即使放在lib目录中也不会被加载）类库加载到虚拟机的内存中。 扩展类加载器：负责加载\\lib\\ext目录中，或者被java.ext.dirs系统变量所指定的路径中所有的类库。 应用程序类加载器：由于应用程序类加载器是ClassLoader类中的getSystemClassLoader()方法的返回值，所以有些场合中也称它为“系统类加载器”。它负责加载用户类路径（ClassPath）上所有的类库，开发者同样可以直接在代码中使用这个类加载器。 JDK 9之前的Java应用都是由这三种类加载器互相配合来完成加载的，如果用户认为有必要，还可以加入自定义的类加载器来进行拓展： 双亲委派模型：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到最顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去完成加载。好处是Java中的类随着它的类加载器一起具备了一种带有优先级的层次关系。 Java模块化系统： 模块的兼容性：JDK 9提出了与“类路径”（ClassPath）相对应的“模块路径”（ModulePath）的概念。简单来说，就是某个类库到底是模块还是传统的JAR包，只取决于它存放在哪种路径上。有如下访问规则： JAR文件在类路径的访问规则：所有类路径下的JAR文件及其他资源文件，都被视为自动打包在一个匿名模块（Unnamed Module）里，这个匿名模块几乎是没有任何隔离的，它可以看到和使用类路径上所有的包、JDK系统模块中所有的导出包，以及模块路径上所有模块中导出的包。 模块在模块路径的访问规则：模块路径下的具名模块（Named Module）只能访问到它依赖定义中列明依赖的模块和包，匿名模块里所有的内容对具名模块来说都是不可见的，即具名模块看不见传统JAR包的内容。 JAR文件在模块路径的访问规则：如果把一个传统的、不包含模块定义的JAR文件放置到模块路径中，它就会变成一个自动模块（Automatic Module）。尽管不包含module-info.class，但自动模块将默认依赖于整个模块路径中的所有模块，因此可以访问到所有模块导出的包，自动模块也默认导出自己所有的包。 模块下的类加载器：JDK 9并没有从根本上动摇从JDK 1.2以来运行了二十年之久的三层类加载器架构以及双亲委派模型。但是为了模块化系统的顺利施行，模块化下的类加载器仍然发生了一些应该被注意到变动，主要包括以下几个方面： 扩展类加载器被平台类加载器取代 平台类加载器和应用程序类加载器都不再派生自java.net.URLClassLoader，现在启动类加载器、平台类加载器、应用程序类加载器全都继承于jdk.internal.loader.BuiltinClassLoader，在BuiltinClassLoader中实现了新的模块化架构下类如从模块中加载的逻辑，以及模块中资源可访问性的处理 JDK 9中虽然仍然维持着三层类加载器和双亲委派的架构，但类加载的委派关系也发生了变动。当平台及应用程序类加载器收到类加载请求，在委派给父加载器加载前，要先判断该类是否能够归属到某一个系统模块中，如果可以找到这样的归属关系，就要优先委派给负责那个模块的加载器完成加载 第八章 虚拟机字节码执行引擎运行时栈帧结构：下图是 JVM 的栈和栈帧的总体结构： 每个栈帧里面包含有局部变量表、操作数栈、动态连接、方法返回地址和一些额外的附加信息。 局部变量表：是一组变量值的存储空间，用于存放方法参数和方法内部定义的局部变量。变量槽空间一般是 32 位，对于long类型的变量，需要两个槽来保存。当方法被调用的时候，首先存储相关的实参，然后再存储方法内部的局部变量。比如，对于实例方法，局部变量表第0位代表的就是方法所属对象的引用，方法中通过 this 隐式访问到。另外，局部变量表中的变量槽可以被重用，这可能会带来副作用，如 gc 过程。最后，局部变量没有所谓的“准备阶段”，因此，对局部变量引用前需要先赋值。 操作数栈：用于保存相应的操作数。在进行运算的时候需要检查指令和对应的数据类型是否匹配。在概念模型上，两个不同的栈帧是完全相互独立的，但是在实际过程中，可能存在重合，这样做的好处是节约空间，同时无需进行额外的实参-形参转换。 动态链接：每个栈帧都包含一个指向运行时常量池[1]中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接（Dynamic Linking）。 方法返回地址：正常返回上层方法调用者，可能会提供返回值，异常返回的话，不带任何返回值。推出的过程实际上等同于将当前栈帧出栈。 方法调用： 解析：在类加载的过程中，如果方法的调用版本在运行期不可变，就可以将方法的符号引用转化为直接引用，该类方法的调用称为解析。在 Java 中，这样的方法有静态方法，私有方法，实例构造器，父类方法，final 方法。这些方法称为“非虚方法”，其他的就成为“虚方法”。 分派（dispatch）： 静态分派：假设 Human man = new Man()，那么Human成为变量的静态类型，或者是外观类型，后面的Man则称为变量的实际类型或者运行时类型。所有依赖静态类型来决定方法执行版本的分派动作，都称为静态分派。静态分派的最典型应用表现就是方法重载。静态分派发生在编译阶段。虽然编译器能够在确定方法重载版本，但是实际上只是选择一个相对更合适的版本。假设有一个类实现了sayHello方法，重载了所有类型的参数。那么对应sayHello(&#39;a&#39;)中的a的类型被解析为char，如果注释掉char类型的重载，那么a会被解析成int类型，依次往后是：Character，Serializable(Character的一个接口)，Object（父类），变长参数。 动态分派：与重写有关。Human man = new Man()，man执行重写方法的时候，会执行Man类里面的对应的方法，而不是Woman里面的重载方法，这与变量的实际类型有关。调用重写方法的时候，执行指令是invokevirtual，其运行过程如下： 找到变量指向对象的实际类型，记做C 如果在C中找到与方法签名一直的方法，进行访问权限校验，通过直接返回这个方法的直接调用，否则返回java.lang.IllegalAccessError 否则，按照继承关系从下往上依次对C的各个父类进行2操作 如果始终没有找到合适方法，抛出java.lang.AbstractMethodError异常 注意，方法存在多态，但是字段不存在多态。 虚拟机动态分派的实现：通常虚拟机会创建一个虚方法表（vtable，对应的还有接口方法表itable），使用虚方法表索引来代替元数据查找以提高性能。 动态类型语言支持： 动态类型语言：动态类型语言的关键特征是它的类型检查的主体过程是在运行期而不是编译期进行的，如Javascript等。那相对地，在编译期就进行类型检查过程的语言，譬如C++和Java等就是最常用的静态类型语言。 Java与动态类型：在 Java7 之前的4条方法调用指令（invoke*）的第一个参数都是被调用方法的符号引用。前面已经提到过，方法的符号引用在编译时产生，而动态类型语言只有在运行期才能确定方法的接收者。这样，在Java虚拟机上实现的动态类型语言就不得不使用“曲线救国”的方式（如编译时留个占位符类型，运行时动态生成字节码实现具体类型到占位符类型的适配）来实现，但这样势必会让动态类型语言实现的复杂度增加，也会带来额外的性能和内存开销。 java.lang.invoke：该包提供了一种新的动态确定方法的机制，称为方法句柄。 1234567891011121314151617181920212223import static java.lang.invoke.MethodHandles.lookup;import java.lang.invoke.MethodHandle;import java.lang.invoke.MethodType;public class MethodHandleTest &#123; static class ClassA &#123; public void println(String s) &#123; System.out.println(s); &#125; &#125; public static void main(String[] args) throws Throwable &#123; Object obj = System.currentTimeMillis() % 2 == 0 ? System.out : new ClassA(); // 无论obj最终是哪个实现类，下面这句都能正确调用到println方法。 getPrintlnMH(obj).invokeExact(&quot;icyfenix&quot;); &#125; private static MethodHandle getPrintlnMH(Object reveiver) throws Throwable &#123; // MethodType：代表“方法类型”，包含了方法的返回值（methodType()的第一个参数）和具体参数（methodType()第二个及以后的参数）。 MethodType mt = MethodType.methodType(void.class, String.class); // lookup()方法来自于MethodHandles.lookup，这句的作用是在指定类中查找符合给定的方法名称、方法类型，并且符合调用权限的方法句柄。 // 因为这里调用的是一个虚方法，按照Java语言的规则，方法第一个参数是隐式的，代表该方法的接收者，也即this指向的对象，这个参数以前是放在参数列表中进行传递，现在提供了bindTo()方法来完成这件事情。 return lookup().findVirtual(reveiver.getClass(), &quot;println&quot;, mt).bindTo(reveiver); &#125;&#125; MethodHandle在使用方法和效果上与Reflection有众多相似之处。不过，它们也有以下这些区别： Reflection和MethodHandle机制本质上都是在模拟方法调用，但是Reflection是在模拟Java代码层次的方法调用，而MethodHandle是在模拟字节码层次的方法调用。 Reflection中的java.lang.reflect.Method对象远比MethodHandle机制中的java.lang.invoke.MethodHandle对象所包含的信息来得多。 Reflection API的设计目标是只为Java语言服务的，而MethodHandle则设计为可服务于所有Java虚拟机之上的语言。 invokedynamic指令：作为Java诞生以来唯一一条新加入的字节码指令，都是为了解决原有4条“invoke*”指令方法分派规则完全固化在虚拟机之中的问题，把如何查找目标方法的决定权从虚拟机转嫁到具体用户代码之中。invokedynamic指令的第一个参数不再是代表方法符号引用的CONSTANT_Methodref_info常量，而是变为JDK 7时新加入的CONSTANT_InvokeDynamic_info常量，从这个新常量中可以得到3项信息：引导方法（Bootstrap Method，该方法存放在新增的BootstrapMethods属性中）、方法类型（MethodType）和名称。 掌控方法分派规则：子类方法不能直接调用祖父类方法，可以通过MethodHandle来进行访问，如遇到权限问题，可以使用lookupImpl.setAccessible(true)来解决。 基于栈的字节码解释执行引擎： 解释执行：Java语言被定为解释执行的语言，这在JDK1.0时代算是准确的，但是之后Java也发展出了可以生成本地代码的编译器，这个时候说Java是解释执行的语言就不再准确了。下图中间分支指代解释执行过程，最下面分支指代编译执行过程： 基于栈的指令集与基于寄存器的指令集：Java指令基于栈结构，x86指令基于寄存器，使用栈结构带来的好处是可移植性更强，缺点是运行速度慢。 第九章 类加载案例案例分析： Tomcat：在Tomcat中一种有四种目录存放Java类库： 放置在&#x2F;common目录中。类库可被Tomcat和所有的Web应用程序共同使用。 放置在&#x2F;server目录中。类库可被Tomcat使用，对所有的Web应用程序都不可见。 放置在&#x2F;shared目录中。类库可被所有的Web应用程序共同使用，但对Tomcat自己不可见。 放置在&#x2F;WebApp&#x2F;WEB-INF目录中。类库仅仅可以被该Web应用程序使用，对Tomcat和其他Web应用程序都不可见。 为了支持这套目录，并且对目录里面的类库进行加载和隔离，Tomcat实现了自定义的类加载器，按照双亲委派模型： 在Tomcat6之后，只有指定了tomcat&#x2F;conf&#x2F;catalina.properties配置文件的server.loader和share.loader项后才会真正建立Catalina类加载器和Shared类加载器的实例，否则会用到这两个类加载器的地方都会用Common类加载器的实例代替。同时前文提到的前三个目录也会被改为一个&#x2F;lib目录。 OSGi：是OSGi联盟（OSGi Alliance）制订的一个基于Java语言的动态模块化规范。OSGi中的每个模块（Bundle）可以声明它所依赖的Package（通过Import-Package描述），也可以声明它允许导出发布的Package（通过Export-Package描述）。这和后来出现的Java模块化功能重合了。由于模块之间相依依赖的原因，加载器之间的关系不再是双亲委派模型的树形结构，而是已经进一步发展成一种更为复杂的、运行时才能确定的网状结构。在模块中相互依赖会造成死锁。 Backport工具：将高级的Java语法转化为低版本Java也能运行的语句代码的工具。 第十章 前端编译和优化Java中前端编译一般指将*.java编译为*.class字节码文件的过程，主要有下列过程： 准备过程：初始化插入式注解处理器 解析与填充符号表过程：词法，语法分析，填充符号表 插入式注解处理器的注解处理过程：插入式注解处理器的执行阶段 分析与字节码生成过程：标注检查，解语法糖，字节码生成 解析与填充符号表： 词法语法分析：词法分析用于生成标记（token）集合的过程，语法分析则是根据标记序列构造抽象语法树的过程。 填充符号表：符号表（Symbol Table）是由一组符号地址和符号信息构成的数据结构，符号表中所登记的信息在编译的不同阶段都要被用到，如类型检查等。 注解处理器：可以把插入式注解处理器看作是一组编译器的插件，当这些插件工作时，允许读取、修改、添加抽象语法树中的任意元素。如果这些插件在处理注解期间对语法树进行过修改，编译器将回到解析及填充符号表的过程重新处理，直到所有插入式注解处理器都没有再对语法树进行修改为止，每一次循环过程称为一个轮次（Round）。 语义分析和字节码生成： 标注检查：变量使用前是否已被声明、变量与赋值之间的数据类型是否能够匹配，等等，在该过程中顺便执行常量折叠优化。 数据及控制流分析：是对程序上下文逻辑更进一步的验证，它可以检查出诸如程序局部变量在使用前是否有赋值、方法的每条路径是否都有返回值、是否所有的受查异常都被正确处理了等问题。final修饰的变量不可变就是在这一阶段完成的。 解语法糖：将一些语法糖进行还原，Java中常见语法糖有泛型，变长参数，自动装箱拆箱等。 字节码生成：把前面各个步骤所生成的信息（语法树、符号表）转化成字节码指令写到磁盘中，编译器还进行了少量的代码添加和转换工作。例如前文多次登场的实例构造器&lt;init&gt;()方法和类构造器&lt;clinit&gt;()方法就是在这个阶段被添加到语法树之中的。实例构造器并不等同于默认构造函数。&lt;init&gt;()和&lt;clinit&gt;()这两个构造器的产生实际上是一种代码收敛的过程，编译器会把语句块（对于实例构造器而言是“{}”块，对于类构造器是“static{}”块）、变量初始化（实例变量和类变量）、调用父类的实例构造器（仅仅是实例构造器，&lt;clinit&gt;()方法中无须调用父类的&lt;clinit&gt;()方法，Java虚拟机会自动保证父类构造器的正确执行，但在&lt;clinit&gt;()方法中经常会生成调用java.lang.Object的&lt;init&gt;()方法的代码）等操作收敛到&lt;init&gt;()和&lt;clinit&gt;()方法之中。 Java语法糖： 泛型：Java选择的泛型实现方式是类型擦除式泛型，而C#选择的泛型实现方式是具现化式泛型。由于采用的是类型擦出式泛型，以下操作不合法： 123456789public class TypeErasureGenerics&lt;E&gt; &#123; public void doSomething(Object item) &#123; if (item instanceof E) &#123; // 不合法，无法对泛型进行实例判断 ... &#125; E newItem = new E(); // 不合法，无法使用泛型创建对象 E[] itemArray = new E[10]; // 不合法，无法使用泛型创建数组 &#125;&#125; java中的泛型只在程序源码中存在，在编译后的字节码文件中，全部泛型都被替换为原来的裸类型（Raw Type）了，并且在相应的地方插入了强制转型代码，因此对于运行期的Java语言来说，ArrayList&lt;int&gt;与ArrayList&lt;String&gt;其实是同一个类型。当初Java选择这种方式实现泛型的历史原因在于Java语言的向后兼容性。 将这段Java代码编译成Class文件，然后再使用反编译工具： 由于类型擦除，导致的问题有：不支持原始类型的泛型，运行期无法取到泛型类型信息。 上述代码不能被编译，相反下列代码可以被编译，因为方法签名不同： 自动装箱，拆箱与遍历循环： 遍历循环的类需要实现Iterable接口的原因从上图可以看出。 条件编译：java语言没有预处理器，但是可以实现条件编译，使用if(true)等。 第十一章 后端编译和优化即时（JIT）编译器：在运行时，虚拟机将会把热点代码编译成本地机器码，并以各种手段尽可能地进行代码优化，运行时完成这个任务的后端编译器被称为即时编译器。 解释器和编译器：解释器与编译器两者各有优势：当程序需要迅速启动和执行的时候，解释器可以首先发挥作用，省去编译的时间，立即运行。当程序启动后，随着时间的推移，编译器逐渐发挥作用，把越来越多的代码编译成本地代码，这样可以减少解释器的中间损耗，获得更高的执行效率。在Java中，解释器和编译器是相互协作的： 为了使程序启动响应速度与运行效率之间达到最佳平衡，HotSpot虚拟机在编译子系统中加入了分层编译的功能。 编译对象与触发条件：热点代码指的是被多次调用的方法，或者是多次执行的循环体。发现对应的热点代码后，编译的目标对象都是整个方法体。为了判断某段代码是不是热点代码，可以使用基于采样的热点探测和基于计数器的热点探测。HotSpot使用的是后者，为了实现热点计数，需要两类计数器：方法调用计数器和回边计数器。一旦超过阈值，就会触发即时编译。 编译过程：在编译请求产生时，虚拟机在编译器还未完成编译之前，都仍然将按照解释方式继续执行代码，而编译动作则在后台的编译线程中进行。下图是编译器的全过程示意图： 提前编译器：目前提前编译有两条路径：一条分支是做与传统C、C++编译器类似的，在程序运行之前把程序代码编译成机器码的静态翻译工作；另外一条分支是把原本即时编译器在运行时要做的编译工作提前做好并保存下来，下次运行到这些代码（譬如公共库代码在被同一台机器其他Java进程使用）时直接把它加载进来使用。第一条路径直指Java中的即时编译的最大弱点：即时编译需要占用程序的运行时间和运算资源。第二条路径本质上是给即时编译器做缓存加速，可以成为动态提前编译（Dynamic AOT）。即时编译器的优点在：性能分析制导优化，激进预测性优化和链接时优化。Java中的提前编译器有Jaotc。 编译器优化技术： 方法内联：是其他优化的基础，减少方法分派的开销 逃逸分析：分析对象动态作用域，当一个对象在方法里面被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他方法中，这种称为方法逃逸；甚至还有可能被外部线程访问到，譬如赋值给可以在其他线程中访问的实例变量，这种称为线程逃逸；从不逃逸、方法逃逸到线程逃逸，称为对象由低到高的不同逃逸程度。根据不同逃逸程度：可以执行栈上分配，标量替换，同步消除。 公共子表达式消除 数组边界检查消除 第十二章 Java内存模型与线程硬件的效率与一致性：由于处理器的运行速度远高于IO的速度，为此引入了高速缓存，但是引入高速缓存又造成了缓存一致性的问题。由此产生一致性协议：MSI，MESI，MOSI等。 Java内存模型： 主内存与工作内存：规定了所有的变量都存储在主内存（Main Memory）中，每条线程还有自己的工作内存（Working Memory，可与前面讲的处理器高速缓存类比），线程的工作内存中保存了被该线程使用的变量的主内存副本，线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，而不能直接读写主内存中的数据。 内存间交互操作：lock，unlock，read，load，store，write，use，assign。伤处操作都是原子的，不可再分的。 对于volatile型变量的特殊规则：当一个变量被volatile定义的时候，将具有： 保证此变量对所有线程的可见性，这里的“可见性”是指当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。 禁止指令重排序优化，普通的变量仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执行顺序一致。 针对long和double型变量的特殊规则：允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32位的操作来进行，即允许虚拟机实现自行选择是否要保证64位数据类型的load、store、read和write这四个操作的原子性。 原子性，可见性与有序性：由Java内存模型来直接保证的原子性变量操作包括read、load、assign、use、store和write这六个；可见性就是指当一个线程修改了共享变量的值时，其他线程能够立即得知这个修改；如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的。 先行发生原则： 程序次序规则：在一个线程内，按照控制流顺序，书写在前面的操作先行发生于书写在后面的操作，是控制流顺序。 管程锁定规则：一个unlock操作先行发生于后面对同一个锁的lock操作。 volatile变量规则：对一个volatile变量的写操作先行发生于后面对这个变量的读操作。 线程启动规则：Thread对象的start()方法先行发生于此线程的每一个动作。 线程终止规则：线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread::join()方法是否结束、Thread::isAlive()的返回值等手段检测线程是否已经终止执行。 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread::interrupted()方法检测到是否有中断发生。 对象终结规则：一个对象的初始化完成（构造函数执行结束）先行发生于它的finalize()方法的开始。 传递性：如果操作A先行发生于操作B，操作B先行发生于操作C，那就可以得出操作A先行发生于操作C的结论。 Java与线程： 线程的实现：内核线程实现，用户线程实现，混合实现。 Java线程调度： 协同式线程调度：线程的执行时间由线程本身来控制，线程把自己的工作执行完了之后，要主动通知系统切换到另外一个线程上去。 抢占式线程：每个线程将由系统来分配执行时间，线程的切换不由线程本身来决定。 Java线程调度是由系统自动完成的，但是可以为不同的线程分配不同的优先级，来建议操作系统多分配一些时间在优先级高的线程上。 状态转换：Java中定义了6种线程状态： Java与协程： 内核线程的局限：天然的缺陷是切换、调度成本高昂，系统能容纳的线程数量也很有限。 协程的复苏：由于最初多数的用户线程是被设计成协同式调度的，所以它有了一个别名——“协程”（Coroutine）。又由于这时候的协程会完整地做调用栈的保护、恢复工作，所以今天也被称为“有栈协程”。协程的主要优势是轻量，缺点是需要在应用层面实现的内容（调用栈、调度器这些）特别多。 Java的解决方案：纤程（fiber），一种轻量的线程，使用JVM调度，而不是操作系统。 第十三章 线程安全与锁优化线程安全：当多个线程同时访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那就称这个对象是线程安全的。 Java语言中的线程安全： 不可变：不可变的对象一定是线程安全，无论是对象的方法实现还是方法的调用者，都不需要再进行任何线程安全保障措施。基本类型数据使用final关键字修饰可以保证不可变，如果想要保证对象不可变，需要将对象的字段设置为final才可以。 绝对线程安全：不管运行时环境如何，调用者都不需要任何额外的同步措施。 相对线程安全：通常意义上所讲的线程安全，它需要保证对这个对象单次的操作是线程安全的，我们在调用的时候不需要进行额外的保障措施，但是对于一些特定顺序的连续调用，就可能需要在调用端使用额外的同步手段来保证调用的正确性。在Java语言中，大部分声称线程安全的类都属于这种类型，例如Vector。 线程兼容：指对象本身并不是线程安全的，但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全地使用。我们平常说一个类不是线程安全的，通常就是指这种情况。Java中的ArrayList就是这种情况。 线程对立：是指不管调用端是否采取了同步措施，都无法在多线程环境中并发使用代码。 线程安全的方法实现： 互斥同步：临界区（Critical Section）、互斥量（Mutex）和信号量（Semaphore）都是常见的互斥实现方式。在Java里面，互斥同步手段是synchronized关键字，这是一种块结构的同步语法。该关键字经过编译之后，会产生monitorenter和monitorexit这两个字节码指令。这两个字节码指令都需要一个reference类型的参数来指明要锁定和解锁的对象。如果Java源码中的synchronized明确指定了对象参数，那就以这个对象的引用作为reference；如果没有明确指定，那将根据synchronized修饰的方法类型（如实例方法或类方法），来决定是取代码所在的对象实例还是取类型对应的Class对象来作为线程要持有的锁。另外的话也有重入锁（ReentrantLock），相较于synchronized，重入锁提供：等待可中断，公平锁，锁绑定多个条件。 非阻塞同步：互斥同步面临的主要问题是进行线程阻塞和唤醒所带来的性能开销，因此这种同步也被称为阻塞同步。基于冲突检测的乐观并发策略，通俗地说就是不管风险，先进行操作，如果没有其他线程争用共享数据，那操作就直接成功了；如果共享的数据的确被争用，产生了冲突，那再进行其他的补偿措施，最常用的补偿措施是不断地重试，直到出现没有竞争的共享数据为止。这种乐观并发策略的实现不再需要把线程阻塞挂起，因此这种同步操作被称为非阻塞同步。这种方法需要硬件支持，因为我们必须要求操作和冲突检测这两个步骤具备原子性。如测试并设置（Test-and-Set）；获取并增加（Fetch-and-Increment）；交换（Swap）；比较并交换（Compare-and-Swap，下文称CAS）。 无同步方案：如果能让一个方法本来就不涉及共享数据，那它自然就不需要任何同步措施去保证其正确性，因此会有一些代码天生就是线程安全的。 锁优化： 自旋锁与自适应自旋：互斥同步对性能最大的影响是阻塞的实现，挂起线程和恢复线程的操作都需要转入内核态中完成，这些操作给Java虚拟机的并发性能带来了很大的压力。如果物理机器有一个以上的处理器或者处理器核心，能让两个或以上的线程同时并行执行，我们就可以让后面请求锁的那个线程“稍等一会”，但不放弃处理器的执行时间，看看持有锁的线程是否很快就会释放锁。为了让线程等待，我们只须让线程执行一个忙循环（自旋），这项技术就是所谓的自旋锁。自适应意味着自旋的时间不再是固定的了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定的。 锁消除：锁消除是指虚拟机即时编译器在运行时，对一些代码要求同步，但是对被检测到不可能存在共享数据竞争的锁进行消除。锁消除的主要判定依据来源于逃逸分析的数据支持，如果判断到一段代码中，在堆上的所有数据都不会逃逸出去被其他线程访问到，那就可以把它们当作栈上数据对待，认为它们是线程私有的，同步加锁自然就无须再进行。 锁粗化：原则上，总是推荐将同步块的作用范围限制得尽量小，这样是为了使得需要同步的操作数量尽可能变少，即使存在锁竞争，等待锁的线程也能尽可能快地拿到锁。但是如果一系列的连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体之中的，那即使没有线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗。因此可以进行锁粗化操作。 轻量级锁：HotSpot虚拟机对象头布局： 在代码即将进入同步块的时候，如果此同步对象没有被锁定（锁标志位为“01”状态），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝： 然后，虚拟机将使用CAS操作尝试把对象的Mark Word更新为指向Lock Record的指针。如果这个更新动作成功了，即代表该线程拥有了这个对象的锁，并且对象Mark Word的锁标志位将转变为“00”，表示此对象处于轻量级锁定状态。 偏向锁：它的目的是消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能。如果说轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS操作都不去做了。偏向锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁一直没有被其他的线程获取，则持有偏向锁的线程将永远不需要再进行同步。 在Java语言里面一个对象如果计算过哈希码，就应该一直保持该值不变，否则很多依赖对象哈希码的API都可能存在出错风险。因此，当一个对象已经计算过一致性哈希码后，它就再也无法进入偏向锁状态了；而当一个对象当前正处于偏向锁状态，又收到需要计算其一致性哈希码请求时，它的偏向状态会被立即撤销，并且锁会膨胀为重量级锁。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.zsstrike.tech/tags/Java/"}]},{"title":"设计模式","slug":"设计模式","date":"2020-11-07T02:52:47.000Z","updated":"2022-07-04T06:40:11.462Z","comments":true,"path":"2020/11/07/设计模式/","link":"","permalink":"http://blog.zsstrike.tech/2020/11/07/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。本文介绍设计模式。","text":"设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。本文介绍设计模式。 设计模式简介设计模式是一套被反复使用的、多数人知晓的、经过分类编目的、代码设计经验的总结。使用设计模式是为了重用代码、让代码更容易被他人理解、保证代码可靠性。 GoF：四位作者合称，他们提出的设计模式主要基于以下面向对象设计原则： 对接口编程而不是对实现编程。 优先使用对象组合而不是继承。 设计模式的用途：是开发人员的共同平台，代表着最佳的实践。 设计模式的类型：创建型模式，结构型模式，行为型模式。另外将介绍 J2EE 模式： 创建型模式：这些设计模式提供了一种在创建对象的同时隐藏创建逻辑的方式，而不是使用 new 运算符直接实例化对象。这使得程序在判断针对某个给定实例需要创建哪些对象时更加灵活。 工厂模式（Factory Pattern） 抽象工厂模式（Abstract Factory Pattern） 单例模式（Singleton Pattern） 建造者模式（Builder Pattern） 原型模式（Prototype Pattern） 结构型模式：这些设计模式关注类和对象的组合。继承的概念被用来组合接口和定义组合对象获得新功能的方式。 适配器模式（Adapter Pattern） 桥接模式（Bridge Pattern） 过滤器模式（Filter、Criteria Pattern） 组合模式（Composite Pattern） 装饰器模式（Decorator Pattern） 外观模式（Facade Pattern） 享元模式（Flyweight Pattern） 代理模式（Proxy Pattern） 行为型模式：这些设计模式特别关注对象之间的通信。 责任链模式（Chain of Responsibility Pattern） 命令模式（Command Pattern） 解释器模式（Interpreter Pattern） 迭代器模式（Iterator Pattern） 中介者模式（Mediator Pattern） 备忘录模式（Memento Pattern） 观察者模式（Observer Pattern） 状态模式（State Pattern） 空对象模式（Null Object Pattern） 策略模式（Strategy Pattern） 模板模式（Template Pattern） 访问者模式（Visitor Pattern） J2EE 模式：这些设计模式特别关注表示层。这些模式是由 Sun Java Center 鉴定的。 MVC 模式（MVC Pattern） 业务代表模式（Business Delegate Pattern） 组合实体模式（Composite Entity Pattern） 数据访问对象模式（Data Access Object Pattern） 前端控制器模式（Front Controller Pattern） 拦截过滤器模式（Intercepting Filter Pattern） 服务定位器模式（Service Locator Pattern） 传输对象模式（Transfer Object Pattern） 设计模式六大原则： 开闭原则：对扩展开放，对修改关闭，实现热插拔，提高扩展性 里氏代换原则：任何基类可以出现的地方，子类一定可以出现，实现抽象的规范，实现子父类互相替换 依赖倒转原则：针对接口编程，依赖于抽象而不依赖于具体 接口隔离原则：使用多个隔离的接口，比使用单个接口要好，降低类之间的耦合度 迪米特法则：一个实体应当尽量少地与其他实体之间发生相互作用，使得系统功能模块相对独立 合成复用原则：尽量使用合成&#x2F;聚合的方式，而不是使用继承 工厂模式介绍：工厂模式（Factory Pattern）是 Java 中最常用的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。 优点：1、一个调用者想创建一个对象，只要知道其名称就可以了。 2、扩展性高，如果想增加一个产品，只要扩展一个工厂类就可以。 3、屏蔽产品的具体实现，调用者只关心产品的接口。 缺点：每次增加一个产品时，都需要增加一个具体类和对象实现工厂，使得系统中类的个数成倍增加，在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。 使用场景： 1、日志记录器：记录可能记录到本地硬盘、系统事件、远程服务器等，用户可以选择记录日志到什么地方。 2、数据库访问，当用户不知道最后系统采用哪一类数据库，以及数据库可能有变化时。 3、设计一个连接服务器的框架，需要三个协议，”POP3”、”IMAP”、”HTTP”，可以把这三个作为产品类，共同实现一个接口。 实现： 1234567891011121314151617public class ShapeFactory &#123; //使用 getShape 方法获取形状类型的对象 public Shape getShape(String shapeType)&#123; if(shapeType == null)&#123; return null; &#125; if(shapeType.equalsIgnoreCase(&quot;CIRCLE&quot;))&#123; return new Circle(); &#125; else if(shapeType.equalsIgnoreCase(&quot;RECTANGLE&quot;))&#123; return new Rectangle(); &#125; else if(shapeType.equalsIgnoreCase(&quot;SQUARE&quot;))&#123; return new Square(); &#125; return null; &#125;&#125; 抽象工厂模式介绍：抽象工厂模式（Abstract Factory Pattern）是围绕一个超级工厂创建其他工厂。该超级工厂又称为其他工厂的工厂。在抽象工厂模式中，接口是负责创建一个相关对象的工厂，不需要显式指定它们的类。每个生成的工厂都能按照工厂模式提供对象。 优点：当一个产品族中的多个对象被设计成一起工作时，它能保证客户端始终只使用同一个产品族中的对象。 缺点：产品族扩展非常困难，要增加一个系列的某一产品，既要在抽象的 Creator 里加代码，又要在具体的里面加代码。 使用场景： 1、QQ 换皮肤，一整套一起换。 2、生成不同操作系统的程序。 实现： 12345678910public class FactoryProducer &#123; public static AbstractFactory getFactory(String choice)&#123; if(choice.equalsIgnoreCase(&quot;SHAPE&quot;))&#123; return new ShapeFactory(); &#125; else if(choice.equalsIgnoreCase(&quot;COLOR&quot;))&#123; return new ColorFactory(); &#125; return null; &#125;&#125; 单例模式介绍：单例模式（Singleton Pattern）是 Java 中最简单的设计模式之一。这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。 优点：1、在内存里只有一个实例，减少了内存的开销，尤其是频繁的创建和销毁实例（比如管理学院首页页面缓存）。2、避免对资源的多重占用（比如写文件操作）。 缺点：没有接口，不能继承，与单一职责原则冲突，一个类应该只关心内部逻辑，而不关心外面怎么样来实例化。 使用场景：1、要求生产唯一序列号。2、WEB 中的计数器，不用每次刷新都在数据库里加一次，用单例先缓存起来。3、创建的一个对象需要消耗的资源过多，比如 I&#x2F;O 与数据库的连接等。 实现： 懒汉式，线程不安全 1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 懒汉式，线程安全 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 饿汉式 1234567public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; 双重校验锁（DCL，即 double-checked locking）：采用双锁机制，安全且在多线程情况下能保持高性能。 1234567891011121314public class Singleton &#123; private volatile static Singleton singleton; private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; &#125; 静态内部类：能达到和双重校验锁一样的效果，但是实现更加简单 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 枚举：这种实现方式还没有被广泛采用，但这是实现单例模式的最佳方法。它更简洁，自动支持序列化机制，绝对防止多次实例化。 12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125; &#125; 建造者模式介绍：建造者模式（Builder Pattern）使用多个简单的对象一步一步构建成一个复杂的对象。一个 Builder 类会一步一步构造最终的对象。该 Builder 类是独立于其他对象的。 优点： 1、建造者独立，易扩展。 2、便于控制细节风险。 缺点： 1、产品必须有共同点，范围有限制。 2、如内部变化复杂，会有很多的建造类。 使用场景： 1、需要生成的对象具有复杂的内部结构。 2、需要生成的对象内部属性本身相互依赖。 实现： 12345678910111213141516public class MealBuilder &#123; public Meal prepareVegMeal ()&#123; Meal meal = new Meal(); meal.addItem(new VegBurger()); meal.addItem(new Coke()); return meal; &#125; public Meal prepareNonVegMeal ()&#123; Meal meal = new Meal(); meal.addItem(new ChickenBurger()); meal.addItem(new Pepsi()); return meal; &#125;&#125; 原型模式介绍：原型模式（Prototype Pattern）是用于创建重复的对象，同时又能保证性能。这种模式是实现了一个原型接口，该接口用于创建当前对象的克隆。当直接创建对象的代价比较大时，则采用这种模式。 优点： 1、性能提高。 2、逃避构造函数的约束。 缺点： 1、配备克隆方法需要对类的功能进行通盘考虑，这对于全新的类不是很难，但对于已有的类不一定很容易，特别当一个类引用不支持串行化的间接对象，或者引用含有循环结构的时候。 2、必须实现 Cloneable 接口。 使用场景： 1、资源优化场景。 2、类初始化需要消化非常多的资源，这个资源包括数据、硬件资源等。 3、性能和安全要求的场景。 4、通过 new 产生一个对象需要非常繁琐的数据准备或访问权限，则可以使用原型模式。 5、一个对象多个修改者的场景。 6、一个对象需要提供给其他对象访问，而且各个调用者可能都需要修改其值时，可以考虑使用原型模式拷贝多个对象供调用者使用。 7、在实际项目中，原型模式很少单独出现，一般是和工厂方法模式一起出现，通过 clone 的方法创建一个对象，然后由工厂方法提供给调用者。原型模式已经与 Java 浑然一体，大家可以随手拿来使用。 实现： 1234567891011121314151617181920212223242526272829import java.util.Hashtable; public class ShapeCache &#123; private static Hashtable&lt;String, Shape&gt; shapeMap = new Hashtable&lt;String, Shape&gt;(); public static Shape getShape(String shapeId) &#123; Shape cachedShape = shapeMap.get(shapeId); return (Shape) cachedShape.clone(); &#125; // 对每种形状都运行数据库查询，并创建该形状 // shapeMap.put(shapeKey, shape); // 例如，我们要添加三种形状 public static void loadCache() &#123; Circle circle = new Circle(); circle.setId(&quot;1&quot;); shapeMap.put(circle.getId(),circle); Square square = new Square(); square.setId(&quot;2&quot;); shapeMap.put(square.getId(),square); Rectangle rectangle = new Rectangle(); rectangle.setId(&quot;3&quot;); shapeMap.put(rectangle.getId(),rectangle); &#125;&#125; 1234567891011121314151617181920212223242526272829public abstract class Shape implements Cloneable &#123; private String id; protected String type; abstract void draw(); public String getType()&#123; return type; &#125; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public Object clone() &#123; // clone 方法实现 Object clone = null; try &#123; clone = super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return clone; &#125;&#125; 适配器模式介绍：适配器模式（Adapter Pattern）是作为两个不兼容的接口之间的桥梁。这种模式涉及到一个单一的类，该类负责加入独立的或不兼容的接口功能。举个真实的例子，读卡器是作为内存卡和笔记本之间的适配器。您将内存卡插入读卡器，再将读卡器插入笔记本，这样就可以通过笔记本来读取内存卡。 优点： 1、可以让任何两个没有关联的类一起运行。 2、提高了类的复用。 3、增加了类的透明度。 4、灵活性好。 缺点： 1、过多地使用适配器，会让系统非常零乱，不易整体进行把握。比如，明明看到调用的是 A 接口，其实内部被适配成了 B 接口的实现，一个系统如果太多出现这种情况，无异于一场灾难。因此如果不是很有必要，可以不使用适配器，而是直接对系统进行重构。 2.由于 JAVA 至多继承一个类，所以至多只能适配一个适配者类，而且目标类必须是抽象类。 使用场景：有动机地修改一个正常运行的系统的接口，这时应该考虑使用适配器模式。 实现： 123456789101112131415161718192021public class MediaAdapter implements MediaPlayer &#123; AdvancedMediaPlayer advancedMusicPlayer; public MediaAdapter(String audioType)&#123; if(audioType.equalsIgnoreCase(&quot;vlc&quot;) )&#123; advancedMusicPlayer = new VlcPlayer(); &#125; else if (audioType.equalsIgnoreCase(&quot;mp4&quot;))&#123; advancedMusicPlayer = new Mp4Player(); &#125; &#125; @Override public void play(String audioType, String fileName) &#123; if(audioType.equalsIgnoreCase(&quot;vlc&quot;))&#123; advancedMusicPlayer.playVlc(fileName); &#125;else if(audioType.equalsIgnoreCase(&quot;mp4&quot;))&#123; advancedMusicPlayer.playMp4(fileName); &#125; &#125;&#125; 12345678910111213141516171819202122public class AudioPlayer implements MediaPlayer &#123; MediaAdapter mediaAdapter; @Override public void play(String audioType, String fileName) &#123; //播放 mp3 音乐文件的内置支持 if(audioType.equalsIgnoreCase(&quot;mp3&quot;))&#123; System.out.println(&quot;Playing mp3 file. Name: &quot;+ fileName); &#125; //mediaAdapter 提供了播放其他文件格式的支持 else if(audioType.equalsIgnoreCase(&quot;vlc&quot;) || audioType.equalsIgnoreCase(&quot;mp4&quot;))&#123; mediaAdapter = new MediaAdapter(audioType); mediaAdapter.play(audioType, fileName); &#125; else&#123; System.out.println(&quot;Invalid media. &quot;+ audioType + &quot; format not supported&quot;); &#125; &#125; &#125; 桥接模式介绍：桥接（Bridge）是用于把抽象化与实现化解耦，使得二者可以独立变化。这种模式涉及到一个作为桥接的接口，使得实体类的功能独立于接口实现类。这两种类型的类可被结构化改变而互不影响。 优点： 1、抽象和实现的分离。 2、优秀的扩展能力。 3、实现细节对客户透明。 缺点：桥接模式的引入会增加系统的理解与设计难度，由于聚合关联关系建立在抽象层，要求开发者针对抽象进行设计与编程。 使用场景： 1、如果一个系统需要在构件的抽象化角色和具体化角色之间增加更多的灵活性，避免在两个层次之间建立静态的继承联系，通过桥接模式可以使它们在抽象层建立一个关联关系。 2、对于那些不希望使用继承或因为多层次继承导致系统类的个数急剧增加的系统，桥接模式尤为适用。 3、一个类存在两个独立变化的维度，且这两个维度都需要进行扩展。 实现： 1234567public abstract class Shape &#123; protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI)&#123; this.drawAPI = drawAPI; &#125; public abstract void draw(); &#125; 过滤器模式介绍：过滤器模式（Filter Pattern）或标准模式（Criteria Pattern）是一种设计模式，这种模式允许开发人员使用不同的标准来过滤一组对象，通过逻辑运算以解耦的方式把它们连接起来。 实现： 12345import java.util.List; public interface Criteria &#123; public List&lt;Person&gt; meetCriteria(List&lt;Person&gt; persons);&#125; 组合模式介绍：组合模式（Composite Pattern），又叫部分整体模式，是用于把一组相似的对象当作一个单一的对象。组合模式依据树形结构来组合对象，用来表示部分以及整体层次。这种模式创建了一个包含自己对象组的类。该类提供了修改相同对象组的方式。所谓组合模式，其实说的是对象包含对象的问题，通过组合的方式（在对象内部引用对象）来进行布局。 优点： 1、高层模块调用简单。 2、节点自由增加。 缺点：在使用组合模式时，其叶子和树枝的声明都是实现类，而不是接口，违反了依赖倒置原则。 使用场景：部分、整体场景，如树形菜单，文件、文件夹的管理。 实现： 1234567891011121314151617181920212223242526272829303132333435import java.util.ArrayList;import java.util.List; public class Employee &#123; private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; //构造函数 public Employee(String name,String dept, int sal) &#123; this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList&lt;Employee&gt;(); &#125; public void add(Employee e) &#123; subordinates.add(e); &#125; public void remove(Employee e) &#123; subordinates.remove(e); &#125; public List&lt;Employee&gt; getSubordinates()&#123; return subordinates; &#125; public String toString()&#123; return (&quot;Employee :[ Name : &quot;+ name +&quot;, dept : &quot;+ dept + &quot;, salary :&quot; + salary+&quot; ]&quot;); &#125; &#125; 装饰器模式介绍：装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种模式创建了一个装饰类，用来包装原有的类，并在保持类方法签名完整性的前提下，提供了额外的功能。 优点：装饰类和被装饰类可以独立发展，不会相互耦合，装饰模式是继承的一个替代模式，装饰模式可以动态扩展一个实现类的功能。 缺点：多层装饰比较复杂。 使用场景： 1、扩展一个类的功能。 2、动态增加功能，动态撤销。 实现： 1234567891011public abstract class ShapeDecorator implements Shape &#123; protected Shape decoratedShape; public ShapeDecorator(Shape decoratedShape)&#123; this.decoratedShape = decoratedShape; &#125; public void draw()&#123; decoratedShape.draw(); &#125; &#125; 12345678910111213141516public class RedShapeDecorator extends ShapeDecorator &#123; public RedShapeDecorator(Shape decoratedShape) &#123; super(decoratedShape); &#125; @Override public void draw() &#123; decoratedShape.draw(); setRedBorder(decoratedShape); &#125; private void setRedBorder(Shape decoratedShape)&#123; System.out.println(&quot;Border Color: Red&quot;); &#125;&#125; 外观模式介绍：外观模式（Facade Pattern）隐藏系统的复杂性，并向客户端提供了一个客户端可以访问系统的接口。这种模式涉及到一个单一的类，该类提供了客户端请求的简化方法和对现有系统类方法的委托调用。 优点： 1、减少系统相互依赖。 2、提高灵活性。 3、提高了安全性。 缺点：不符合开闭原则，如果要改东西很麻烦，继承重写都不合适。 使用场景： 1、为复杂的模块或子系统提供外界访问的模块。 2、子系统相对独立。 3、预防低水平人员带来的风险。 实现： 123456789101112131415161718192021public class ShapeMaker &#123; private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() &#123; circle = new Circle(); rectangle = new Rectangle(); square = new Square(); &#125; public void drawCircle()&#123; circle.draw(); &#125; public void drawRectangle()&#123; rectangle.draw(); &#125; public void drawSquare()&#123; square.draw(); &#125;&#125; 享元模式介绍：享元模式（Flyweight Pattern）主要用于减少创建对象的数量，以减少内存占用和提高性能。享元模式尝试重用现有的同类对象，如果未找到匹配的对象，则创建新对象。 优点：大大减少对象的创建，降低系统的内存，使效率提高。 缺点：提高了系统的复杂度，需要分离出外部状态和内部状态，而且外部状态具有固有化的性质，不应该随着内部状态的变化而变化，否则会造成系统的混乱。 使用场景： 1、系统有大量相似对象。 2、需要缓冲池的场景。 实现： 12345678910111213141516import java.util.HashMap; public class ShapeFactory &#123; private static final HashMap&lt;String, Shape&gt; circleMap = new HashMap&lt;&gt;(); public static Shape getCircle(String color) &#123; Circle circle = (Circle)circleMap.get(color); if(circle == null) &#123; circle = new Circle(color); circleMap.put(color, circle); System.out.println(&quot;Creating circle of color : &quot; + color); &#125; return circle; &#125;&#125; 代理模式介绍：在代理模式（Proxy Pattern）中，一个类代表另一个类的功能。这种类型的设计模式属于结构型模式。在代理模式中，我们创建具有现有对象的对象，以便向外界提供功能接口。 优点： 1、职责清晰。 2、高扩展性。 3、智能化。 缺点： 1、由于在客户端和真实主题之间增加了代理对象，因此有些类型的代理模式可能会造成请求的处理速度变慢。 2、实现代理模式需要额外的工作，有些代理模式的实现非常复杂。 使用场景：按职责来划分，通常有以下使用场景： 1、远程代理。 2、虚拟代理。 3、Copy-on-Write 代理。 4、保护（Protect or Access）代理。 5、Cache代理。 6、防火墙（Firewall）代理。 7、同步化（Synchronization）代理。 8、智能引用（Smart Reference）代理。 实现： 1234567891011121314151617public class ProxyImage implements Image&#123; private RealImage realImage; private String fileName; public ProxyImage(String fileName)&#123; this.fileName = fileName; &#125; @Override public void display() &#123; if(realImage == null)&#123; realImage = new RealImage(fileName); &#125; realImage.display(); &#125;&#125; 责任链模式介绍：责任链模式（Chain of Responsibility Pattern）为请求创建了一个接收者对象的链。这种模式给予请求的类型，对请求的发送者和接收者进行解耦。在这种模式中，通常每个接收者都包含对另一个接收者的引用。如果一个对象不能处理该请求，那么它会把相同的请求传给下一个接收者，依此类推。 优点： 1、降低耦合度。它将请求的发送者和接收者解耦。 2、简化了对象。使得对象不需要知道链的结构。 3、增强给对象指派职责的灵活性。通过改变链内的成员或者调动它们的次序，允许动态地新增或者删除责任。 4、增加新的请求处理类很方便。 缺点： 1、不能保证请求一定被接收。 2、系统性能将受到一定影响，而且在进行代码调试时不太方便，可能会造成循环调用。 3、可能不容易观察运行时的特征，有碍于除错。 使用场景： 1、有多个对象可以处理同一个请求，具体哪个对象处理该请求由运行时刻自动确定。 2、在不明确指定接收者的情况下，向多个对象中的一个提交一个请求。 3、可动态指定一组对象处理请求。 实现： 1234567891011121314151617181920212223242526public abstract class AbstractLogger &#123; public static int INFO = 1; public static int DEBUG = 2; public static int ERROR = 3; protected int level; //责任链中的下一个元素 protected AbstractLogger nextLogger; public void setNextLogger(AbstractLogger nextLogger)&#123; this.nextLogger = nextLogger; &#125; public void logMessage(int level, String message)&#123; if(this.level &lt;= level)&#123; write(message); &#125; if(nextLogger !=null)&#123; nextLogger.logMessage(level, message); // 向下层传播 &#125; &#125; abstract protected void write(String message); &#125; 命令模式介绍：命令模式（Command Pattern）是一种数据驱动的设计模式，它属于行为型模式。请求以命令的形式包裹在对象中，并传给调用对象。调用对象寻找可以处理该命令的合适的对象，并把该命令传给相应的对象，该对象执行命令。 优点： 1、降低了系统耦合度。 2、新的命令可以很容易添加到系统中去。 缺点：使用命令模式可能会导致某些系统有过多的具体命令类。 使用场景：认为是命令的地方都可以使用命令模式，比如： 1、GUI 中每一个按钮都是一条命令。 2、模拟 CMD。 实现： 1234567891011121314151617import java.util.ArrayList;import java.util.List; public class Broker &#123; private List&lt;Order&gt; orderList = new ArrayList&lt;Order&gt;(); public void takeOrder(Order order)&#123; orderList.add(order); &#125; public void placeOrders()&#123; for (Order order : orderList) &#123; order.execute(); &#125; orderList.clear(); &#125;&#125; 解释器模式介绍：解释器模式（Interpreter Pattern）提供了评估语言的语法或表达式的方式，它属于行为型模式。这种模式实现了一个表达式接口，该接口解释一个特定的上下文。这种模式被用在 SQL 解析、符号处理引擎等。 优点： 1、可扩展性比较好，灵活。 2、增加了新的解释表达式的方式。 3、易于实现简单文法。 缺点： 1、可利用场景比较少。 2、对于复杂的文法比较难维护。 3、解释器模式会引起类膨胀。 4、解释器模式采用递归调用方法。 使用场景： 1、可以将一个需要解释执行的语言中的句子表示为一个抽象语法树。 2、一些重复出现的问题可以用一种简单的语言来进行表达。 3、一个简单语法需要解释的场景。 实现： 123456789101112131415public class OrExpression implements Expression &#123; private Expression expr1 = null; private Expression expr2 = null; public OrExpression(Expression expr1, Expression expr2) &#123; this.expr1 = expr1; this.expr2 = expr2; &#125; @Override public boolean interpret(String context) &#123; return expr1.interpret(context) || expr2.interpret(context); &#125;&#125; 迭代器模式介绍：迭代器模式（Iterator Pattern）是 Java 和 .Net 编程环境中非常常用的设计模式。这种模式用于顺序访问集合对象的元素，不需要知道集合对象的底层表示。 优点： 1、它支持以不同的方式遍历一个聚合对象。 2、迭代器简化了聚合类。 3、在同一个聚合上可以有多个遍历。 4、在迭代器模式中，增加新的聚合类和迭代器类都很方便，无须修改原有代码。 缺点：由于迭代器模式将存储数据和遍历数据的职责分离，增加新的聚合类需要对应增加新的迭代器类，类的个数成对增加，这在一定程度上增加了系统的复杂性。 使用场景： 1、访问一个聚合对象的内容而无须暴露它的内部表示。 2、需要为聚合对象提供多种遍历方式。 3、为遍历不同的聚合结构提供一个统一的接口。 实现： 1234567891011121314151617181920212223242526272829public class NameRepository implements Container &#123; public String names[] = &#123;&quot;Robert&quot; , &quot;John&quot; ,&quot;Julie&quot; , &quot;Lora&quot;&#125;; @Override public Iterator getIterator() &#123; return new NameIterator(); &#125; private class NameIterator implements Iterator &#123; int index; @Override public boolean hasNext() &#123; if(index &lt; names.length)&#123; return true; &#125; return false; &#125; @Override public Object next() &#123; if(this.hasNext())&#123; return names[index++]; &#125; return null; &#125; &#125;&#125; 中介者模式介绍：中介者模式（Mediator Pattern）是用来降低多个对象和类之间的通信复杂性。这种模式提供了一个中介类，该类通常处理不同类之间的通信，并支持松耦合，使代码易于维护。 优点： 1、降低了类的复杂度，将一对多转化成了一对一。 2、各个类之间的解耦。 3、符合迪米特原则。 缺点：中介者会庞大，变得复杂难以维护。 使用场景： 1、系统中对象之间存在比较复杂的引用关系，导致它们之间的依赖关系结构混乱而且难以复用该对象。 2、想通过一个中间类来封装多个类中的行为，而又不想生成太多的子类。 实现： 12345678import java.util.Date; public class ChatRoom &#123; public static void showMessage(User user, String message)&#123; System.out.println(new Date().toString() + &quot; [&quot; + user.getName() +&quot;] : &quot; + message); &#125;&#125; 备忘录模式介绍：备忘录模式（Memento Pattern）保存一个对象的某个状态，以便在适当的时候恢复对象。 优点： 1、给用户提供了一种可以恢复状态的机制，可以使用户能够比较方便地回到某个历史的状态。 2、实现了信息的封装，使得用户不需要关心状态的保存细节。 缺点：消耗资源。如果类的成员变量过多，势必会占用比较大的资源，而且每一次保存都会消耗一定的内存。 使用场景： 1、需要保存&#x2F;恢复数据的相关状态场景。 2、提供一个可回滚的操作。 实现： 1234567891011121314import java.util.ArrayList;import java.util.List; public class CareTaker &#123; private List&lt;Memento&gt; mementoList = new ArrayList&lt;Memento&gt;(); public void add(Memento state)&#123; mementoList.add(state); &#125; public Memento get(int index)&#123; return mementoList.get(index); &#125;&#125; 观察者模式介绍：当对象间存在一对多关系时，则使用观察者模式（Observer Pattern）。比如，当一个对象被修改时，则会自动通知依赖它的对象。 优点： 1、观察者和被观察者是抽象耦合的。 2、建立一套触发机制。 缺点： 1、如果一个被观察者对象有很多的直接和间接的观察者的话，将所有的观察者都通知到会花费很多时间。 2、如果在观察者和观察目标之间有循环依赖的话，观察目标会触发它们之间进行循环调用，可能导致系统崩溃。 3、观察者模式没有相应的机制让观察者知道所观察的目标对象是怎么发生变化的，而仅仅只是知道观察目标发生了变化。 使用场景： 一个抽象模型有两个方面，其中一个方面依赖于另一个方面。将这些方面封装在独立的对象中使它们可以各自独立地改变和复用。 一个对象的改变将导致其他一个或多个对象也发生改变，而不知道具体有多少对象将发生改变，可以降低对象之间的耦合度。 一个对象必须通知其他对象，而并不知道这些对象是谁。 需要在系统中创建一个触发链，A对象的行为将影响B对象，B对象的行为将影响C对象……，可以使用观察者模式创建一种链式触发机制。 实现： 12345678910111213141516171819202122232425262728import java.util.ArrayList;import java.util.List; public class Subject &#123; private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); private int state; public int getState() &#123; return state; &#125; public void setState(int state) &#123; this.state = state; notifyAllObservers(); &#125; public void attach(Observer observer)&#123; observers.add(observer); &#125; public void notifyAllObservers()&#123; for (Observer observer : observers) &#123; observer.update(); &#125; &#125; &#125; 状态模式介绍：在状态模式（State Pattern）中，类的行为是基于它的状态改变的。在状态模式中，我们创建表示各种状态的对象和一个行为随着状态对象改变而改变的 context 对象。 优点： 1、封装了转换规则。 2、枚举可能的状态，在枚举状态之前需要确定状态种类。 3、将所有与某个状态有关的行为放到一个类中，并且可以方便地增加新的状态，只需要改变对象状态即可改变对象的行为。 4、允许状态转换逻辑与状态对象合成一体，而不是某一个巨大的条件语句块。 5、可以让多个环境对象共享一个状态对象，从而减少系统中对象的个数。 缺点： 1、状态模式的使用必然会增加系统类和对象的个数。 2、状态模式的结构与实现都较为复杂，如果使用不当将导致程序结构和代码的混乱。 3、状态模式对”开闭原则”的支持并不太好，对于可以切换状态的状态模式，增加新的状态类需要修改那些负责状态转换的源代码，否则无法切换到新增状态，而且修改某个状态类的行为也需修改对应类的源代码。 使用场景： 1、行为随状态改变而改变的场景。 2、条件、分支语句的代替者。 实现： 1234567891011public class StopState implements State &#123; public void doAction(Context context) &#123; System.out.println(&quot;Player is in stop state&quot;); context.setState(this); &#125; public String toString()&#123; return &quot;Stop State&quot;; &#125;&#125; 空对象模式介绍：在空对象模式（Null Object Pattern）中，一个空对象取代 NULL 对象实例的检查。Null 对象不是检查空值，而是反应一个不做任何动作的关系。这样的 Null 对象也可以在数据不可用的时候提供默认的行为。在空对象模式中，我们创建一个指定各种要执行的操作的抽象类和扩展该类的实体类，还创建一个未对该类做任何实现的空对象类，该空对象类将无缝地使用在需要检查空值的地方。 实现： 123456789101112public class NullCustomer extends AbstractCustomer &#123; @Override public String getName() &#123; return &quot;Not Available in Customer Database&quot;; &#125; @Override public boolean isNil() &#123; return true; &#125;&#125; 策略模式介绍：在策略模式（Strategy Pattern）中，一个类的行为或其算法可以在运行时更改。在策略模式中，我们创建表示各种策略的对象和一个行为随着策略对象改变而改变的 context 对象。策略对象改变 context 对象的执行算法。 优点： 1、算法可以自由切换。 2、避免使用多重条件判断。 3、扩展性良好。 缺点： 1、策略类会增多。 2、所有策略类都需要对外暴露。 使用场景： 1、如果在一个系统里面有许多类，它们之间的区别仅在于它们的行为，那么使用策略模式可以动态地让一个对象在许多行为中选择一种行为。 2、一个系统需要动态地在几种算法中选择一种。 3、如果一个对象有很多的行为，如果不用恰当的模式，这些行为就只好使用多重的条件选择语句来实现。 实现： 1234567891011public class Context &#123; private Strategy strategy; public Context(Strategy strategy)&#123; this.strategy = strategy; &#125; public int executeStrategy(int num1, int num2)&#123; return strategy.doOperation(num1, num2); &#125;&#125; 模板模式介绍：在模板模式（Template Pattern）中，一个抽象类公开定义了执行它的方法的方式&#x2F;模板。它的子类可以按需要重写方法实现，但调用将以抽象类中定义的方式进行。 优点： 1、封装不变部分，扩展可变部分。 2、提取公共代码，便于维护。 3、行为由父类控制，子类实现。 缺点：每一个不同的实现都需要一个子类来实现，导致类的个数增加，使得系统更加庞大。 使用场景： 1、有多个子类共有的方法，且逻辑相同。 2、重要的、复杂的方法，可以考虑作为模板方法。 实现： 123456789101112131415161718public abstract class Game &#123; abstract void initialize(); abstract void startPlay(); abstract void endPlay(); //模板 public final void play()&#123; //初始化游戏 initialize(); //开始游戏 startPlay(); //结束游戏 endPlay(); &#125;&#125; 访问者模式介绍：在访问者模式（Visitor Pattern）中，我们使用了一个访问者类，它改变了元素类的执行算法。通过这种方式，元素的执行算法可以随着访问者改变而改变。这种类型的设计模式属于行为型模式。根据模式，元素对象已接受访问者对象，这样访问者对象就可以处理元素对象上的操作。 优点： 1、符合单一职责原则。 2、优秀的扩展性。 3、灵活性。 缺点： 1、具体元素对访问者公布细节，违反了迪米特原则。 2、具体元素变更比较困难。 3、违反了依赖倒置原则，依赖了具体类，没有依赖抽象。 使用场景： 1、对象结构中对象对应的类很少改变，但经常需要在此对象结构上定义新的操作。 2、需要对一个对象结构中的对象进行很多不同的并且不相关的操作，而需要避免让这些操作”污染”这些对象的类，也不希望在增加新操作时修改这些类。 实现： 1234567public class Mouse implements ComputerPart &#123; @Override public void accept(ComputerPartVisitor computerPartVisitor) &#123; computerPartVisitor.visit(this); &#125;&#125; MVC模式介绍：MVC 模式代表 Model-View-Controller（模型-视图-控制器） 模式。这种模式用于应用程序的分层开发。 Model（模型） - 模型代表一个存取数据的对象或 JAVA POJO。它也可以带有逻辑，在数据变化时更新控制器。 View（视图） - 视图代表模型包含的数据的可视化。 Controller（控制器） - 控制器作用于模型和视图上。它控制数据流向模型对象，并在数据变化时更新视图。它使视图与模型分离开。 实现： 1234567891011121314151617181920212223242526272829public class StudentController &#123; private Student model; private StudentView view; public StudentController(Student model, StudentView view)&#123; this.model = model; this.view = view; &#125; public void setStudentName(String name)&#123; model.setName(name); &#125; public String getStudentName()&#123; return model.getName(); &#125; public void setStudentRollNo(String rollNo)&#123; model.setRollNo(rollNo); &#125; public String getStudentRollNo()&#123; return model.getRollNo(); &#125; public void updateView()&#123; view.printStudentDetails(model.getName(), model.getRollNo()); &#125; &#125; 业务代表模式介绍：业务代表模式（Business Delegate Pattern）用于对表示层和业务层解耦。它基本上是用来减少通信或对表示层代码中的业务层代码的远程查询功能。在业务层中我们有以下实体。 客户端（Client） - 表示层代码可以是 JSP、servlet 或 UI java 代码。 业务代表（Business Delegate） - 一个为客户端实体提供的入口类，它提供了对业务服务方法的访问。 查询服务（LookUp Service） - 查找服务对象负责获取相关的业务实现，并提供业务对象对业务代表对象的访问。 业务服务（Business Service） - 业务服务接口。实现了该业务服务的实体类，提供了实际的业务实现逻辑。 实现： 1234567891011121314public class BusinessDelegate &#123; private BusinessLookUp lookupService = new BusinessLookUp(); private BusinessService businessService; private String serviceType; public void setServiceType(String serviceType)&#123; this.serviceType = serviceType; &#125; public void doTask()&#123; businessService = lookupService.getBusinessService(serviceType); businessService.doProcessing(); &#125;&#125; 组合实体模式介绍：组合实体模式（Composite Entity Pattern）用在 EJB 持久化机制中。一个组合实体是一个 EJB 实体 bean，代表了对象的图解。当更新一个组合实体时，内部依赖对象 beans 会自动更新，因为它们是由 EJB 实体 bean 管理的。以下是组合实体 bean 的参与者。 组合实体（Composite Entity） - 它是主要的实体 bean。它可以是粗粒的，或者可以包含一个粗粒度对象，用于持续生命周期。 粗粒度对象（Coarse-Grained Object） - 该对象包含依赖对象。它有自己的生命周期，也能管理依赖对象的生命周期。 依赖对象（Dependent Object） - 依赖对象是一个持续生命周期依赖于粗粒度对象的对象。 策略（Strategies） - 策略表示如何实现组合实体。 实现： 123456789101112public class CompositeEntity &#123; private CoarseGrainedObject cgo = new CoarseGrainedObject(); public void setData(String data1, String data2)&#123; cgo.setData(data1, data2); &#125; public String[] getData()&#123; return cgo.getData(); &#125;&#125; 数据访问对象模式介绍：数据访问对象模式（Data Access Object Pattern）或 DAO 模式用于把低级的数据访问 API 或操作从高级的业务服务中分离出来。以下是数据访问对象模式的参与者。 数据访问对象接口（Data Access Object Interface） - 该接口定义了在一个模型对象上要执行的标准操作。 数据访问对象实体类（Data Access Object concrete class） - 该类实现了上述的接口。该类负责从数据源获取数据，数据源可以是数据库，也可以是 xml，或者是其他的存储机制。 模型对象&#x2F;数值对象（Model Object&#x2F;Value Object） - 该对象是简单的 POJO，包含了 get&#x2F;set 方法来存储通过使用 DAO 类检索到的数据。 实现： 12345678910111213141516171819202122232425262728293031323334353637383940import java.util.ArrayList;import java.util.List; public class StudentDaoImpl implements StudentDao &#123; //列表是当作一个数据库 List&lt;Student&gt; students; public StudentDaoImpl()&#123; students = new ArrayList&lt;Student&gt;(); Student student1 = new Student(&quot;Robert&quot;,0); Student student2 = new Student(&quot;John&quot;,1); students.add(student1); students.add(student2); &#125; @Override public void deleteStudent(Student student) &#123; students.remove(student.getRollNo()); System.out.println(&quot;Student: Roll No &quot; + student.getRollNo() +&quot;, deleted from database&quot;); &#125; //从数据库中检索学生名单 @Override public List&lt;Student&gt; getAllStudents() &#123; return students; &#125; @Override public Student getStudent(int rollNo) &#123; return students.get(rollNo); &#125; @Override public void updateStudent(Student student) &#123; students.get(student.getRollNo()).setName(student.getName()); System.out.println(&quot;Student: Roll No &quot; + student.getRollNo() +&quot;, updated in the database&quot;); &#125;&#125; 前端控制器模式介绍：前端控制器模式（Front Controller Pattern）是用来提供一个集中的请求处理机制，所有的请求都将由一个单一的处理程序处理。该处理程序可以做认证&#x2F;授权&#x2F;记录日志，或者跟踪请求，然后把请求传给相应的处理程序。以下是这种设计模式的实体。 前端控制器（Front Controller） - 处理应用程序所有类型请求的单个处理程序，应用程序可以是基于 web 的应用程序，也可以是基于桌面的应用程序。 调度器（Dispatcher） - 前端控制器可能使用一个调度器对象来调度请求到相应的具体处理程序。 视图（View） - 视图是为请求而创建的对象。 实现： 1234567891011121314151617181920212223242526public class FrontController &#123; private Dispatcher dispatcher; public FrontController()&#123; dispatcher = new Dispatcher(); &#125; private boolean isAuthenticUser()&#123; System.out.println(&quot;User is authenticated successfully.&quot;); return true; &#125; private void trackRequest(String request)&#123; System.out.println(&quot;Page requested: &quot; + request); &#125; public void dispatchRequest(String request)&#123; //记录每一个请求 trackRequest(request); //对用户进行身份验证 if(isAuthenticUser())&#123; dispatcher.dispatch(request); &#125; &#125; 拦截过滤器模式介绍：拦截过滤器模式（Intercepting Filter Pattern）用于对应用程序的请求或响应做一些预处理&#x2F;后处理。定义过滤器，并在把请求传给实际目标应用程序之前应用在请求上。过滤器可以做认证&#x2F;授权&#x2F;记录日志，或者跟踪请求，然后把请求传给相应的处理程序。以下是这种设计模式的实体。 过滤器（Filter） - 过滤器在请求处理程序执行请求之前或之后，执行某些任务。 过滤器链（Filter Chain） - 过滤器链带有多个过滤器，并在 Target 上按照定义的顺序执行这些过滤器。 Target - Target 对象是请求处理程序。 过滤管理器（Filter Manager） - 过滤管理器管理过滤器和过滤器链。 客户端（Client） - Client 是向 Target 对象发送请求的对象。 实现： 123456789101112131415public class FilterManager &#123; FilterChain filterChain; public FilterManager(Target target)&#123; filterChain = new FilterChain(); filterChain.setTarget(target); &#125; public void setFilter(Filter filter)&#123; filterChain.addFilter(filter); &#125; public void filterRequest(String request)&#123; filterChain.execute(request); &#125;&#125; 服务定位器模式介绍：服务定位器模式（Service Locator Pattern）用在我们想使用 JNDI 查询定位各种服务的时候。考虑到为某个服务查找 JNDI 的代价很高，服务定位器模式充分利用了缓存技术。在首次请求某个服务时，服务定位器在 JNDI 中查找服务，并缓存该服务对象。当再次请求相同的服务时，服务定位器会在它的缓存中查找，这样可以在很大程度上提高应用程序的性能。以下是这种设计模式的实体。 服务（Service） - 实际处理请求的服务。对这种服务的引用可以在 JNDI 服务器中查找到。 Context &#x2F; 初始的 Context - JNDI Context 带有对要查找的服务的引用。 服务定位器（Service Locator） - 服务定位器是通过 JNDI 查找和缓存服务来获取服务的单点接触。 缓存（Cache） - 缓存存储服务的引用，以便复用它们。 客户端（Client） - Client 是通过 ServiceLocator 调用服务的对象。 实现： 123456789101112131415161718192021222324252627282930313233import java.util.ArrayList;import java.util.List; public class Cache &#123; private List&lt;Service&gt; services; public Cache()&#123; services = new ArrayList&lt;Service&gt;(); &#125; public Service getService(String serviceName)&#123; for (Service service : services) &#123; if(service.getName().equalsIgnoreCase(serviceName))&#123; System.out.println(&quot;Returning cached &quot;+serviceName+&quot; object&quot;); return service; &#125; &#125; return null; &#125; public void addService(Service newService)&#123; boolean exists = false; for (Service service : services) &#123; if(service.getName().equalsIgnoreCase(newService.getName()))&#123; exists = true; &#125; &#125; if(!exists)&#123; services.add(newService); &#125; &#125;&#125; 123456789101112131415161718192021public class ServiceLocator &#123; private static Cache cache; static &#123; cache = new Cache(); &#125; public static Service getService(String jndiName)&#123; Service service = cache.getService(jndiName); if(service != null)&#123; return service; &#125; InitialContext context = new InitialContext(); Service service1 = (Service)context.lookup(jndiName); cache.addService(service1); return service1; &#125;&#125; 传输对象模式介绍：传输对象模式（Transfer Object Pattern）用于从客户端向服务器一次性传递带有多个属性的数据。传输对象也被称为数值对象。传输对象是一个具有 getter&#x2F;setter 方法的简单的 POJO 类，它是可序列化的，所以它可以通过网络传输。它没有任何的行为。服务器端的业务类通常从数据库读取数据，然后填充 POJO，并把它发送到客户端或按值传递它。对于客户端，传输对象是只读的。客户端可以创建自己的传输对象，并把它传递给服务器，以便一次性更新数据库中的数值。以下是这种设计模式的实体。 业务对象（Business Object） - 为传输对象填充数据的业务服务。 传输对象（Transfer Object） - 简单的 POJO，只有设置&#x2F;获取属性的方法。 客户端（Client） - 客户端可以发送请求或者发送传输对象到业务对象。 实现： 123456789101112131415161718192021222324252627282930313233343536import java.util.ArrayList;import java.util.List; public class StudentBO &#123; //列表是当作一个数据库 List&lt;StudentVO&gt; students; public StudentBO()&#123; students = new ArrayList&lt;StudentVO&gt;(); StudentVO student1 = new StudentVO(&quot;Robert&quot;,0); StudentVO student2 = new StudentVO(&quot;John&quot;,1); students.add(student1); students.add(student2); &#125; public void deleteStudent(StudentVO student) &#123; students.remove(student.getRollNo()); System.out.println(&quot;Student: Roll No &quot; + student.getRollNo() +&quot;, deleted from database&quot;); &#125; //从数据库中检索学生名单 public List&lt;StudentVO&gt; getAllStudents() &#123; return students; &#125; public StudentVO getStudent(int rollNo) &#123; return students.get(rollNo); &#125; public void updateStudent(StudentVO student) &#123; students.get(student.getRollNo()).setName(student.getName()); System.out.println(&quot;Student: Roll No &quot; + student.getRollNo() +&quot;, updated in the database&quot;); &#125;&#125;","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.zsstrike.tech/tags/Java/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.zsstrike.tech/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"OnJava8笔记","slug":"OnJava8笔记","date":"2020-10-25T15:15:42.000Z","updated":"2022-08-26T07:45:54.456Z","comments":true,"path":"2020/10/25/OnJava8笔记/","link":"","permalink":"http://blog.zsstrike.tech/2020/10/25/OnJava8%E7%AC%94%E8%AE%B0/","excerpt":"本文主要整理了 OnJava8 的阅读笔记。","text":"本文主要整理了 OnJava8 的阅读笔记。 第三章 万物皆对象对象操纵：在 Java 中程序员实际操作的是对象的引用，方法参数中传递的也只是对象的引用。 对象创建：new。 数据存储： 寄存器：Java 中不存在该方式。 栈内存：存放一些 Java 数据，比如对象的引用。 堆内存：Java 对象都存于其中。 常量内存：程序代码中，不会改变。 非 RAM 存储：序列化对象（用于传送）和持久化对象（用于恢复）。 基本类型的存储：不是通过 new 创建，变量直接存储值。有 boolean，byte，short，char，int，float，long，double，void。boolean 类型的大小没有明确规定。 高精度数值：BigInteger 和 BigDecimal。 数组的存储：当创建对象数组的时候，实际上是对象引用的数组，初始化为 null。 代码注释：/* ... */ 和 //。 对象清理： 作用域：&#123;&#125; 决定，不允许父作用域和子作用域声明相同的变量。 对象作用域：使用 new 关键字创建的 Java 对象生命周期超出作用域。 类的创建： 类型：class 字段：类里面声明的变量 方法：类里面定义的函数 基本类型的默认值：全 0，但是不适用于局部变量。 方法签名：方法名和参数列表统称为方法签名。 程序编写： 命名可见性：反向使用自己的网络域名，但是存在空文件夹 使用其他组件：import static 关键字：类变量和类方法声明，在没有对象时候也可以进行调用，另外类变量在所有的对象中共享 第四章 运算符赋值：=，基本类型的赋值都是直接的，而不像对象，赋予的只是其内存的引用。在方法的参数中传递一个对象，在方法体里面对其进行修改，那么在该对象在外部也会被修改。 算术运算符：+，-，*，/，%，其中+, -可以作为一元运算符。 递增和递减：++,--。前缀递增和递减立即修改变量的值，后缀则是使用变量的值，然后再修改。 关系运算符：&gt;, &gt;=, &lt;, &lt;=, ==, !=。判断基本类型的时候使用==，判断对象的使用 equals 方法，判断对象时使用 == 比较的只是引用。 逻辑运算符：&amp;&amp;, ||, !。Java 支持短路。 字面量常量：0x, 0, 0b, L, F, F 可以默认不写。 下划线：用于分割数字字面量。 指数计数法：e。 位运算符：&amp;, |, ^, ~。 移位运算符：&lt;&lt;, &gt;&gt;, &gt;&gt;&gt;。注意 &gt;&gt; 是算术右移，&gt;&gt;&gt; 是逻辑右移（首位添0）。 三目运算符：&lt;boolean condition&gt; ? &lt;value1&gt; : &lt;value2&gt;。 字符串运算符：+。 类型转换：向上转换是安全的，向下转换需要显式说明(type)。对于浮点数向整数转换，小数总是被截断。 Java 没有 sizeof 运算符，因为每种类型的值都是固定的。 第五章 控制流true 和 false：所有关系运算符都能产生条件语句，注意在 Java 中使用数值作为布尔值是非法的。 条件控制：if-else。 迭代语句：while，do-while，for，for-in。 return：退出当前的方法，放回一个方法值。 break 和 continue：break 用于中止内层循环，continue 用于跳过此次迭代。 goto：Java 不支持 goto 语句，但是支持标签语法，可以和 break 和 continue 一起使用。 switch-case：每个 case 后面跟上 break，同时在 Java7 的时候开始支持字符串匹配。 第六章 初始化和清理使用构造器保证初始化：构造器名称和类名相同，每次创建一个对象的时候，自动调用构造器进行初始化。构造器并没有返回值。 方法重载：每个被重载的方法需要具有一个独一无二的参数列表。返回值并不能用来区分重载的方法。 无参构造器：一个无参构造器就是不接受参数的构造器，如果没有显式提供任何构造器，那么编译器会自动提供一个无参构造器。 this：this 关键字只能在非静态的方法内部使用，等同于当前方法所属的对象引用。 在构造器中调用构造器：通过 this(param list) 实现。注意只能通过 this 调用一次构造器，不可重复多次调用构造器。并且，只能在构造器首行进行调用。 static 的含义：static 修饰的方法中不存在 this。静态方法和静态变量是为了类而创建的。 垃圾回收器：在 Java 中，对象并非总是被垃圾回收： 对象可能不被垃圾回收。 垃圾回收不等同于析构。 垃圾回收只和内存有关。 在 Java 中，虽然提供了一个 finialize() 的方法用于清理对象，但是事实上我们并不需要过多使用该方法。记住，无论是”垃圾回收”还是”终结”，都不保证一定会发生。如果 Java 虚拟机（JVM）并未面临内存耗尽的情形，它可能不会浪费时间执行垃圾回收以恢复内存。 垃圾回收器如何工作： 引用计数：每个对象有一个引用计数器，每次有引用指向该对象的时候，引用计数加 1。当引用离开作用域或者被置为 null 的时候，引用计数减 1。垃圾回收器会遍历含有全部对象的列表，当发现某个对象的引用计数为 0 时，就释放其占用的空间（但是，引用计数模式经常会在计数为 0 时立即释放对象）。这个机制存在一个缺点：如果对象之间存在循环引用，那么它们的引用计数都不为 0，就会出现应该被回收但无法被回收的情况。 自适应的垃圾回收技术：对于任意“活”的对象，总是可以追溯到其存活在栈或者静态区的引用，从栈或者静态存储区出发，将会发现所有的活的对象。至于如何处理找到的存活对象，取决于不同的 Java 虚拟机实现。其中有一种做法叫做停止-复制（stop-and-copy）。顾名思义，这需要先暂停程序的运行（不属于后台回收模式），然后将所有存活的对象从当前堆复制到另一个堆，没有复制的就是需要被垃圾回收的。另外，当对象被复制到新堆时，它们是一个挨着一个紧凑排列，然后就可以按照前面描述的那样简单、直接地分配新空间了。上述方法存在缺点：需要两个堆，然后再两个堆之间折腾，得维护比实际空间多一倍的空间；另外在于复制本身，一旦程序进入稳定状态之后，可能只会产生少量垃圾，甚至没有垃圾。尽管如此，复制回收器仍然会将所有内存从一处复制到另一处，这很浪费。为了避免这种状况，一些 Java 虚拟机会进行检查：要是没有新垃圾产生，就会转换到另一种模式（即”自适应”）。这种模式称为标记-清扫（mark-and-sweep）。对一般用途而言，”标记-清扫”方式速度相当慢，但是当你知道程序只会产生少量垃圾甚至不产生垃圾时，它的速度就很快了。”标记-清扫”所依据的思路仍然是从栈和静态存储区出发，遍历所有的引用，找出所有存活的对象。但是，每当找到一个存活对象，就给对象设一个标记，并不回收它。只有当标记过程完成后，清理动作才开始。在清理过程中，没有标记的对象将被释放，不会发生任何复制动作。”标记-清扫”后剩下的堆空间是不连续的，垃圾回收器要是希望得到连续空间的话，就需要重新整理剩下的对象。 成员初始化：在方法中的变量没有默认值，需要手动指定一个值之后才能使用；在类中的变量则会赋予默认值。 初始化的顺序：假设有个名为 Dog 的类： 即使没有显式地使用 static 关键字，构造器实际上也是静态方法。所以，当首次创建 Dog 类型的对象或是首次访问 Dog 类的静态方法或属性时，Java 解释器必须在类路径中查找，以定位 Dog.class。 当加载完 Dog.class 后（后面会学到，这将创建一个 Class 对象），有关静态初始化的所有动作都会执行。因此，静态初始化只会在首次加载 Class 对象时初始化一次。 当用 new Dog() 创建对象时，首先会在堆上为 Dog 对象分配足够的存储空间。 分配的存储空间首先会被清零，即会将 Dog 对象中的所有基本类型数据设置为默认值（数字会被置为 0，布尔型和字符型也相同），引用被置为 null。 执行所有出现在字段定义处的初始化动作。 执行构造器。你将会在”复用”这一章看到，这可能会牵涉到很多动作，尤其当涉及继承的时候。 显式的静态初始化：static &#123; statements； &#125;，与其他静态初始化动作一样，这段代码仅执行一次：当首次创建这个类的对象或首次访问这个类的静态成员（甚至不需要创建该类的对象）时。 实例初始化：&#123; statements; &#125;，实例初始化子句是在构造器之前执行的。 数组初始化：Type[] arg = new Type[length]，Type[] arg = &#123;value1, value2,,,&#125; 可变参数列表：void method(int t, char... args) 枚举类型：enum Type &#123;&#125; 第七章 封装包的概念：包内包含有一组类，它们被组织在一个单独的命名空间下。对于单文件的程序，该文件在默认包（default package）下。另外，每个 Java 源文件只能有一个 public 类。 代码组织：为了将功能相近的 Java 源文件组织到一起，可以使用关键字 package。该关键字必须是文件中除了注释的第一行代码。当需要使用到某个包中的类时，可以使用 import 关键字。 独一无二的包名：通常选择反转的域名。 冲突：当两个包下面含有相同的类时，就会出现名称冲突的问题，可以将特定的类写全名称，比如java.util.ArrayList。 使用包的注意事项：当创建一个包的时候，包名实际上就隐含了目录结构。 访问权限修饰符：Java 访问权限控制符 public，protected，private 位于定义的类名，属性名和方法名前。 public：当使用 public 关键字的时候，意味着 public 后声明的成员对于每个人都是可用的。 default：指不加修饰符定义的成员，可以被相同包下的文件访问。 private：除了包含成员的类，其他任何类都无法访问这个成员。 protected：继承的类可以访问父类中对应的成员，同时也提供了包访问权限。但是需要注意： 基类的 protected 成员是包内可见的，并且对子类可见 若子类与基类不在同一包中，那么在子类中，子类实例可以访问其从基类继承而来的 protected 方法，而不能访问基类实例的 protected 方法 类访问权限：类既不能是 private，也不能是 protected 的，只能使用 public 或者是 包访问权限。 第八章 复用复用方式： 组合：在新类里面创建现有类的对象 继承：创建现有类型的子类 委托：介于继承和组合之间，将一个成员对象放在正在构建的类中，但同时又在新的类中公开来自成员对象的所有方法（Java 中不直接支持） 组合语法：将对象的引用放在一个新的类里面，就算是使用了组合。 继承语法：使用 extends 关键字。继承后，可以在方法里面使用 super 关键字来使用父类的方法。 子类的初始化：当某个派生类被实例化的时候，会递归向上调用父类的构造器，最高层级的父类的构造器首先被执行，然后是最高层级下的子类，，，一直到该派生类构造器。 带参数的构造器：当没有无参的基类构造器，只含有有参的基类构造器，此时就需要通过 super 手动调用基类的构造器。 组合和继承的选择：当想要在新类中包含一个已有类的功能时，使用组合，而非继承；当使用继承时，使用一个现有类并开发出它的新版本，通常这意味着使用一个通用类，并为了某个特殊需求将其特殊化。组合用来表达“有一个”的关系，而继承则是“是一个”的关系。 向上转型：派生类到基类的转型称之为向上转型，向上转型是安全的，因为子类必定包含了所有父类的方法。 final 关键字：final 修饰的数据通常指该数据不能被改变： final 数据：对于基本类型，final 使得数值恒定不变，对于对象引用，final 则是使得引用恒定不变。空白 final 是指没有初始化值的 final 属性，编译器保证在使用空白 final 之前必须被初始化，此时必须在构造器中对 final 变量进行赋值。 final 参数：在参数列表中，将参数声明为 final 意味着在方法中不能改变参数指向的对象或基本变量。 final 方法：给方法上锁，防止子类通过覆写改变方法的行为。类中所有的 private 方法都隐式地指定为 final。 final 类：当说一个类是 final ，就意味着它不能被继承。 类初始化和加载：在 Java 中，每个类的编译代码都存在于它自己独立的文件中，该文件只有在使用程序代码时才会被加载。一般可以说“类的代码在首次使用时加载”。这通常是指创建类的第一个对象，或者是访问了类的 static 属性或方法。构造器也是一个 static 方法尽管它的 static 关键字是隐式的。因此，准确地说，一个类当它任意一个 static 成员被访问时，就会被加载。 第九章 多态向上转型：当使用向上转型的时候，我们可以将所有派生类当做是基类来看待，提高了程序的可拓展性。 方法调用绑定：当派生类重写了基类的方法时，我们使用向上转型后，调用这些被重写的方法时，编译器会动态绑定到派生类中被重写的方法，执行方法调用。Java 中除了 static 和 final 方法（private 方法也是隐式的 final）外，其他所有方法都是后期绑定。 陷阱： 试图重写私有方法 只有普通的方法调用是多态的，属性并不能多态（访问属性的时候看的是类型，也就是说向上转型后，只能直接访问基类中的属性） 构造器和多态： 构造器调用顺序：首先是基类构造器被调用，然后按照顺序初始化成员，接着调用派生类构造器的方法体 继承和清理：在清理工作的时候，应该先释放派生类的对象，然后释放基类的对象 构造器内部多态方法的行为：如果在构造器中调用了正在构造的对象的动态绑定方法，就会用到那个方法的重写定义 协变返回类型：派生类的被重写方法可以返回基类方法返回类型的派生类型。 向下转型：重新将基类类型改为派生类类型，是不安全的。 第十章 接口接口和抽象类提供了一种将接口与实现分离的更加结构化的方法，抽象类是一种介于普通类和接口之间的折中手段。 抽象类和方法：抽象方法只有声明没有方法体，并且使用 abstract 关键字，包含有抽象方法的类称为抽象类，并且类本身也必须限定为抽象。抽象类不能被实例化，如果某个类继承自抽象类，就必须实现该抽象类中所有的抽象方法，如果不这么做的话，新的类也是一个抽象类。 接口创建：使用 interface 关键字创接口。一个接口表示，所有实现了该接口的类看起来都这样。在 Java8 之前，接口里面只允许抽象方法（不用加 abstract 关键字），在 Java8 里面又新增了默认方法。另外，接口同样可以包含属性，这些属性被隐式指明为 static 和 final。使用 implements 关键字使一个类遵循某个特定接口（或一组接口），它表示：接口只是外形，现在我要说明它是如何工作的。最后，接口中的方法是 public 权限的。 默认方法：当实现了某个接口的类没有实现某个方法的时候，此时可以使用接口的默认方法，使用 default 关键字，可以带有方法体。增加默认方法的极具说服力的理由是它允许在不破坏已使用接口的代码的情况下，在接口中增加新的方法。 多继承：Java 中只支持单继承，但是 Java 通过默认方法具有某种多继承的特性，结合带有默认方法的接口意味着结合了多个基类中的行为。因为接口中仍然不允许存在属性（只有静态属性，不适用），所以属性仍然只会来自单个基类或抽象类，也就是说，不会存在状态的多继承。 接口中的静态方法：Java8 中允许在接口中添加静态方法，这么做能恰当地把工具功能置于接口中，从而操作接口，或者成为通用的工具。 抽象类和接口： 特性 接口 抽象类 组合 新类可以组合多个接口 只能继承单一抽象类 状态 不能包含属性（除了静态属性，不支持对象状态） 可以包含属性，非抽象方法可能引用这些属性 默认方法和抽象方法 不需要在子类中实现默认方法。默认方法可以引用其他接口的方法 必须在子类中实现抽象方法 构造器 没有构造器 可以有构造器 可见性 隐式 public 可以是 protected 或友元 完全解耦：使用接口更有利于实现完全解耦，使得代码更具有可复用性。 多接口实现：一个类只能继承自一个父类，同时可以实现多个接口，提高类的灵活度。 使用继承扩展接口：通过继承，可以很容易在接口中增加方法声明，还可以在新的接口中实现多个接口。注意，通常来说，extends 只能用于单一类，但是接口可以多继承。 实现接口时的命名冲突：覆写、实现和重载令人不快地搅和在一起带来了困难，当打算组合接口时，在不同的接口中使用相同的方法名通常会造成代码可读性的混乱，尽量避免这种情况。 接口适配：接口最吸引人的原因之一是相同的接口可以有多个实现。在简单情况下体现在一个方法接受接口作为参数，该接口的实现和传递对象则取决于方法的使用者。 接口字段：因为接口中的字段都自动是 static 和 final 的，所以接口就成为了创建一组常量的方便的工具。但是在 Java8 中，应尽量使用 enum 关键字来定义枚举变量。 接口嵌套：接口可以嵌套在类或者是其他接口中。 第十一章 内部类内部类创建：将类的定义放在外部类的里面。如果想从外部类的非静态方法之外的任意位置创建某个内部类的对象，那么必须具体地指明这个对象的类型：OuterClassName.InnerClassName。 链接外部类：当生成一个内部类的对象的时候，该对象能够访问到外部对象的所有成员，而不需要其他任何特殊权限。当某个外部类的对象创建了一个内部类对象时，此内部类对象必定会秘密地捕获一个指向那个外部类对象的引用。然后，在你访问此外部类的成员时，就是用那个引用来选择外部类的成员。但是这些都是编译器的细节了。 使用 .this 和 .new：如果你需要生成对外部类对象的引用，可以使用外部类的名字后面紧跟圆点和 this。有时你可能想要告知某些其他对象，去创建其某个内部类的对象。要实现此目的，你必须在 new 表达式中提供对其他外部类对象的引用，这是需要使用 .new 语法。 12345678910// innerclasses/DotNew.java// Creating an inner class directly using .new syntaxpublic class DotNew &#123; public class Inner &#123;&#125; public static void main(String[] args) &#123; DotNew dn = new DotNew(); DotNew.Inner dni = dn.new Inner(); &#125;&#125; 内部类和向上转型：当将内部类向上转型为其基类，尤其是转型为一个接口的时候，内部类就有了用武之地。这是因为此内部类-某个接口的实现-能够完全不可见，并且不可用。所得到的只是指向基类或接口的引用，所以能够很方便地隐藏实现细节。 在方法和作用域中声明内部类：可以在一个方法里面或者在任意的作用域内定义内部类。 匿名内部类：通常使用 new ClassName(params) &#123; ... &#125;;，params 用于构造器传参，后面的分号指代语句结束。另外，如果匿名类内部希望使用一个定义在其外部的对象，那么编译器要求其参数引用必须是 final 的。注意在实例化匿名类的时候，可以使用非 final 修饰的变量。匿名内部类与正规的继承相比有些受限，因为匿名内部类既可以扩展类，也可以实现接口，但是不能两者兼备。而且如果是实现接口，也只能实现一个接口。 嵌套类：如果不需要内部类对象与其外部类对象之间有联系，那么可以将内部类声明为 static，这通常称为嵌套类。想要理解 static 应用于内部类时的含义，就必须记住，普通的内部类对象隐式地保存了一个引用，指向创建它的外部类对象。然而，当内部类是 static 的时，就不是这样了。嵌套类意味着： 要创建嵌套类的对象，并不需要其外部类的对象。 不能从嵌套类的对象中访问非静态的外部类对象。 嵌套类与普通的内部类还有一个区别。普通内部类的字段与方法，只能放在类的外部层次上，所以普通的内部类不能有 static 数据和 static 字段，也不能包含嵌套类。但是嵌套类可以包含所有这些东西。 接口内部的类：嵌套类可以作为接口的一部分。你放到接口中的任何类都自动地是 public 和 static 的。 从多层嵌套类中访问外部类的成员：一个内部类被嵌套多少层并不重要——它能透明地访问所有它所嵌入的外部类的所有成员。 为什么需要内部类： 闭包和回调：在 Java8 之前，内部类是实现闭包的唯一方式，在 Java8 中，我们可以使用 lambda 表达式来实现闭包行为，并且更加优雅。 继承内部类：因为内部类的构造器必须连接到指向其外部类对象的引用，所以在继承内部类的时候，事情会变得有点复杂。问题在于，那个指向外部类对象的“秘密的”引用必须被初始化，而在派生类中不再存在可连接的默认对象。 12345678910111213141516// innerclasses/InheritInner.java// Inheriting an inner classclass WithInner &#123; class Inner &#123;&#125;&#125;public class InheritInner extends WithInner.Inner &#123; //- InheritInner() &#123;&#125; // Won&#x27;t compile InheritInner(WithInner wi) &#123; wi.super(); &#125; public static void main(String[] args) &#123; WithInner wi = new WithInner(); InheritInner ii = new InheritInner(wi); &#125;&#125; 内部类标示符：由于编译后每个类都会产生一个 .class 文件，其中包含了如何创建该类型的对象的全部信息。内部类也必须生成一个 .class 文件以包含它们的 Class 对象信息。这些类文件的命名有严格的规则：外部类的名字，加上 “$” ，再加上内部类的名字。如果内部类是匿名的，编译器会简单地产生一个数字作为其标识符。 第十二章 集合泛型和类型安全的集合：通过使用泛型，规定了向某个集合中可以添加的变量类型，方便进行处理，同时不会引发类型转型错误等问题。 Java 集合类库的两个概念：集合（Collection）和映射（Map）。 添加元素组：通过 Arrays.asList 和 Collections.addAll 方法来添加元素组。注意 Arrays.asList 的返回值是一个 List，但是这个 List 不能调整大小。 集合的打印：必须使用 Arrays.toString 来生成数组的可打印形式，但是打印集合无需任何操作。 列表 List：有 ArrayList 和 LinkedList，前者擅长随机访问，后者擅长插入删除操作。当确定元素是否是属于某个 List ，寻找某个元素的索引，以及通过引用从 List 中删除元素时，都会用到 equals() 方法。toArray() 方法将任意的 Collection 转换为数组。 迭代器 Iterators：在任何集合中，都必须有某种方式可以插入元素并再次获取它们。毕竟，保存事物是集合最基本的工作。对于 List ， add() 是插入元素的一种方式， get() 是获取元素的一种方式。如果从更高层次的角度考虑，会发现这里有个缺点：要使用集合，必须对集合的确切类型编程。为此引入迭代器，迭代器相关方法有iterator，next，hasNext，remove。迭代器统一了对集合的访问方式。 ListIterator：ListIterator 是一个更强大的 Iterator 子类型，它只能由各种 List 类生成。 Iterator 只能向前移动，而 ListIterator 可以双向移动。它可以生成迭代器在列表中指向位置的后一个和前一个元素的索引，并且可以使用 set() 方法替换它访问过的最近一个元素。 LinkedList：LinkedList 还添加了一些方法，使其可以被用作栈、队列或双端队列（deque） 。在这些方法中，有些彼此之间可能只是名称有些差异，或者只存在些许差异，以使得这些名字在特定用法的上下文环境中更加适用（特别是在 Queue 中）。如 element，peek，poll，offer 等。 栈 Stack：后进先出规则，Java 1.0 中附带了一个 Stack 类，结果设计得很糟糕（为了向后兼容，我们永远坚持 Java 中的旧设计错误）。Java 6 添加了 ArrayDeque ，其中包含直接实现堆栈功能的方法。 集合 Set：Set 不保存重复的元素，Set 具有与 Collection 相同的接口，因此没有任何额外的功能。HashSet 产生的输出没有可辨别的顺序，这是因为出于对速度的追求， HashSet 使用了散列。由 HashSet 维护的顺序与 TreeSet 或 LinkedHashSet 不同，因为它们的实现具有不同的元素存储方式。TreeSet 将元素存储在红-黑树数据结构中，而 HashSet 使用散列函数。LinkedHashSet 因为查询速度的原因也使用了散列，但是看起来使用了链表来维护元素的插入顺序。 映射 Map：根据键快速查找值的结构。Map 可以返回由其键组成的 Set ，由其值组成的 Collection ，或者其键值对的 Set 。keySet() 方法生成由在 petPeople 中的所有键组成的 Set ，它在 for-in 语句中被用来遍历该 Map 。 队列 Queue：先进先出的集合，LinkedList 实现了 Queue 接口，并且提供了一些方法以支持队列行为，因此 LinkedList 可以用作 Queue 的一种实现。offer() 是与 Queue 相关的方法之一，它在允许的情况下，在队列的尾部插入一个元素，或者返回 false 。 peek() 和 element() 都返回队头元素而不删除它，但是如果队列为空，则 element() 抛出 NoSuchElementException ，而 peek() 返回 null 。 poll() 和 remove() 都删除并返回队头元素，但如果队列为空，poll() 返回 null ，而 remove() 抛出 NoSuchElementException 。 优先级队列 PriorityQueue：先进先出（FIFO）描述了最典型的队列规则（queuing discipline）。优先级队列声明下一个弹出的元素是最需要的元素（具有最高的优先级）。当在 PriorityQueue 上调用 offer() 方法来插入一个对象时，该对象会在队列中被排序。默认的排序使用队列中对象的自然顺序（natural order），但是可以通过提供自己的 Comparator 来修改这个顺序。 PriorityQueue 确保在调用 peek()， poll() 或 remove() 方法时，获得的元素将是队列中优先级最高的元素。 集合和迭代器：Collection 是所有序列集合共有的根接口，使用接口描述的一个理由是它可以使我们创建更通用的代码。通过针对接口而非具体实现来编写代码，我们的代码可以应用于更多类型的对象。为了对集合进行遍历操作，我们可以使用迭代器来进行操作。 for-in 迭代器：到目前为止，for-in 语法主要用于数组，但它也适用于任何 Collection 对象。这样做的原因是 Java 5 引入了一个名为 Iterable 的接口，该接口包含一个能够生成 Iterator 的 iterator() 方法。for-in 使用此 Iterable 接口来遍历序列。 适配器惯用法：如果已经有一个接口并且需要另一个接口时，则编写适配器就可以解决这个问题。在这里，若希望在默认的正向迭代器的基础上，添加产生反向迭代器的能力，因此不能使用覆盖，相反，而是添加了一个能够生成 Iterable 对象的方法，该对象可以用于 for-in 语句。 注意：不要在新代码中使用遗留类 Vector ，Hashtable 和 Stack 。 Java 集合框架简图：黄色为接口，绿色为抽象类，蓝色为具体类。虚线箭头表示实现关系，实线箭头表示继承关系。 第十三章 函数式编程Lambda 表达式：(params) -&gt; &#123; statements; &#125;，只有一个参数的时候，可以省略括号，如果只有一行的话，花括号应该省略。 方法引用：ClassName::MethodName。 未绑定的方法引用：未绑定的方法引用是指没有关联对象的普通（非静态）方法。 使用未绑定的引用时，我们必须先提供对象。 构造函数的引用：ClassName::new 函数式接口：Lambda 表达式包含类型推导，但是如果存在(x, y) -&gt; x + y这样的 lambda 表达式，编译器就不能自动进行类型推导了。因为 x, y 既可以是 String 类型，也可以是 int 类型。此时引入java.util.function包，包含了一组接口，每个接口只有一个抽象方法，称为函数式方法。Java 8 允许我们将函数赋值给接口，这样的语法更加简单漂亮。 多参数函数式接口：在 function 包中，只有很少的接口，我们可以自己定义一个函数接口，如下： 1234567// functional/TriFunction.java@FunctionalInterfacepublic interface TriFunction&lt;T, U, V, R&gt; &#123; R apply(T t, U u, V v);&#125; 高阶函数：消费或产生函数的函数。 12345678910111213141516// functional/ProduceFunction.javaimport java.util.function.*;interface FuncSS extends Function&lt;String, String&gt; &#123;&#125; // [1]public class ProduceFunction &#123; static FuncSS produce() &#123; return s -&gt; s.toLowerCase(); // [2] &#125; public static void main(String[] args) &#123; FuncSS f = produce(); System.out.println(f.apply(&quot;YELLING&quot;)); &#125;&#125; 闭包：对外部变量引用的函数。 1234567891011// functional/Closure1.javaimport java.util.function.*;public class Closure1 &#123; int i; IntSupplier makeFun(int x) &#123; return () -&gt; x + i++; &#125;&#125; 柯里化和部分求值：柯里化意为：将一个多参数的函数，转换为一系列单参数函数。 第十四章 流式编程流式编程的特点：代码可读性更高；懒加载，意味着它只在绝对必要时才计算，由于计算延迟，流使我们能够表示非常大（甚至无限）的序列，而不需要考虑内存问题。 流支持：Java 8 通过在接口中添加default修饰的方法实现流的平滑嵌入。流操作有三种类型：创建流，修改流元素（中间操作），消费流元素（终端操作）。 流创建：通过 Stream.of 将一组元素转化为流，除此之外，每个集合都可以通过调用 stream 方法来产生一个流。除此以外，还有： 随机数流：new Random().ints() int 类型流：IntStream.range(start, end, step) generate：Stream.generate(obj) iterate：Stream.iterate(initValue, cb) 流的构造者模式：首先创建一个 builder 对象，然后将创建流所需的多个信息传递给它，最后builder 对象执行“创建”流的操作。 Arrays: Arrays.stream() 中间操作：用于从一个流中获取对象，并将对象作为另一个流从后端输出，以连接到其他操作。 peek：无修改地查看流中的元素 sorted：排序，可以使用 lambda 参数 distinct：消除重复元素 filter：通过过滤条件的被保存下来，否则删除 map：将函数操作应用在输入流的元素中，并将返回值传递到输出流中。还有 mapToInt, mapToLong 等 flatMap：将产生流的函数应用在每个元素上（与 map() 所做的相同），然后将每个流都扁平化为元素，因而最终产生的仅仅是元素。对应还有 flatMapToInt 等 Optimal 类：一些标准流操作返回 Optional 对象，因为它们并不能保证预期结果一定存在。当流为空的时候你会获得一个 Optional.empty 对象，而不是抛出异常。 解包 Optimal 的函数：ifPresent(Consumer), orElse(otherObject) 创建 Optimal：静态方法有empty(), of(value), ofNullable(value) Optimal 流：假设你的生成器可能产生 null 值，那么当用它来创建流时，你会自然地想到用 Optional 来包装元素。可以使用 filter() 来保留那些非空 Optional 终端操作：以下操作将会获取流的最终结果，终端操作（Terminal Operations）总是我们在流管道中所做的最后一件事。 数组：toArray 循环：forEach，forEachOrdered 集合：collect 组合：reduce 匹配：allMatch，anyMatch，noneMatch 查找：findFirst，findAny 信息：count，max，min 数字流信息：average，max，min 第十五章 异常异常概念：C 以及其他早期语言常常具有多种错误处理模式，这些模式往往建立在约定俗成的基础之上，而并不属于语言的一部分。通常会返回某个特殊值或者设置某个标志，并且假定接收者将对这个返回值或标志进行检查，以判定是否发生了错误。“异常”这个词有“我对此感到意外”的意思。问题出现了，你也许不清楚该如何处理，但你的确知道不应该置之不理，你要停下来，看看是不是有别人或在别的地方，能够处理这个问题。 异常捕获： try 语句块：对可能产生异常的语句进行捕获 catch 语句块：对每种可能出现的异常准备相应的处理语句 终止和恢复：Java 支持终止模型，这种模型假设错误非常严重，以至于程序无法返回到异常发生的地方继续执行。另外一种是恢复模型，如果 Java 想要实现类似恢复的行为，可以把 try 块放在循环里面，直到得到满意的结果 自定义异常：想要自定义异常类，必须从已有的异常类继承。 异常声明：在方法后面加上throws ExceptionType1，ExceptionType2,,, 捕获所有异常：直接捕获基类 Exception 多重捕获：Java7 中支持，使用 | 连接不同类型的异常，如catch(Except1 | Except2 | Except3 | Except4 e) &#123;&#125; 栈轨迹：printStackTrace 重新抛出异常： 1234catch(Exception e) &#123; System.out.println(&quot;An exception was thrown&quot;); throw e;&#125; 使用 finally 进行清理：不管是否发生异常，都会执行 finally 块里面的语句。 finally 作用：用于将资源恢复到初始状态 在 return 中使用 finally：从何处返回无关紧要，finally 子句永远会执行 异常丢失：finally 里面使用 return 将会导致不会抛出任何异常 Try-With-Resources 用法：try()&#123; &#125;catch(Exception e)&#123; &#125;，在 try 小括号里面的声明的对象需要实现java.lang.AutoCloseable接口，接口只有一个方法close()。退出 try 块会调用每个对象的 close() 方法，并以与创建顺序相反的顺序关闭它们。 异常匹配：异常处理系统会按照代码的书写顺序找出“最近”的处理程序。找到匹配的处理程序之后，它就认为异常将得到处理，然后就不再继续查找。 第十七章 文件文件和目录路径：一个 Path 对象表示一个文件或者目录的路径，是一个跨操作系统（OS）和文件系统的抽象，通过 Paths.get(URL) 来获得一个对应的 Path 对象。 选取路径部分片段：getName 路径分析：Files 工具类包含一系列完整的方法用于获得 Path 相关的信息，如exists，size，isDirectory Paths 的增删修改：relative，resolve 遍历目录：Files.walkFileTree，具体操作实现由参数二 FileVisitor 里面的四个抽象方法决定： preVisitDirectory()：在访问目录中条目之前在目录上运行。 visitFile()：运行目录中的每一个文件。 visitFileFailed()：调用无法访问的文件。 postVisitDirectory()：在访问目录中条目之后在目录上运行，包括所有的子目录。 文件系统：可以使用静态的 FileSystems 工具类获取默认的文件系统 路径监听：通过文件系统的 WatchService 可以设置一个进程对目录中的更改做出响应 文件查找：通过在 FileSystem 对象上调用 getPathMatcher 可以获得一个 PathMatcher，传入对应的两种模式：glob 或者 regex。 文件读写：如果文件比较小，使用 Files.readAllLines 可以一次性读取整个文件，返回一个 List&lt;String&gt;；使用 Files.write 写入 byte 数组或者任何可迭代对象。对于文件比较大的时候，可以使用 Files.lines 将文件转换为输入流。 第十八章 字符串字符串的不可变性：String 对象是不可变的，String 类中的每个看起来会修改 String 值的方法，实际上都是创建了一个全新的 String 对象。 + 的重载与 StringBuilder：在 String 中，+ 代表了字符串之间的 append 操作，每次 + 都会创建一个 String 对象，代价高昂。StringBuilder 提供了丰富全面的方法，包括insert，replace，append，delete。另外还有 StringBuffer，和 StringBuilder 不同点在于后者是线程不安全的，前者是线程安全的，从性能上看，可以优先使用 StringBuilder。 意外递归：如&quot;som string&quot; + this ，我们想要打印出某个字符串的地址，但是编译器首先辨别出左边是 String 对象，+ 要求右边的变量也是 String 对象（先进行转换），这就涉及到了意外递归。可以使用Object.toString()打印地址。 字符串操作：当需要改变字符串的内容时，String 类的方法都会返回一个新的 String 对象。同时，如果内容不改变，String 方法只是返回原始对象的一个引用而已。这可以节约存储空间以及避免额外的开销。 格式化输出： System.out.printf, System.out.format 格式化修饰符：%[argument_index$][flags][width][.precision]conversion Formatter 转换： 类型 含义 d 整型（十进制） c Unicode字符 b Boolean值 s String f 浮点数（十进制） e 浮点数（科学计数） x 整型（十六进制） h 散列码（十六进制） % 字面值“%” String.format 正则化表达式： 在正则表达式中，用 \\d 表示一位数字。如果在其他语言中使用过正则表达式，那你可能就能发现 Java 对反斜线 \\ 的不同处理方式。在其他语言中，\\\\ 表示“我想要在正则表达式中插入一个普通的（字面上的）反斜线，请不要给它任何特殊的意义。”而在Java中，\\\\ 的意思是“我要插入一个正则表达式的反斜线，所以其后的字符具有特殊的意义。”例如，如果你想表示一位数字，那么正则表达式应该是 \\\\d。如果你想插入一个普通的反斜线，应该这样写 \\\\\\。不过换行符和制表符之类的东西只需要使用单反斜线：\\n\\t。如果要表示“可能有一个负号，后面跟着一位或多位数字”，可以这样： 1-?\\\\d+ 表达式： 表达式 含义 . 任意字符 [abc] 包含a、b或c的任何字符（和&#96;a [^abc] 除a、b和c之外的任何字符（否定） [a-zA-Z] 从a到z或从A到Z的任何字符（范围） [abc[hij]] a、b、c、h、i、j中的任意字符（与&#96;a [a-z&amp;&amp;[hij]] 任意h、i或j（交） \\s 空白符（空格、tab、换行、换页、回车） \\S 非空白符（[^\\s]） \\d 数字（[0-9]） \\D 非数字（[^0-9]） \\w 词字符（[a-zA-Z_0-9]） \\W 非词字符（[^\\w]） CharSequence：接口从 CharBuffer，String，StringBuffer，StringBuilder 抽象出了一般化定义 1234567interface CharSequence &#123; char charAt(int i); int length(); CharSequence subSequence(int start, int end); String toString(); &#125; Pattern 和 Matcher：根据一个 String 对象生成一个 Pattern 对象，通过 Pattern 对象的 match 方法产生一个 Matcher 对象。 组（group）：A(B(C))D 中有三个组：组 0 是 ABCD，组 1 是 BC，组 2 是 C。通过 Matcher 对象的 group 方法可以获取到每个组。 第十九章 类型信息Java 在运行时识别对象和类的信息的方式：传统的 RTTI(RunTime Type Information，运行时类型信息)，反射机制。 RTTI 必要性：下面这个代码展示了 Shape 基类下的派生类的相关操作，使用 RTTI，我们可以在运行时进行类型确认，同时，编码的时候只需要注意对基类的处理就行，不会影响代码的可扩展性。 1234567public class Shapes &#123; public static void main(String[] args) &#123; Stream.of( new Circle(), new Square(), new Triangle()) .forEach(Shape::draw); &#125;&#125; 实际上，上述代码编译时候，Stream 和 Java 泛型系统确保放入的都是 Shape 对象或者其派生类，运行时，自动类型转换确保从 Stream 中取出的对象都是 Shape 类型。 Class 对象：Class 对象包含了与类有关的信息，每个类都会产生一个 Class 对象，每当编译一个新类，就会产生一个 Class 对象（保存在同名的 .class 文件中），为了生成该类的对象，JVM 首先会调用类加载器子系统将这个类加载到内存中。Java 是动态加载的，即只有在类需要的时候才会进行类的加载。所有的 Class 对象都属于 Class 类，可以通过Class.forName()来得到类的 Class 对象，或者通过someInstance.getClass()得到。 类字面常量：对于一个 FancyToy.class 的文件，我们可以直接使用FancyToy.class得到对应的类对象，相较于Class.forName的方式，该种方式更加简单和安全，并且效率更高。另外，使用类字面常量的时候，不会自动初始化该 Class 对象。为了使用类的三个步骤： 加载：查找字节码，并且创建一个 Class 对象 链接：验证字节码，为 static 字段分配存储空间，如果需要，将解析这个类对其他类的引用 初始化：先初始化基类，然后执行 static 初始化器和 static 初始化块 泛化的 Class 引用：Class 引用总是指向某个 Class 对象，而 Class 对象可以用于产生类的实例，并且包含可作用于这些实例的所有方法代码。使用 Class&lt;?&gt; 表示通配所有类型，Class&lt;? extends Sup&gt;表示通配所有 Sup 的派生类型，Class&lt;? super Sub&gt;表示通配 Sub 的基类。 cast 方法：接受参数对象，并将其类型转换为 Class 引用的类型。 类型转换检测：Java 中支持自动向上转型，但是向下转型是强制的，需要用户指代向下转换的类型，如果没有通过向下类型转换，就会报错，否则转型成功。另外，可以使用 instanceof 判断某个实例是否是某个对象的实例。Class.isInstance 可以动态测试对象类型。 类的等价比较：当查询类型信息的时候，使用 instanceof 或者 isInstance，这两种方式产生的结果相同，但是 Class 对象直接比较与上述方式不同。instanceof 说的是“你是这个类，还是从这个类派生的类？”。而如果使用 &#x3D;&#x3D; 比较实际的Class 对象，则与继承无关 —— 它要么是确切的类型，要么不是。 反射：运行时类信息。java.lang.reflect库中包含了相关的类来实现反射这一机制。RTTI 和反射的真正区别在于，使用 RTTI 时，编译器在编译时会打开并检查 .class 文件。换句话说，你可以用“正常”的方式调用一个对象的所有方法；而通过反射，.class 文件在编译时不可用，它由运行时环境打开并检查。 类方法提取器：getMethods 和 getConstructors 获取对应的类方法 动态代理：代理是基本的设计模式之一。一个对象封装真实对象，代替其提供其他或不同的操作—这些操作通常涉及到与“真实”对象的通信，因此代理通常充当中间对象。通过调用静态方法Proxy.newProxyInstance来创建动态代理。 接口和类型：interface 关键字的一个重要目标就是允许程序员隔离组件，进而降低耦合度。 第二十章 泛型简单泛型：直接使用类型暂代符表示某种类型即可，下图是一个二元组： 12345678// onjava/Tuple2.javapackage onjava;public class Tuple2&lt;A, B&gt; &#123; public final A a1; public final B a2; public Tuple2(A a, B b) &#123; a1 = a; a2 = b; &#125;&#125; 泛型接口：泛型可以应用于接口，例如生成器，这是一种专门负责创建对象的类。注意，Java 中支持基本类型作为泛型类型。 泛型方法：泛型方法独立于类而改变方法。作为准则，请“尽可能”使用泛型方法。通常将单个方法泛型化要比将整个类泛型化更清晰易懂。 1234567// generics/GenericMethods.javapublic class GenericMethods &#123; public &lt;T&gt; void f(T x) &#123; System.out.println(x.getClass().getName()); &#125;&#125; 泛型擦除：Java 泛型是使用擦除实现的。这意味着当你在使用泛型时，任何具体的类型信息都被擦除了，你唯一知道的就是你在使用一个对象。因此，List&lt;String&gt; 和 List&lt;Integer&gt; 在运行时实际上是相同的类型。它们都被擦除成原生类型 List，使用 getClass 得到的结果相同。 特殊方式：当我们想要调用某个泛型类型的方法的时候，我们可以使用&lt;T extends Sup&gt;，这样我们才能调用 Sup 里面的相关方法。 擦除的问题：由于擦除的存在，所有关于参数的信息就丢失了。当你在编写泛型代码时，必须时刻提醒自己，你只是看起来拥有有关参数的类型信息而已。 补偿擦除：由于擦除的存在，我们无法在运行时知道参数的确切类型，为了解决这个问题，我们显式传递一个 Class 对象，以在类型表达式中使用。 创建类型的实例：直接通过new T()是行不通的，但是我们可以通过对应的 Class 对象的 newInstance 方法来创建新的实例 泛型数组：我们无法创建泛型数组，解决方式是在试图创建泛型数组的时候使用 ArrayList 边界：由于擦除会删除类型信息，因此唯一可用的无限制泛型参数的方法是那些 Object 可用的方法。但是，如果将该参数限制为某类型的子集，则可以调用该子集中的方法。为了应用约束，Java 泛型使用了 extends 关键字。 通配符：使用？表示。使用&lt;? extends Sup&gt;表示继承自 Sup 的类，使用&lt;? super Sub&gt;表示 Sub 的基类，使用&lt;?&gt;表示任意一种类型。 使用泛型的问题： 任何基本类型都不能作为类型参数 一个类不能实现同一个泛型接口的两种变体，由于擦除的原因，这两个变体会成为相同的接口 使用带有泛型类型参数的转型或 instanceof 不会有任何效果 自限定的类型：下图代码展示了这种惯用法： 1class SelfBounded&lt;T extends SelfBounded&lt;T&gt;&gt; &#123; // ... 古怪的循环泛型（CRG）： 123456// generics/CuriouslyRecurringGeneric.javaclass GenericType&lt;T&gt; &#123;&#125;public class CuriouslyRecurringGeneric extends GenericType&lt;CuriouslyRecurringGeneric&gt; &#123;&#125; 自限定： 1class A extends SelfBounded&lt;A&gt;&#123;&#125; 参数协变：自限定类型的价值在于它们可以产生协变参数类型，即方法参数类型会随子类而变化。 动态类型安全：Java5 中的 Collections 有一组便利工具函数，可以解决类型检查问题，如checkedMap等。 泛型异常：由于擦除的原因，catch 语句不能捕获泛型类型的异常，因为在编译期和运行时都必须知道异常的确切类型。 混型（Mixin）：最基本的概念是混合多个类的能力，以产生一个可以表示混型中所有类型的类。 与接口混合 使用装饰器模式 与动态代理混合 潜在类型机制：也称作鸭子类型机制，即“如果它走起来像鸭子，并且叫起来也像鸭子，那么你就可以将它当作鸭子对待”。 第二十一章 数组数组特性：效率，类型，保存基本数据类型的能力。 一等对象：不管使用什么类型的数组，数组中的数据集实际上都是对堆中真正对象的引用。注意区分聚合初始化和动态聚合初始化。 返回数组：在 Java 中，可以直接返回一个数组，而不用担心其内存消耗情况，垃圾回收期会自动处理。 多维数组：使用多层方括号界定每个维度的大小，同样的也存在有不规则的数组。可以使用 Arrays.deepToString 来查看多维数组里面的内容。Arrays.setAll 方法用于初始化数组。 泛型数组：数组和泛型并不能很好的结合，不能实例化参数化类型的数组，但是允许您创建对此类数组的引用。 Arrays 相关方法： fill：将单个值复制到整个数组，或者在对象数组的情况下，将相同的引用复制到整个数组 setAll：使用一个生成器用于生成不同的值，生成器的参数是 int 数组索引 asList：将数组转换为列表 copyOf：以新的长度复制数组 copyOfRange：复制现有数组的一部分数据 equals：判断数组是否相同 deepEquals：多维数组相等性比较 stream：生成流 sort：排序 binarySearch：二分查找 toString，deepToString：数组的字符串表示 数组元素修改：通过使用 setAll 方法来索引现有数据元素 第二十二章 枚举基本 enum 特性：调用 values 方法，可以返回对应的数组，同时调用 ordinal 方法可以知道某个 value 的次序，这个次序默认从 0 开始。可以使用 import static 导入 enum 类型。 方法添加：除了不能继承自一个 enum 之外，我们基本上可以将 enum 看作一个常规的类。如果你打算定义自己的方法，那么必须在 enum 实例序列的最后添加一个分号。 switch 语句中的 enum：enum 的 values 本来就具有顺序，可以搭配 switch 使用。 values 方法：enum 类型的对象会有一个 values 方法，这个方法是由编译器添加的 static 方法。 实现而非继承：enum 继承自 Enum 类，由于 Java 不支持多继承，enum 不能再次继承其他类，但是创建一个新的 enum 时，可以实现一个或者多个接口。 使用接口组织枚举：无法从 enum 继承子类很令人沮丧，但是我们可以尝试使用接口来组织枚举类。 使用 EnumSet 代替 Flags：EnumSet 中的元素必须来自一个 enum。EnumSet 基础是 long，只有 64 位，但是在需要的时候，会增加一个 long。 使用 EnumMap：要求键必须来自一个 enum，EnumMap 在内部使用数组实现。 使用 enum 的状态机：枚举类型很适合用于创建状态机。 第二十三章 注解java.lang 中的注解包括：@Override，@Deprecated，@SuppressWarnings，@SafeVarargs，@FunctionalInterface。 基本语法： 定义注解：注解的定义看起来很像接口的定义，事实上，他们和其他 Java 接口一样，也会被编译成 class 文件。 1234567// onjava/atunit/Test.java// The @Test tagpackage onjava.atunit;import java.lang.annotation.*;@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface Test &#123;&#125; @target 标示注解的对象，@Retention 标示注解在哪里可用。 元注解：@target，@Retention，@Documented，@Inherited，@Repeatable 编写注解处理器：使用反射机制 API 实现注解的读取。 注解元素：基本类型，String，Class，enum，Annotation，以上类型的数组 默认值限制：首先，元素不能有不确定的值。也就是说，元素要么有默认值，要么就在使用注解时提供元素的值。任何非基本类型的元素，无论是在源代码声明时还是在注解接口中定义默认值时，都不能使用 null 作为其值。 生成外部文件：Web Service，自定义标签库以及对象&#x2F;关系映射工具（例如 Toplink 和 Hibernate）通常都需要 XML 描述文件，而这些文件脱离于代码之外。除了定义 Java 类，程序员还必须忍受沉闷，重复的提供某些信息，例如类名和包名等已经在原始类中提供过的信息。每当你使用外部描述文件时，他就拥有了一个类的两个独立信息源，这经常导致代码的同步问题。 注解不支持继承：不能使用 extends 关键字来继承 @interfaces。 实现处理器：通过 getAnnotation 来检查是否存在注解，如果存在的话做出相应的操作 基于注解的单元测试：如 JUnit。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.zsstrike.tech/tags/Java/"}]},{"title":"hexo和typora搭配写博客","slug":"hexo和typora搭配写博客","date":"2020-10-21T06:17:54.000Z","updated":"2022-05-16T07:41:46.120Z","comments":true,"path":"2020/10/21/hexo和typora搭配写博客/","link":"","permalink":"http://blog.zsstrike.tech/2020/10/21/hexo%E5%92%8Ctypora%E6%90%AD%E9%85%8D%E5%86%99%E5%8D%9A%E5%AE%A2/","excerpt":"本文介绍使用Typora写博客，使用Hexo发布文章的技巧。主要涉及图片的路径问题。","text":"本文介绍使用Typora写博客，使用Hexo发布文章的技巧。主要涉及图片的路径问题。 解决Hexo图片路径问题在使用Typora的时候，首先进入到Typora的设置里面，将图片插入格式改为如下设置： 这样的话，在写作的时候，我们就可以实时预览到自己插入的图片了。 接着，在博客仓库的_config.yml中设置post_asset_folder: true。 但是这样设置的话会产生一个问题，就是在执行hexo g的时候，得到的博客文章路径会多一个&#123;&#123;title&#125;&#125;,导致图片在发布的时候渲染不出来。为了解决这个问题，可以使用hexo-typora-img， 1npm i hexo-typora-img 这个插件会将原来的路径在渲染前将其改为Hexo可以识别的图片路径，从而在预览发布的时候也可以看到图片。","categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://blog.zsstrike.tech/tags/Hexo/"},{"name":"Typora","slug":"Typora","permalink":"http://blog.zsstrike.tech/tags/Typora/"}]},{"title":"Redis 设计与实现笔记","slug":"Redis设计与实现笔记","date":"2020-10-18T15:14:52.000Z","updated":"2022-12-06T02:39:02.940Z","comments":true,"path":"2020/10/18/Redis设计与实现笔记/","link":"","permalink":"http://blog.zsstrike.tech/2020/10/18/Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E7%AC%94%E8%AE%B0/","excerpt":"本文章是对《Redis 设计与实现》书籍的一个整理笔记，记录了其中个人认为比较重要的部分。","text":"本文章是对《Redis 设计与实现》书籍的一个整理笔记，记录了其中个人认为比较重要的部分。 第二章 简单动态字符串 SDS 定义： SDS 与 C 字符串的区别： 常数复杂度获取字符串的长度 杜绝缓冲区溢出 减少修改字符串时带来的内存分配的次数，包括空间预分配和惰性空间释放 二进制安全 兼容部分 C 字符串函数 第二章 链表 Redis 的链表实现的特性可以总结如下： 双端 无环 带表头指针和表尾指针 带链表长度计数器 多态：链表节点使用 void*指针来保存节点值，并且可以通过 list 结构的 dup，free，match 三个属性为节点值设置类型特定函数，所以链表可以用于保存各种不同类型的值 第三章 字典 Redis 普通状态下的字典： 哈希算法：首先计算哈希值，然后计算出索引值 Redis 使用的 MurmurHash 算法计算建的哈希值，该算法的优点在于即使输入的键时有规律的，算法仍然能给出一个很好的随机分布性。 解决键冲突：使用链地址法解决键冲突，使用头插法进行插入。 rehash：当负载因子过大的时候，就会开始进行相应的扩展或者收缩。使用 ht[1] 协助扩展。 渐进式 rehash：扩展或收缩哈希表需要将 ht[0] 里面的所有键值对 rehash 到 ht[1] 里面，但是，这个 rehash 动作并不是一次性、集中式地完成的，而是分多次、渐进式地完成。在渐进期间，字典会同时使用两个哈希表，但是插入的时候只会插入到 ht[1]。 第五章 跳跃表 跳跃表：有序的数据结构，通过在节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。支持 平均 O(logn)，最坏 O(N)复杂度的节点查找。 跳跃表结构示意图： header 和 tail 分别表示表头节点和表尾节点，level 表示的层数，length 时跳跃表的长度。BW 表示的是回退指针，指向上一个跳跃表节点。箭头线上面的数字是跨度，表示跨过了几个节点。 在 Redis 中，每个节点的层高是 1 到 32 之间的随机数，在同一个跳跃表中，多个节点可以包含相同的分值，但是每个节点的额成员对象必须唯一，另外，跳跃表中的节点按照分值大小排序，如果分值大小相同，则按照成员对象的大小排序。 第六章 整数集合 整数集合实现： 虽然 contents 是 int8_t 类型的数组，但实际上数组并不保存任何 int8_t 类型的值，该数组的真正类型取决于 encoding 属性的值。 升级：每当我们要将一个新元素添加到整数集合里面，并且新元素的类型比整数集合现有所有元素的类型都要长时，整数集合需要先进行升级(upgrade)，然后才能将新元素添加到整数集合里面。首先根据新元素的类型，扩展整数集合底层数组的空间大小，并且为新元素分配相应的空间；将底层数组现有的所有元素都转换成与新元素相同的类型，并且将其放到正确的位置上，保持有序性不变；将新元素添加到底层数组里面（新元素要么在最后位置，要么在首位置）。 升级的好处：提升灵活性，节约内存。 降级：整数集合不支持降级操作，一旦对数据进行了升级，编码就会一直保持升级之后的状态。 第七章 压缩列表 压缩列表的构成： zlbytes 记录整个压缩列表占用的内存字节数，zltail 记录列表表位距离压缩列表的起始地址有多少字节，zllen 记录节点个数，zlend 特殊值 0xFF，标记为压缩列表的末端。 压缩列表节点的构成： 根据 previous_entry_length 可以计算出上一个节点的地址，根据 encoding 可以知道存放的数据类型和长度，content 则是一个字节数组或者整数。 连锁更新：当压缩列表的原来的节点的数值在 250-254 之间的时候，此时如果新增或者（删除）一个节点，会导致原来的首节点 previous_entry_length 大小从 1 字节转换为五个字节，从而引发连锁更新： 尽管连锁更新的复杂度较高，但是真正造成性能问题的几率还是很低的。 第八章 对象 对象的类型和编码：Redis 中的对象的结构如下： 其中，type 表示对象的类型： 对象的 ptr 指向对象的底层实现数据结构，而这些数据结构有对象的 encoding 属性决定： 每种类型的对象至少使用了两种不同的编码： 通过 encoding 来设定对象的编码，极大提高了 Redis 的灵活性和效率。 字符串对象：编码可以是 int，raw 或者 embstr。 列表对象：编码可以是 ziplist 或者 linkedlist。 哈希对象：编码可以是 ziplist 或者 hashtable。 集合对象：编码可以是 intset 或者 hashtable。 有序集合对象：编码可以是 ziplist 或者 skiplist。有序集合同时使用跳跃表可字典来实现的原因是能够让有序集合的查找和范围型的操作都尽可能快的执行，减少时间复杂度。 类型检查和命令多态：类型检查的实现是通过键中的类型来进行的，命令的多态则是根据值对象的编码方式进行的。 内存回收：采用引用计数的方式实现垃圾回收。 对象共享：对象的引用计数属性还有对象共享的作用。目前来说，Redis 会在初始化服务器时，创建一万个字符串对象，这些对象包含了从 0 到 9999 的所有整数值，当服务器需要用到值为 0 到 9999 的字符串对象时，服务器就会使用这些共享对象，而不是新创建对象。 对象的空转时间：redisObject 结构还包含了一个属性 lru，用于记录对象最后一次被命令程序访问的时间。 第九章 数据库 服务器中的数据库：Redis 服务器将所有的数据库保存在服务器状态中的 redis.h&#x2F;redisServer 结构 db 数组中，每个 redis.h&#x2F;redisDb 结构代表着一个数据库，另外程序会根据 dbnum 来决定应该创建多少个数据库： 1234567struct redisServer &#123; // 数据库 redisDb *db; // 数据库的数量 int dbnum; // ...&#125; 切换数据库：通过 SELECT 命令实现，实际上是通过修改客户端的 db 指针来实现的 数据库键空间：每个数据库由 RedisDb 保存，其中 RedisDb.dict 保存了数据库中的所有键值对。 增删查改都是在 dict 结构上进行的，另外，在读写键空间的时候，还会执行一些其他的额外操作，比如更新 LRU 时间，提前判断键是否过期，标记键为 dirty 等等维护数据库一致性的操作。 设置键的生存时间或过期时间：在 Redis 中有四个不同的命令来设置键的生存时间或者过期时间，分别是 EXPIRE，PEXPIRE，EXPIREAT，PEXPIREAT 命令，前三个命令都是转换为 PEXPIREAT 命令执行，转换图如下： RedisDb 结构中的 expires 字典保存着数据库中所有过期键的过期时间，称该字典是过期字典，其中键是一个指针，指向某个键空间的某个键对象，过期字典的值保存着 long long 类型的整数，保存着过期时间。 由此，查看某个键对应的值之前需要先判断一下是否在过期字典中，同时检测其是否过期。 过期键删除策略：定时删除，惰性删除，定期删除。定时删除策略对内存是最友好的:通过使用定时器，定时除策略可以保证过期键会尽 可能快地被删除，并释放过期键所占用的内存。惰性删除策略对 CPU 时间来说是最友好的:程序只会在取出键时才对键进行过期检查，这可以保证删除过期键的操作只会在非做不可的情况下进行，并且删除的目标仅限于当前处理的键，这个策略不会在删除其他无关的过期键上花费任何 CPU 时间。定期删除策略每隔一段时间执行一次删除过期键操作，并通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响 。 Redis 的过期键的删除策略：使用惰性删除和定期删除两种策略。惰性删除策略通过 db.c&#x2F;expireIfNeeded 函数实现，定期删除则是通过 redis.c&#x2F;activeExpireCycle 函数实现（分多次遍历服务器中的各个数据库，从过期字典中随机抽查一部分键的过期时间，并且删除其中的过期键，其中 current_db 全部变量保存着当前指向到那个数据库中了，下次便利的时候就可以接着从上次数据库的下一个接着检查）。 AOF，RDB 和复制功能对过期键的处理： RDB：在执行 SAWE 命令或者 BGSAVE 命令创建一个新的 RDB 文件时，程序会对数据库中的 键进行检查，已过期的键不会被保存到新创建的 RDB 文件中。 载入 RDB 文件的时候，如果服务器以主服务器模式运行，那么在载入 RDB 文件的时候，程序会剔除过期的键，如果是以从服务器模式运行的话，那么就会保存所有的键。 AOF：AOF 文件重写的时候，会对数据库中的键进行检查，已经过期的键不会保存到 AOF 文件中。 复制：当服务器运行在复制模式下面的时候，从服务器的过期键的删除动作由主服务控制，从服务器不会主动删除过期的键，除非主服务器发送 DEL 命令来显式删除某个键。 数据库通知：通过发布订阅模式实现。 第十章 RDB 持久化 RDB 文件的创建和载入：有两个命令可以用于生成 RDB 文件，一个是 SAVE，另外一个是 BGSAVE，后者是非阻塞的。另外由于 AOF 文件的更新频率比 RDB 文件的更新频率高一些，会首选 AOF 文件来恢复数据库状态。 虽然 BGSAVE 执行的时候仍然可以继续处理客户端的请求，但是 SAVE，BGSAVE 和 BGREWRITEAOF 命令却不能再次执行。在服务器载入 RDB 文件的时候，会一直处于阻塞状态，直到载入工作完成为止。 自动间隔性保存：当服务器满足一定的条件的时候，就会自动执行相应 BGSAVE 命令，来及时保存数据库的状态。默认保存条件如下： 123save 900 1save 300 10save 60 10000 上述代码的含义是 900 秒之内，至少修改了一次数据库，或者 300 秒之内，修改了至少 10 次数据库，或者 60 秒内，至少修改了 10000 次数据库，这些配置文件会被保存在 saveparams 属性中。除了 saveparams 数组之外，还有一个 dirty 计数器，以及一个 lastsave 属性，通过上述三个属性我们就可以判断是否存在必要来执行自动保存功能了。 123456struct redisServer &#123; struct saveparam *saveparams; long long dirty; time_t lastsave; // ...&#125; RDB 文件结构如下： 第十一章 AOF 持久化 AOF 持久化的实现：当 AOF 的功能打开的时候，服务器在完成一个命令的之后，会将其保存在 redisServer 的 aof_buf 缓冲区的末尾。Redis 的服务器就是一个时间循环，这个循环中负责接受客户端的命令请求，以及向客户端发送命令回复，而时间事件则负责执行 serverCron 这样需要定时运行的函数。每次结束一个事件循环的时候，需要考虑是否需要将缓冲区的内容写入到 AOF 文件中。 由于 AOF 文件包含了重建数据库的所有写命令，所以数据库只需要读入并执行一遍 AOF 里面的命令就可以恢复数据库关闭前的状态。 AOF 重写：随着时间的流逝，AOF 文件的内容会越来越多，不加以控制的话，会很容易超过体积最大限制造成影响。为了解决这个问题，Redis 提供了 AOF 文件重写的功能。重写功能是通过读取当前数据库的状态来实现的。另外为了提高服务器的可用性，一般执行 AOF 重写的时候采用的是后台重写，以此防止阻塞的问题。但是使用子进程进行 AOF 文件的重写的时候，服务器会接受客户端的命令，而新的命令可能会造成数据库状态的修改，从而使得当前数据库状态和重写后的 AOF 文件保存的数据库状态不一致。为了解决这种数据不一致问题，Redis 服务器设置了一个 AOF 重写缓冲区，这个缓冲区在服务器创建子进程之后开始使用，当 Redis 服务器执行完一个写命令之后，它会同时将这个写命令发送给 AOF 缓冲区和 AOF 重写缓冲区，如图 11-4 所示。 当子进程完成重写操作的时候，会向父进程发送信号，父进程此时会将 AOF 重写缓冲区的内容写入到新的 AOF 文件中，最后执行改名覆盖现有的 AOF 文件，实现新旧两个 AOF 文件的替换。 第十二章 事件 Redis 服务器是一个事件驱动程序，服务器需要处理两类事件：文件事件，时间事件。文件事件是服务器对套接字的抽象，时间事件则是定时操作的抽象。 文件事件：Redis 基于 Reactor 模式开发出了自己的网络事件处理器，这个处理器被称作为文件事件处理器，组成如下： IO 多路复用程序的实现，通过包装常见的 select，epoll，evport 和 kqueue 来实现的。 时间事件：分为定时事件和周期性事件。目前 Redis 中只是用了周期性事件，没有使用定时事件。服务器将所有时间事件都放在一个无序链表中，每当时间事件执行器运行时，它就遍历整个链表，査找所有已到达的时间事件，并调用相应的事件处理器。 持续运行的 Redis 服务器需要对自身的资源和状态进行检查和调整，从而可以确保服务器可以长期稳定的运行，这些定期操作被封装到 redis.c&#x2F;serverCron 函数中执行。正常模式下 Redis 服务器只使用 serverCron 一个时间事件。 事件的调度与执行： 由于时间事件在文件事件之后执行，并且事件之间不会出现抢占，所以时间事件的实际处理时间，通常会比时间事件设定的到达事件稍晚一些。 第十三章 客户端 Redis 服务器会为每个客户端创建一个 redis.h&#x2F;redisClient 结构，用于保存客户端的信息，Redis 服务器中还会保存着一个 clients 的链表，用于保存所有和服务器相连接的客户端。 客户端属性：通用属性和特定属性。有以下几种属性： 套接字描述符：fd，伪客户端为 -1，否则为大于 -1 的整数。 名字：name 标志：flags，记录了客户端的角色 输入缓冲区：querybuf，保存客户端发送的命令请求 命令和命令参数：argc 和 argv，服务器对 querybuf 解析后将参数的个数和参数存入这两个变量 命令的实现函数：cmd，当服务器解析出来命令之后，就可以查找对应的命令的实现函数，然后将其指针值复制到 client 中 输出缓冲区：buf[MAX_BYTES]，命令回复会保存在这里面 身份验证：authenticated，记录客户端是否通过了身份验证 时间：包含客户端创建的时间，最后一次和服务器互动的时间 客户端的创建和关闭：如果是普通的客户端，那么就会在 clients 链表后面追加上一个 redisClient 结构。服务器使用两种模式来限制客户端缓冲区的大小：硬性限制，软性限制。另外，处理 Lua 脚本的伪客户端在服务器初始化时创建，直到服务器关闭，而 AOF 文件载入的时候的伪客户端则在载入结束之后关闭。 第十四章 服务器 命令请求的执行过程：客户端发送命令请求，服务器端读取命令请求，接下来分析命令，查找命令表获取命令实现函数，调用获取到的命令实现函数，命令实现函数执行完后执行后续的工作，最后将命令回复发送给客户端，客户端接受并且打印命令回复。 ServerCron 函数每隔 100 毫秒执行一次，这个函数负责管理服务器的资源，并且保持服务器自身的良好运转。它具有以下功能： 更新服务器事件缓存 更新 LRU 时钟 更新服务器每秒执行命令次数 更新服务器内存峰值记录 处理 SIGTERM 信号 管理客户端的资源 管理数据库的资源 执行被延迟的 BGREWRITEAOF 检查持久化操作的运行状态 将 AOF 缓冲区的内容写入 AOF 文件 关闭异步客户端 增加 cronloops 计数器的值 初始化服务器： 初始化服务器的状态结构 载入配置选项 初始化服务器数据结构 还原数据库状态 执行事件循环 第十五章 复制 在 Redis 中，用户通过命令 SLAVEOF 让一个服务器去复制另外一个服务器，被复制的服务器成为主服务器，对主服务器进行复制的服务器称为从服务器，集群对外提供读写分离功能。 旧版复制功能的实现： 同步：将从服务器的数据库状态更新至主服务器当前所处的数据库状态。通过 SYNC 命令完成： 从服务器向主服务器发送 SYNC 命令 主服务器执行 BGSAVE 命令，后台生成 RDB 文件，同时用一个缓冲区记录从现在开始执行后的所有写命令 RDB 生成后，将其发送给从服务器，从服务器加载 RDB 文件 从服务器加载完成后，主服务器将缓冲区（repl_buffer）里面的所有写命令发送给从服务器，保持一致性 命令传播：当主服务的数据库状态被修改的时候，此时造成数据库状态的不一致性，通过命令传播让主从服务器状态重新回到一致状态。 旧版复制功能的缺陷：复制可以分为初次复制和断线后重复制，旧版复制功能并没有很好解决断线后复制时间长的问题：只需要将断线后的命令发送给从服务器就行，不需要全部复制一遍。 新版复制功能的实现：新版复制功能使用 PSYNC 命令代替 SYNC 命令来执行同步操作。PSYNC 命令有完整重同步和部分重同步两种模式，前者用于处理初次复制的情况，后者用于处理断线后重复制情况。 部分重同步的实现： 主从服务器的复制偏移量：主服务器每次向从服务器传播 N 个字节的数据时，就将自己的复制偏移量加上 N，从服务器每次收到主服务器传播过来的 N 个字节的数据时，就将自己的复制偏移量加上 N，通过对比主从服务器的复制偏移量，可以很容易知道主从服务器是否处于一致状态 服务器的复制积压缓冲区：由主服务器维护的一个固定长度的先进先出的队列，默认 1MB。当主服务器进行命令传播的时候，不仅将写命令发送到从服务器，还将写命令入队到复制积压缓冲区里面。当从服务器重新连接到主服务器的时候，根据 offset 偏移量之后的数据在缓冲区里面，执行部分重同步，否则执行完整重同步 服务器的运行 ID：每个 Redis 服务器都有自己的运行 ID。初次复制的时候，主服务器向从服务器发送自己的 ID，从服务器重新连接的时候，通过 ID 和重连服务器的 ID 比对来判断是否重连到相同的主服务器 PSYNC 命令的实现： 复制的实现： 设置主服务器的地址和端口 建立套接字连接 发送 PING 命令，检测网络是否通畅，不通畅的话断开并且重连 身份验证 发送端口信息 同步 命令传播 心跳检测：在命令传播阶段，从服务器默认会每秒一次的频率，向主服务器发送 REPLCONF ACK &lt;replication_offset&gt;，其作用有： 检测主从服务器的网络连接状态 辅助实现 min-slaves 选项：可以防止主服务器在不安全的情况下执行写命令，如配置 min-slaves-to-write 为 3，那么在从服务器的数量小于 3 时，拒接执行写命令 检测命令丢失：如果丢包的话，通过 offset 实现一致性 第十六章 Sentinel Sentinel 是 Redis 的高可用性解决方案：由一个或多个 Sentinel 实例组成的 Sentinel 系统可以监视任意多个主服务器，以及这些主服务器属下的所有从服务器，当被监视的主服务器进入下线的状态时，自动将下线服务器的某个从服务器升级为新的主服务器，然后由新的主服务器代替已经下线的主服务器处理命令请求，当原来的主服务器又重新上线的时候，将其设置为新的主服务器的一个从服务器。 启动并初始化 Sentinel： 初始化服务器：Sentinel 本质上是一个运行在特殊模式下的 Redis 服务器，但是初始化的时候不载入数据 使用 Sentinel 专用代码 初始化 Sentinel 状态：服务器会初始化一个 sentinelState 结构 初始化 Sentinel 的 masters 属性：键是被监视主服务器的名字，值是被监视主服务器对应的 sentinelRedisInstance 结构，每个这样的结构代表一个被 Sentinel 监视的实例，可以是主服务器，从服务器，或者另外一个 Sentinel 创建连向主服务器的网络连接：会创建两个连接，一个是命令链接，另外一个是订阅连接 获取主服务器信息：Sentinel 每十秒一次的频率，向被监视的主服务器发送 INFO 命令，获取当前信息。当 Sentinel 分析 INFO 命令中包含的从服务器的信息的时候，会检查从服务器对应的实例是否存在于其主服务器实例 slaves 字典中，存在的话更新相关信息，否则的话创建新的实例 获取从服务器信息：Sentinel 每十秒一次的频率，向被监视的从服务器发送 INFO 命令，获取当前信息。会对从服务器实例进行更新 向主服务器和从服务器发送信息：默认情况下，PUBLISH 信息到主从服务器： 接收来自主从服务器的频道信息： 更新 sentinels 字典 创建连接其他 Sentinel 的命令连接 检测主观下线行为：默认情况下，Sentinel 每秒向其创建了命令连接的实例发送 PING 命令，通过 PONG 回复检测是否在线。如果一个实例在 down-after-millseconds 毫秒内，连续向 Sentinel 返回无效回复，那么 Sentinel 会修改这个实例的结构，将 flags 属性中的 SRI_S_DOWN 设置为真，表示其主观下线。 检查客观下线状态：当 Sentinel 将一个服务器判断为主观下线之后，为了确保是否真的下线了，需要向其他监视了这个服务器的 Sentinel 进行询问，看他们是否也认为主服务器也已经进入了下线状态。当接收到足够的下线判断之后，Sentinel 就会将从服务器判定为客观下线，并对主服务器执行故障转移操作。 选举领头 Sentinel：当一个主服务器被判断为客观下线时，监视这个下线服务器的各个 Sentinel 会进行协商，选举出一个领头 Sentinel，由领头 Sentinel 对下线服务器执行故障转移操作。 Raft算法的领头选举方法的实现。 故障转移： 选出新的主服务器 修改从服务器的复制目标 将旧的主服务器变为从服务器 第十七章 集群 Redis 集群是 Redis 提供的分布式数据库方案，集群通过分片(sharding) 来进行数据共享，并提供复制和故障转移功能。 节点：一个 Redis 集群通常由多个节点(node) 组成，在刚开始的时候，每个节点都是相互独立的，它们都处于一个只包含自已的集群当中，要组建一个真正可工作的集群，我们必须将各个独立的节点连接起来，构成一个包含多个节点的集群。 启动节点：节点实际上就是一个运行在集群模式下的 Redis 服务器，通过 cluster-enabled 选项进行配置。 集群数据结构：clusterNode 结构保存了一个节点的当前状态，ClusterState 结构记录了在当前节点的视角下，集群目前所处的状态 CLUSTER MEET 命令实现： 槽指派：Redis 集群通过分片的方式来保存数据库中的键值对：集群的整个数据库被分为 16384 个槽(slot)，数据库中的每个键都属于这 16384 个槽的其中一个，集群中的每个节点可以处理 0 个或最多 16384 个槽。当数据库中的 16384 个槽都有节点在处理时，集群处于上线状态(ok)；相反地，如果数据库中有任何一个槽没有得到处理，那么集群处于下线状态(fail)。 记录节点的槽指派信息：clusterNode 结构中的 slots 属性记录了节点当前负责处理那些槽： slots数组是一个二进制数组，为 1 则该节点负责该槽，否则不负责该槽。 传播节点的槽指派信息：当节点 A 通过消息从节点 B 那里接收到节点 B 的 slots 数组时，节点 A 会在自已的 clusterState.nodes 字典中査找节点 B 对应的 clusterNode 结构，并对结构中的 slots 数组进行保存或者更新 记录集群所有槽的指派信息：clusterState 结构中的 slots 数组记录了集群中所有的 16384 个槽的指派信息 CLUSTER ADDSLOTS 命令的实现：首先改动 clusterState.slots 指针，将对应槽指向自己，然后修改 clusterNode.slots 数组，将对应的 slots 置位。 在集群中执行命令：当集群的 16384 个槽都指派后，集群就会进入上线状态，这时客户端就可以向集群中的节点发送命令了。 计算键输入哪个槽：CRC16(key) &amp; 16383 判断槽是否由当前节点负责处理：clusterState.slots[i] &#x3D;&#x3D; clusterState.myself MOVED 错误：当节点发现键所在的槽不由自己处理的时候，返回 MOVED &lt;SLOT&gt; &lt;ip&gt;:&lt;addr&gt;，客户端自动转向到对应的节点，再次发送命令 节点数据库的实现：集群节点保存键值对的方式和单机 Redis 服务器保存方式完全相同。唯一区别是节点只能使用 0号数据库。另外，节点会使用 clusterState.slots_of_keys 跳跃表保存槽和键之间的关系，命令 CLUSTER GETKEYSINSLOT &lt;slot&gt; &lt;count&gt; 就是建立在该结构上的。 重新分片：Redis 集群的重新分片操作可以将任意数量已经指派给某个节点(源节点)的槽改为指派给另一个节点(目标节点)，并且相关槽所属的键值对也会从源节点被移动到目标节点。重新分片操作可以在线(online)进行，在重新分片的过程中，集群不需要下线，并且源节点和目标节点都可以继续处理命令请求。以下是对单个槽 slot流程： ASK 错误：当执行分片期间，可能会存在这样一种情况：输入被迁移槽的一部分节点键值对保存在源节点里面，另外一部分保存在目标节点里面。此时响应客户端的命令如下： CLSUTER SETSLOT IMPORTING 命令的实现：clusterState.importing_slots_from 记录了当前节点正在从其他节点导入的槽。 CLUSTER SETSLOT MIGRATING 命令的实现：clusterState.migrating_slots_to 数组记录当前节点正在迁移至其他节点的槽。 ASK 错误：如果槽正在迁移，会发送 ASK &lt;SLOT&gt; &lt;ip&gt;:&lt;port&gt; ASKING 命令：打开发送该命令的客户端的 REDIS_ASKING 标示，该标示是一个一次性标示，当成功执行完一次命令的时候，此时该标示就会被移除。 ASK 错误和 MOVED 错误：前者发生在节点间迁移槽的时候，是一种临时措施，后者发生在槽的负责权已经从一个节点迁移到了另外一个节点。 复制与故障转移：Redis 集群中的节点分为主节点和从节点，主节点用于处理槽，从节点用于复制。故障转移措施和第十五章类似。 设置从节点：CLUSTER REPLICATE &lt;node_id&gt; 让接受命令的节点成为 node_id 所指向节点的从节点。此时从节点中设置 clusterState.myself.slaveof 属性，主节点设置 clusterNode.slaves 属性。 故障检测：集群中每个节点定期向其他节点发送 PING 消息，以此检测是否在线，如果超时，标记为 PFAIL（probable fail）。在集群中超过半数的节点都认为某个主节点 PFAIL，那么这个节点会被标记为下线（FAIL），同时广播这条消息。 故障转移：从 FAIl 的主节点的从节点里面选择一个作为主节点，然后将原来主节点的额槽指派给自己，新的主节点广播 PONG 信息，让其他的节点知道这个节点已经选为主节点，最后新的节点开始接收和负责处理自己的槽有关的命令请求，故障转移完成。 选举新的节点：Raft 算法。 消息：节点发送的消息一般有五种：MEET 消息，PING 消息，PONG 消息，FAIL 消息，PUBLISH 消息。 第十八章 发布与订阅 Redis 的发布与订阅的功能由 PUBLISH，SUBSCRIBE，PSUBSCRIB 等命令组成，每当有其他客户端向被订阅的频道发送消息时，频道的所有订阅者都会收到这个消息。 频道的订阅与退订：Redis 将所有订阅关系保存在 RedisServer.pubsub_channels 字典里面，键是某个被订阅的频道，键值则是一个链表，保存着所有订阅这个频道的客户端。 订阅频道：如果有频道已经在字典中，直接尾部插入订阅者，否则创建字典项，键为频道，键值为该客户端 退订频道：根据被退订的频道名字，从订阅者链表中删去客户端，如果此时订阅者链表为空，则删除对应的字典项 模式的订阅与退订：Redis 将所有模式的订阅关系保存在 RedisServer.pubsub_patterns 列表里面。 订阅模式：新建一个 pubsubPattern 结构，设置好 client 和 pattern 属性，然后将其加入到 pubsub_patterns 链表的表尾。 退订模式：从 pubsub_patterns 中查找对应的被退订的 pubsubPattern 结构，然后将其删除。 发送消息：当执行 PUBLISH &lt;channel&gt; &lt;message&gt; 时候，服务器需要将消息 message 发送到对应的 channel 的所有订阅者，另外如果有模式匹配这个 channel，那么需要将 message 发送给 pattern 模式的订阅者。 将消息发送给频道订阅者：从 pubsub_channels 字典里面找到订阅者链表，然后将消息发送给名单上的所有客户端 将消息发送给模式订阅者：遍历 pubsub_patterns 链表，查找那些与 channel 频道匹配的模式，并且将消息发送到这些模式的客户端。 查看订阅消息：PUBSUB 命令可以查看频道或者模式的相关信息 PUBSUB CHANNELS [pattern]：返回与 pattern 匹配的频道 PUBSUB NUMSUB [channel-1 channel-2]：返回频道对应订阅者的数量 PUBSUB NUMPAT：返回当前服务器被订阅模式的数量 第十九章 事务 Redis 通过 MULTI，EXEC，WATCH 等命令实现事务功能。在事务执行期间，服务器不会中断事务去执行其他客户端的请求，他会将事务中的命令执行完毕之后才去处理其他的客户端的请求。 事务的实现：包括三个部分，如下： 事务开始：MULTI 命令的执行标志者事务的开始，会将执行该命令的客户端从非事务状态转换为事务状态 命令入队：当客户端处于事务状态之后，如果客户端发送的指令是 EXEC，DISCARD，WATCH，MULTI 命令之一，那么立即执行，否则的话将其放入事务队列之中，然后返回 QUEUED 回复。每个 Redis 客户端都有自己的事务状态 multiState，里面包含一个事务队列： 执行事务：当发送 EXEC 命令的时候，此时就开始执行遍历这个客户端的事务队列，执行其中的所有命令，然后将得到的回复返回给客户端。 WATCH 命令的实现：WATCH 命令是一个乐观锁，它可以在 EXEC 命令执行之前，监视任意数量的数据库键，并且在 EXEC 命令执行的时候，检查被监视的键中是否至少有一个已经被修改过了，如果是的话，服务器将拒绝执行事务，并向客户端返回执行失败的空回复。 WATCH 命令监视数据库键：每个 Redis 数据库都保存着一个 watched_keys 字典，字典的键是某个数据库键，而字典的值则是一个链表，记录所有的监视相应数据库键的客户端： 1234typedef struct redisDb &#123; dict *watched_keys; // ...&#125; 监视机制的触发：所有对数据库进行修改的命令，在执行之后会调用 touchWatchKey 函数对监视字典进行检查，如果某个键被修改，会将对应的客户端 REDIS_DIRTY_CAS 标示打开，表示客户端的安全性已经被破坏。 判断事务是否安全：服务器根据客户端的 REDIS_DIRTY_CAS 标识来决定是否执行事务： 事务的 ACID 性质：在 Redis 中，事务总是具有原子性（存疑），一致性，隔离性，当 Redis 运行在某种持久化模式下，也具有持久性。 原子性：数据库将事务中的多个操作当作一个整体来执行，服务器要么就执行事务中的所有操作，要么就一个操作也不执行。 Redis 的事务和传统的关系型数据库事务的最大区别在于，Redis 不支持事务回滚机制 (rollback)，即使事务队列中的某个命令在执行期间出现了错误，整个事务也会继续执行下去，直到将事务队列中的所有命令都执行完毕为止。 Redis 的作者在事务功能的文档中解释说，不支持事务回滚是因为这种复杂的功能和 Redis 追求简单高效的设计主旨不相符，并且他认为，Redis 事务的执行时错误通常都是编程错误产生的，这种错误通常只会出现在开发环境中，而很少会在实际的生产环境中出现，所以他认为没有必要为 Redis 开发事务回滚功能。 一致性：如果数据库在执行事务之前是一致的，那么在事务执行之后，不论事务是否执行成功，数据库也应该仍然是一致的。Redis 事务可能出错的地方有入队错误，执行错误，服务器宕机。 隔离性：即使数据库中有多个事务并发地执行，各个事务之间也不会互相影响，并且在并发状态下执行的事务和串行执行的事务产生的结果完全相同。 因为 Redis 使用单线程的方式来执行事务(以及事务队列中的命令)，并且服务器保证在执行事务期间不会对事务进行中断，因此，Redis 的事务总是以串行的方式运行的，并且事务也总是具有隔离性的。 持久性：当一个事务执行完毕时，执行这个事务所得的结果已经被保存到永久性存储介质(比如硬盘)里面了，即使服务器在事务执行完毕之后停机，执行事务所得的结果也不会丢失。只有当服务器运行在 AOF 持久化模式下，并且 appendfsync 选项的值为 ALWAYS 的时候，这种配置下的事务是具有持久性的。 第二十一章 排序 SORT &lt;key&gt; 命令的实现：假设已经执行 RPUSH numbers 3 1 2，现在执行 SORT numbers， 首先创建一个和 numbers 列表长度相同的数组，数组的每一项是 redis.h&#x2F; redisSortObject 结构 遍历数组，将数组项的 obj 指针指向对应的列表项，一一对应 遍历数组，将 obj 指针指向的列表项转换为一个 double 类型的浮点数，存到 u.score 中 根据 u.score 中的值，进行升序排序 遍历数组，将各个数组项的 obj 指针指向的列表项返回给客户端 redisSortObject 结构如下： ALPHA 选项实现：字典序排序 ASC 选项和 DESC 选项的实现：默认是升序排序，升序和降序排序只不过是排序算法使用的对比函数不同而已。 BY 选项实现：默认情况下，SORT 命令使用被排序键包含的元素作为排序的权重，元素本身决定了排序所处的位置。可以使用 BY 选项来改变这种情况。 带有 ALPHA 选项的 BY 选项的实现：BY 选项默认权重值保存的值是数字值，如果是字符串值的话，需要在使用 BY 选项的同时，配合使用 ALPHA 选项。 LIMIT 选项的实现：LIMIT &lt;offset&gt; &lt;count&gt;。 GET 选项的实现：更具被排序结果中的元素，查找相关的信息。 STORE 选项的实现：STORE 选项可以保存排序结果在指定的键里面 多个选项的执行顺序：排序，限制长度，获取外部键，保存排序结果集。 除了 GET 选项外，改变选项的摆放顺序不会影响 SORT 命令执行这些选项的顺序。 第二十三章 慢查询日志 慢查询日志：记录执行时间超过指定时长的命令请求，用户可以通过该功能产生的日志来优化查询速度。服务器有两个和慢查询日志相关的选项： slowlog-log-slower-than: 指定执行时间上限，超过上限的会被记录到日志上 slowlog-max-len: 记录慢查询日志最大条数 慢查询日志记录的保存：服务器状态中有几个和慢查询日志有关的属性： 其中 slowlog 链表保存所有的慢查询日志。每个节点是一个 slowlogEntry： 慢查询日志的阅览和删除：遍历查询和遍历删除 添加新日志：对慢查询日志进行头插法插入 第二十四章 监视器 通过执行 MONITOR 命令，客户端可以将自己变为一个监视器，实时接收并打印服务器当前处理的命令请求。 成为监视器：执行 MONITOR 命令后。客户端的 REDIS_MONITOR 标志会被打开，并且这个客户端会被添加到 monitors 链表的表尾。 向监视器发送命令信息：服务器每次执行命令之前，都会调用 replicationFeedMonitor 函数，由这个函数将命令请求信息发送给各个监视器。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.zsstrike.tech/tags/Redis/"}]},{"title":"使用 Travis CI持续部署博客","slug":"使用-Travis-CI持续部署博客","date":"2020-04-11T09:38:33.000Z","updated":"2022-05-16T07:41:46.271Z","comments":true,"path":"2020/04/11/使用-Travis-CI持续部署博客/","link":"","permalink":"http://blog.zsstrike.tech/2020/04/11/%E4%BD%BF%E7%94%A8-Travis-CI%E6%8C%81%E7%BB%AD%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2/","excerpt":"本文主要介绍使用 Travis 来自动将我们的博客内容 push 到 Github Page 上，也就是所谓的持续集成&#x2F;持续部署。","text":"本文主要介绍使用 Travis 来自动将我们的博客内容 push 到 Github Page 上，也就是所谓的持续集成&#x2F;持续部署。 持续集成Travis CI 提供的是持续集成服务（Continuous Integration，简称 CI）。它绑定 Github 上面的项目，只要有新的代码，就会自动抓取。然后，提供一个运行环境，执行测试，完成构建，还能部署到服务器。 持续集成指的是只要代码有变更，就自动运行构建和测试，反馈运行结果。确保符合预期以后，再将新代码”集成”到主干。 持续集成的好处在于，每次代码的小幅变更，就能看到运行结果，从而不断累积小的变更，而不是在开发周期结束时，一下子合并一大块代码。 使用 Travis 持续部署博客我们的博客使用 Hexo 生成，博客的仓库是名是&lt;username&gt;.github.io。有两个分支，其中 master 分支用于放置我们的内容，而 hexo-project 分支用于存储我们的 hexo 工程文件。 首先登录到官网：travis-ci.org，接着点击右上角个人头像，选择博客的仓库，并且打开开关。一旦我们激活了这个仓库，那么 Travis 就能监听这个仓库的所有变化。 .travis.ymlTravis 要求项目的根目录下面，必须有一个.travis.yml文件。这是配置文件，指定了 Travis 的行为。该文件必须保存在 Github 仓库里面，一旦代码仓库有新的 Commit，Travis 就会去找这个文件，执行里面的命令。 对于我们的要求，我们只需要配置如下就行： 12345678910111213141516171819202122232425262728293031323334# 开发语言和版本language: node_jsnode_js: stable# 监听分支branches: only: hexo-project# 缓存 cache: directories: - node_modules before_install: - npm install -g hexo-cli# 安装依赖 install: - npm install - npm install hexo-deployer-git --save# 执行脚本 script: - hexo clean - hexo g# 将博客内容部署到 master 分支中after_success: - cd ./public - git init - git add --all . - git commit -m &quot;Travis CI Auto Build&quot; - git config user.name &quot;username&quot; - git config user.email &quot;emial&quot; - git push --quiet --force https://$&#123;GH_TOKEN&#125;@$&#123;GH_REF&#125; master:master# 设置环境变量 env: global: - GH_REF: github.com/&lt;username&gt;/&lt;username&gt;.github.io.git 其中，我们需要获取到&#123;GH_TOKEN&#125;，这是用于我们能够正常访问 Github API 的基础。 Tokens you have generated that can be used to access the GitHub API. 可以在用户-&gt;setting-&gt;Personal access token中创建 Token。 之后，在 Travis 网站相应的仓库中设置环境变量就可以了。 另外，如果想要获取 build 的状态图片，可以在 Travis 中将图片的 markdown 格式复制下来，放在 readme.md 中。","categories":[],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://blog.zsstrike.tech/tags/CI-CD/"}]},{"title":"Nodejs 用户注册登录和授权处理","slug":"Nodejs-用户注册登录和授权处理","date":"2020-04-11T07:27:40.000Z","updated":"2022-05-16T07:41:46.087Z","comments":true,"path":"2020/04/11/Nodejs-用户注册登录和授权处理/","link":"","permalink":"http://blog.zsstrike.tech/2020/04/11/Nodejs-%E7%94%A8%E6%88%B7%E6%B3%A8%E5%86%8C%E7%99%BB%E5%BD%95%E5%92%8C%E6%8E%88%E6%9D%83%E5%A4%84%E7%90%86/","excerpt":"本文介绍 Nodejs 搭配 Express 框架实现服务器中常见的功能：用户注册登录和授权的处理。","text":"本文介绍 Nodejs 搭配 Express 框架实现服务器中常见的功能：用户注册登录和授权的处理。 用户注册逻辑首先我们新建一个 Express 服务器： 12345678// server.jsconst express = require(&#x27;express&#x27;)const app = express()// 处理 POST 中的 Body 数据app.use(express.json())app.listen(3000) 接着使用 MongoDB 来新建一个 Collection 的 Model 对象： 12345678910111213141516171819202122232425// models.jsconst mongoose = require(&#x27;mongoose&#x27;)mongoose.connect(&#x27;mongodb://localhost:27017/test&#x27;, &#123; useNewUrlParser: true, useUnifiedTopology: true, useCreateIndex: true&#125;)const UserSchema = new mongoose.Schema(&#123; username: &#123; type: String, required: true, unique: true &#125;, password: &#123; type: String, required: true, &#125;&#125;)const User = mongoose.model(&#x27;User&#x27;, UserSchema)module.exports = &#123; User &#125; 其中，username是不能重复的。 接下来，创建用户注册请求路由： 123456789101112// server.jsapp.post(&#x27;/api/register&#x27;, (req, res) =&gt; &#123; const jsonData = req.body const user = new User(jsonData) user.save((err, user) =&gt; &#123; if(err)&#123; res.json(&#123;msg: &#x27;failed to save&#x27;&#125;) return console.log(err) &#125; res.json(user) &#125;)&#125;) 为了测试接口，可以下载 VSCode 扩展商店中REST Client，用于发送 HTTP 请求和检测响应的数据。 安装完成后，编写用户注册请求： 1234567891011// test.http@baseUrl=http://localhost:3000/api### 注册POST &#123;&#123;baseUrl&#125;&#125;/register HTTP/1.1Content-Type: application/json&#123; &quot;username&quot;: &quot;user1&quot;, &quot;password&quot;: &quot;password1&quot;&#125; 当发送请求后，可以得到服务器返回的数据。 但是这样的话，我们存入的用户密码是明文存储的，不是很安全，为此我们需要在数据存入的时候，对密码进行 hash 处理，在此使用bcrypt进行哈希： 1234567891011121314151617// models.jsconst bcrypt = require(&#x27;bcrypt&#x27;)const UserSchema = new mongoose.Schema(&#123; username: &#123; type: String, required: true, unique: true &#125;, password: &#123; type: String, required: true, set(val) &#123; // 存入的时候先进行 hash return bcrypt.hashSync(val, 10) &#125; &#125;&#125;) A library to help you hash passwords. Based on the Blowfish cipher. 用户登录接下来，创建用户登录请求路由： 123456789101112131415161718// server.jsapp.post(&#x27;/api/login&#x27;, (req, res) =&gt; &#123; const userData = req.body User.findOne(&#123; username: userData.username &#125;, (err, user) =&gt; &#123; if(err || !user) &#123; return res.status(422).json(&#123;msg: &#x27;非法用户名&#x27;&#125;) &#125; const isValid = bcrypt.compareSync(userData.password, user.password) if(!isValid)&#123; return res.status(422).json(&#123;msg: &#x27;密码错误&#x27;&#125;) &#125; res.json(&#123; user &#125;) &#125;)&#125;) 根据用户输入的密码和哈希处理的密码进行比对，判断用户输入的密码是否正确。这是基本的用户登录流程的处理。 但是我们希望用户登录之后能够保存这些状态，传统处理方法是：用户登陆成功后，产生 Session_ID 并且将其发送给客户端使其保存在 Cookie 中，之后客户端每次请求都携带 Session_ID。 在此，我们使用 JsonWebToken 来保存我们的数据，将其发送到客户端，客户端保存在 LocalStorage 中，根据其中的数据来保存用户的状态。 修改用户登录路由： 1234567891011121314151617181920212223app.post(&#x27;/api/login&#x27;, (req, res) =&gt; &#123; console.log(req.body) const userData = req.body User.findOne(&#123; username: userData.username &#125;, (err, user) =&gt; &#123; if(err || !user) &#123; return res.status(422).json(&#123;msg: &#x27;非法用户名&#x27;&#125;) &#125; const isValid = bcrypt.compareSync(userData.password, user.password) if(!isValid)&#123; return res.status(422).json(&#123;msg: &#x27;密码错误&#x27;&#125;) &#125; // jwt token const token = jwt.sign(&#123; id: user._id &#125;, SECRET) res.json(&#123; user, token &#125;) &#125;)&#125;) 注意，本文中为了演示，将SECRET硬编码进了 server.js 中，更实际的情况时我们将其保存在一个被 gitignore 的文件中，通过读取文件配置 SECRET。 用户授权当用户登录之后，客户端保存下来了 token 值，接下来假设用户想要获取个人信息，这只有在用户登录之后才有权限进行操作，为此就需要 token 的帮助了： 123### 获取个人信息GET &#123;&#123;baseUrl&#125;&#125;/profile HTTP/1.1Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IjVlOTE2OWVhYmNlZWNkM2I2YTI0NDg2OSIsImlhdCI6MTU4NjU4ODYyMn0.VZb_0Rlw27mAShcJCRpoURenfy8IoluGgQ-VDwkqyFM 上面是用户发送的请求，其中 Authorization 头部的格式如下： Authorization: &lt;type&gt; &lt;credentials&gt; 接下来处理这个请求路由： 123456789101112app.get(&#x27;/api/profile&#x27;, (req, res) =&gt; &#123; const tokenData = req.headers.authorization.split(&#x27; &#x27;).pop() const token = jwt.verify(tokenData, SECRET) User.findOne(&#123; _id: token.id &#125;, (err, user) =&gt; &#123; if(err)&#123; return res.json(&#123;msg: &#x27;error token&#x27;&#125;) &#125; res.json(&#123;msg: &#x27;your profile&#x27;, user: req.user&#125;) &#125;)&#125;) 这样我们就能够根据请求的 Authorization 头获取到用户的信息了。这就是用户授权的基本过程。 另外，如果有很多需要用户登陆之后操作，我们需要将用户验证这个操作转换成中间件的形式，这样就能够在多个路由中使用这个中间件了： 1234567891011121314151617const auth = (req, res, next) =&gt; &#123; const tokenData = req.headers.authorization.split(&#x27; &#x27;).pop() const token = jwt.verify(tokenData, SECRET) User.findOne(&#123; _id: token.id &#125;, (err, user) =&gt; &#123; if(err)&#123; return res.json(&#123;msg: &#x27;error token&#x27;&#125;) &#125; req.user = user next() &#125;)&#125;app.get(&#x27;/api/profile&#x27;, auth, (req, res) =&gt; &#123; res.json(&#123;msg: &#x27;your profile&#x27;, user: req.user&#125;)&#125;) 至于 token 的过期时间我们可以使用预定义的键exp来设置过期时间。","categories":[],"tags":[{"name":"Nodejs","slug":"Nodejs","permalink":"http://blog.zsstrike.tech/tags/Nodejs/"}]},{"title":"Vim常用命令","slug":"Vim常用命令","date":"2020-02-23T03:38:22.000Z","updated":"2022-05-16T07:41:46.120Z","comments":true,"path":"2020/02/23/Vim常用命令/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/23/Vim%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"本文介绍Vim中常用的命令，帮助我们快速使用Vim处理文本文档。","text":"本文介绍Vim中常用的命令，帮助我们快速使用Vim处理文本文档。 基本操作在Vim中一共有三种模式：命令模式，插入模式和编辑模式（Visual mode）。 在命令模式下可以使用h，j，k，l来移动，分别表示左下上右四个移动方向。 命令模式下使用x来删除单个字符。 命令模式下使用u来撤销，使用CTRL-R来取消撤销。 使用:wq保存文件并且退出，使用:q!强制退出。 使用i在光标前插入字符，使用a在光标后插入字符。 删除一整行使用dd命令。 在光标下插入新行使用o命令，在光标上插入新行使用O命令。 有些命令可以先输入Count值，再输入命令，表示的意思就是执行命令Count次。9k表示上移9行，3x表示删除三个字符。 快速操作 单词间移动：w表示后移到单词的开头，b表示前移到单词的开头。 行首和行末移动：$表示移动到本行行尾，^表示移动到本行开头。 行内单个字符查找：fx表示查找从光标向右查找字符x，Fx表示从光标向左查找字符x。 移动到特定行：&lt;num&gt;G表示移动到num行，gg表示移动到第一行，G表示移动到最后一行。 显示行号：:set nu。 滑动窗口：CTRL-U上滑半个屏幕，CTRL-D下滑半个屏幕。 删除文本：dw删除单个单词，d$删除光标到末尾的文本。 修改文本：cw修改单个单词，c$修改光标到末尾的文本。 重复上次删除或者修改命令：.。 连接不同行内容到一行上：使用3J将三行内容移动到一行上。 替换单个字符：rx将光标字符修改为x字符。 修改大小写：~将字符进行大小写转换。 键盘宏：使用q[a-z]开始记录，再次q结束，调用键盘宏使用@[a-z]，用于重复执行复杂操作。 搜索 搜索文本：/string搜索string文本，使用n可以跳转到下一个被搜索到的文本。 取消高亮：被搜索到的文本会被高亮，当不再需要高亮的时候使用:noh命令。 反向搜索文本：?string反向搜索string文本，使用n跳转到下一个被搜索到的文本。 改变搜索反向：n跳转到下一个被搜索到的文本，N跳转到上一个被搜索到的文本。 正则搜索：^表示行首，$表示行尾，\\c忽略大小写，同样可以使用其他的正则表达式。 多窗口与多文件 粘贴文本：p命令可以在光标后粘贴被保存的文本，P命令可以在光标前粘贴被保存的文本。被保存的文本包括用x，d删除的文本。 标记：使用m[a-z]对所在位置进行标记，&#96;&#96;a表示移动到刚刚被标记的位置，‘a&#96;表示移动到刚刚被标记位置的行首。 查看标记：:marks查看所作的标记。 复制文本：y命令可以复制文本，Y或者yy命令可以一整行的文本。 打开新的文件：使用:vi file.txt打开file.txt文件。 打开多个文件：使用vim one.c two.c there.c。 在多文件下转换：:next跳转到下一个文件并且打开该文件，:previous跳转到上一个文件并且打开该文件，:first跳转到第一个文件，:last跳转到最后一个文件。 查看当前所在的文件：:args可以查看自己所处的文件（用[]包括起来）。 窗口 打开新的窗口：:[count] split [filename]水平打开一个新的窗口，大小为count值，文件是filename，:[count] vsplit [filename]垂直打开一个新的窗口，大小为count值，文件是filename。 窗口间移动：CTRL-W[hjkl]根据方向键改变窗口，CTRL-W CTRL-W在不同窗口间进行移动。 改变窗口大小：CTRL-W+增加窗口大小，CTRL-W-减小窗口大小，CTRL-W=使窗口大小相同。 基本的编辑模式 三种编辑模式：v字符编辑模式，V行编辑模式，CTRL-V矩形编辑模式。 删除文本：d删除所选的文本，D删除所选文本行（从光标到末尾）。 复制文本：y复制所选的文本，Y复制所选文本行（从光标到末尾）。 修改文本：c修改所选的文本，C修改所选的文本行（从光标到末尾）。 多行合并：J将所选的文本合并到一行。 缩进：&lt;和&gt;进行左缩进和右缩进。 矩形编辑模式下的特殊操作： 插入文本：Istring&lt;Esc&gt;在矩形前面插入string文本。 修改文本：cstring&lt;Esc&gt;修改矩形中的文本。 替换文本：rchar&lt;Esc&gt;替换矩形中的文本。 程序员相关指令 打开语法高亮：:syntax on。 设置文件的格式以适应语法高亮：:set filetype=c。 行缩进：&lt;&lt;或者是&gt;&gt;。 行缩进大小：:set shiftwidth=4。 设置缩进方式：:set (cindent|smartindent|autoindent)。 自动缩进大括号内的内容：=%。 查找单词：[CTRL-I全文查找单词，]CTRL-I从光标到文件末尾查找单词。 跳转到变量定义：gd跳转到局部变量定义，gD跳转到全局变量定义。 跳转到宏定义：[CTRL-D跳转到第一个宏定义，]CTRL-D跳转到下一个宏定义。 查看宏定义：[d查找显示第一个宏定义，]d从光标处开始查找宏定义。[D显示所有匹配的宏定义列表，]D显示光标后所有匹配的宏定义的列表。 查看匹配的括号对：%查找并且跳转到匹配的括号对上。 缩进代码块：&gt;%或者&lt;%。 自动补全：CTRL-P前向搜索补全词汇，CTRL-N后向搜索补全词汇。","categories":[],"tags":[{"name":"Vim","slug":"Vim","permalink":"http://blog.zsstrike.tech/tags/Vim/"}]},{"title":"MySQL基本操作","slug":"MySQL基本操作","date":"2020-02-19T03:48:27.000Z","updated":"2022-11-16T11:36:10.064Z","comments":true,"path":"2020/02/19/MySQL基本操作/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/19/MySQL%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","excerpt":"本文介绍MySQL数据库的简单使用方法，包括数据库的启动和连接，以及数据的增删改查等。操作环境在CentOS 7中。","text":"本文介绍MySQL数据库的简单使用方法，包括数据库的启动和连接，以及数据的增删改查等。操作环境在CentOS 7中。 数据库连接和断开连接在进行数据库的连接之前，我们需要先启动数据库服务： 1shell&gt; systemctl start mysqld.service 数据库连接命令： 12shell&gt; mysql [-h host] -u root -pEnter password: ***** 缺省host就表示连接本地的mysql数据库。 数据库断开连接： 12mysql&gt; quitBye 查询连接到mysql后，可以查询版本信息，当前日期和时间和当前用户： 1234567mysql&gt; SELECT VERSION(), CURDATE(), NOW(), USER();+-----------+--------------+---------------------+----------------+| VERSION() | CURDATE() | NOW() | USER() |+-----------+--------------+---------------------+----------------+| 8.0.19 | 2020-02-19 | 2020-02-19 18:03:54 | root@localhost |+-----------+--------------+---------------------+----------------+1 row in set (0.00 sec) 创建和使用数据库查询当前服务器中的数据库有哪些： 1234567891011mysql&gt; SHOW DATABASES;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys || test |+--------------------+5 rows in set (0.01 sec) 使用某个数据库： 12mysql&gt; USE test;Database changed 在test数据库中创建的任何数据可能会被别人删除，可以让管理员执行以下命令使得只有你能使用某个数据库(menagerie)： 1mysql&gt; GRANT ALL ON menagerie.* TO &#x27;your_mysql_name&#x27;@&#x27;your_client_host&#x27;; 创建和选择数据库创建新的数据库： 12mysql&gt; CREATE DATABASE menagerie;Query OK, 1 row affected (0.00 sec) 选择数据库： 12mysql&gt; USE menagerie;Database changed 查询当前使用的数据库： 1234567mysql&gt; SELECT DATABASE();+------------+| DATABASE() |+------------+| menagerie |+------------+1 row in set (0.00 sec) 创建表格展示当前数据库中有哪些表格： 12mysql&gt; SHOW TABLES;Empty set (0.00 sec) 创建新的表格： 12345678mysql&gt; CREATE TABLE pet ( -&gt; name VARCHAR(20), -&gt; owner VARCHAR(20), -&gt; species VARCHAR(20), -&gt; sex CHAR(1), -&gt; birth DATE, -&gt; death DATE);Query OK, 0 rows affected (0.06 sec) 查看表格的表头和描述： 1234567891011mysql&gt; DESCRIBE pet;+---------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+---------+-------------+------+-----+---------+-------+| name | varchar(20) | YES | | NULL | || owner | varchar(20) | YES | | NULL | || species | varchar(20) | YES | | NULL | || sex | char(1) | YES | | NULL | || birth | date | YES | | NULL | || death | date | YES | | NULL | |+---------+-------------+------+-----+---------+-------+ 写入数据使用文件载入数据： 123mysql&gt; LOAD DATA LOCAL INFILE &#x27;/usr/local/src/pet.txt&#x27; INTO TABLE pet;Query OK, 8 rows affected, 1 warning (0.00 sec)Records: 8 Deleted: 0 Skipped: 0 Warnings: 1 pet.txt 的内容如下： 12345678Fluffy Harold cat f 1993-02-04 \\NClaws Gwen cat m 1994-03-17 \\NBuffy Harold dog f 1989-05-13 \\NFang Benny dog m 1990-08-27 \\NBowser Diane dog m 1979-08-31 1995-07-29Chirpy Gwen bird f 1998-09-11 \\NWhistler Gwen bird \\N 1997-12-09 Slim Benny snake m 1996-04-29 \\N 其中每条记录之间使用TAB分割，每个行结束符是\\r，\\N表示的是NULL值。 如果执行命令出错，可能需要先开启local_infile变量： 1mysql&gt; set global local_infile = &#x27;ON&#x27;; 然后通过下述命令进入mysql交互环境： 1$ mysql --local-infile=1 -u root -p 同样地，我们可以通过INSERT语句来插入数据： 123mysql&gt; INSERT INTO pet -&gt; VALUES (&#x27;Puffball&#x27;, &#x27;Diane&#x27;, &#x27;hamster&#x27;, &#x27;f&#x27;, &#x27;1999-3-30&#x27;, NULL);Query OK, 1 row affected (0.00 sec) 从表格中获取信息从表格获取信息的方式一般形式是： 123SELECT what_to_selectFROM which_tableWHERE conditions_to_satisfy; 选择全部数据： 1234567891011121314mysql&gt; SELECT * FROM pet;+----------+--------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+----------+--------+---------+------+------------+------------+| Fluffy | Harold | cat | f | 1993-02-04 | NULL || Claws | Gwen | cat | m | 1994-03-17 | NULL || Buffy | Harold | dog | f | 1989-05-13 | NULL || Fang | Benny | dog | m | 1990-08-27 | NULL || Bowser | Diane | dog | m | 1979-08-31 | 1995-07-29 || Chirpy | Gwen | bird | f | 1998-09-11 | NULL || Whistler | Gwen | bird | NULL | 1997-12-09 | 0000-00-00 || Slim | Benny | snake | m | 1996-04-29 | NULL || Puffball | Diane | hamster | f | 1999-03-30 | NULL |+----------+--------+---------+------+------------+------------+ 从表中我们发现Bowser的birth值不太正确，我们可以 在pet.txt中修改文件，然后清空pet表格，接着载入数据 12mysql&gt; DELETE FROM pet;mysql&gt; LOAD DATA LOCAL INFILE &#x27;/usr/local/src/pet.txt&#x27; INTO TABLE pet; 使用UPDATE语句： 12mysql&gt; UPDATE pet SET birth = &#x27;1989-08-31&#x27; WHERE name = &#x27;Bowser&#x27;;Query OK, 1 row affected (0.00 sec) 有条件地选择可以通过WHERE来实现： 123456789101112131415161718192021222324252627282930313233343536373839mysql&gt; SELECT * FROM pet WHERE name = &#x27;Bowser&#x27;;+--------+-------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+--------+-------+---------+------+------------+------------+| Bowser | Diane | dog | m | 1989-08-31 | 1995-07-29 |+--------+-------+---------+------+------------+------------+mysql&gt; SELECT * FROM pet WHERE birth &gt;= &#x27;1998-1-1&#x27;;+----------+-------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+----------+-------+---------+------+------------+-------+| Chirpy | Gwen | bird | f | 1998-09-11 | NULL || Puffball | Diane | hamster | f | 1999-03-30 | NULL |+----------+-------+---------+------+------------+-------+mysql&gt; SELECT * FROM pet WHERE species = &#x27;dog&#x27; AND sex = &#x27;f&#x27;;+-------+--------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+-------+--------+---------+------+------------+-------+| Buffy | Harold | dog | f | 1989-05-13 | NULL |+-------+--------+---------+------+------------+-------+mysql&gt; SELECT * FROM pet WHERE species = &#x27;snake&#x27; OR species = &#x27;bird&#x27;;+----------+-------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+----------+-------+---------+------+------------+-------+| Chirpy | Gwen | bird | f | 1998-09-11 | NULL || Whistler | Gwen | bird | NULL | 1997-12-09 | NULL || Slim | Benny | snake | m | 1996-04-29 | NULL |+----------+-------+---------+------+------------+-------+mysql&gt; SELECT * FROM pet WHERE (species = &#x27;cat&#x27; AND sex = &#x27;m&#x27;) OR (species = &#x27;dog&#x27; AND sex = &#x27;f&#x27;);+-------+--------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+-------+--------+---------+------+------------+-------+| Claws | Gwen | cat | m | 1994-03-17 | NULL || Buffy | Harold | dog | f | 1989-05-13 | NULL |+-------+--------+---------+------+------------+-------+ 选择特定的列可以通过SELECT语句实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051mysql&gt; SELECT name, birth FROM pet;+----------+------------+| name | birth |+----------+------------+| Fluffy | 1993-02-04 || Claws | 1994-03-17 || Buffy | 1989-05-13 || Fang | 1990-08-27 || Bowser | 1989-08-31 || Chirpy | 1998-09-11 || Whistler | 1997-12-09 || Slim | 1996-04-29 || Puffball | 1999-03-30 |+----------+------------+mysql&gt; SELECT owner FROM pet;+--------+| owner |+--------+| Harold || Gwen || Harold || Benny || Diane || Gwen || Gwen || Benny || Diane |+--------+mysql&gt; SELECT DISTINCT owner FROM pet;+--------+| owner |+--------+| Benny || Diane || Gwen || Harold |+--------+mysql&gt; SELECT name, species, birth FROM pet WHERE species = &#x27;dog&#x27; OR species = &#x27;cat&#x27;;+--------+---------+------------+| name | species | birth |+--------+---------+------------+| Fluffy | cat | 1993-02-04 || Claws | cat | 1994-03-17 || Buffy | dog | 1989-05-13 || Fang | dog | 1990-08-27 || Bowser | dog | 1989-08-31 |+--------+---------+------------+ 数据间的排序可以通过ORDER BY实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445mysql&gt; SELECT name, birth FROM pet ORDER BY birth;+----------+------------+| name | birth |+----------+------------+| Buffy | 1989-05-13 || Bowser | 1989-08-31 || Fang | 1990-08-27 || Fluffy | 1993-02-04 || Claws | 1994-03-17 || Slim | 1996-04-29 || Whistler | 1997-12-09 || Chirpy | 1998-09-11 || Puffball | 1999-03-30 |+----------+------------+mysql&gt; SELECT name, birth FROM pet ORDER BY birth DESC;+----------+------------+| name | birth |+----------+------------+| Puffball | 1999-03-30 || Chirpy | 1998-09-11 || Whistler | 1997-12-09 || Slim | 1996-04-29 || Claws | 1994-03-17 || Fluffy | 1993-02-04 || Fang | 1990-08-27 || Bowser | 1989-08-31 || Buffy | 1989-05-13 |+----------+------------+mysql&gt; SELECT name, species, birth FROM pet ORDER BY species, birth DESC;+----------+---------+------------+| name | species | birth |+----------+---------+------------+| Chirpy | bird | 1998-09-11 || Whistler | bird | 1997-12-09 || Claws | cat | 1994-03-17 || Fluffy | cat | 1993-02-04 || Fang | dog | 1990-08-27 || Bowser | dog | 1989-08-31 || Buffy | dog | 1989-05-13 || Puffball | hamster | 1999-03-30 || Slim | snake | 1996-04-29 |+----------+---------+------------+ 和时间相关的处理： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081mysql&gt; SELECT name, birth, CURDATE(), TIMESTAMPDIFF(YEAR,birth,CURDATE()) AS age FROM pet;+----------+------------+------------+------+| name | birth | CURDATE() | age |+----------+------------+------------+------+| Fluffy | 1993-02-04 | 2003-08-19 | 10 || Claws | 1994-03-17 | 2003-08-19 | 9 || Buffy | 1989-05-13 | 2003-08-19 | 14 || Fang | 1990-08-27 | 2003-08-19 | 12 || Bowser | 1989-08-31 | 2003-08-19 | 13 || Chirpy | 1998-09-11 | 2003-08-19 | 4 || Whistler | 1997-12-09 | 2003-08-19 | 5 || Slim | 1996-04-29 | 2003-08-19 | 7 || Puffball | 1999-03-30 | 2003-08-19 | 4 |+----------+------------+------------+------+mysql&gt; SELECT name, birth, CURDATE(), TIMESTAMPDIFF(YEAR,birth,CURDATE()) AS age FROM pet ORDER BY name;+----------+------------+------------+------+| name | birth | CURDATE() | age |+----------+------------+------------+------+| Bowser | 1989-08-31 | 2003-08-19 | 13 || Buffy | 1989-05-13 | 2003-08-19 | 14 || Chirpy | 1998-09-11 | 2003-08-19 | 4 || Claws | 1994-03-17 | 2003-08-19 | 9 || Fang | 1990-08-27 | 2003-08-19 | 12 || Fluffy | 1993-02-04 | 2003-08-19 | 10 || Puffball | 1999-03-30 | 2003-08-19 | 4 || Slim | 1996-04-29 | 2003-08-19 | 7 || Whistler | 1997-12-09 | 2003-08-19 | 5 |+----------+------------+------------+------+mysql&gt; SELECT name, birth, CURDATE(), TIMESTAMPDIFF(YEAR,birth,CURDATE()) AS age FROM pet ORDER BY age;+----------+------------+------------+------+| name | birth | CURDATE() | age |+----------+------------+------------+------+| Chirpy | 1998-09-11 | 2003-08-19 | 4 || Puffball | 1999-03-30 | 2003-08-19 | 4 || Whistler | 1997-12-09 | 2003-08-19 | 5 || Slim | 1996-04-29 | 2003-08-19 | 7 || Claws | 1994-03-17 | 2003-08-19 | 9 || Fluffy | 1993-02-04 | 2003-08-19 | 10 || Fang | 1990-08-27 | 2003-08-19 | 12 || Bowser | 1989-08-31 | 2003-08-19 | 13 || Buffy | 1989-05-13 | 2003-08-19 | 14 |+----------+------------+------------+------+mysql&gt; SELECT name, birth, death, TIMESTAMPDIFF(YEAR,birth,death) AS age FROM pet WHERE death IS NOT NULL ORDER BY age;+--------+------------+------------+------+| name | birth | death | age |+--------+------------+------------+------+| Bowser | 1989-08-31 | 1995-07-29 | 5 |+--------+------------+------------+------+mysql&gt; SELECT name, birth, MONTH(birth) FROM pet;+----------+------------+--------------+| name | birth | MONTH(birth) |+----------+------------+--------------+| Fluffy | 1993-02-04 | 2 || Claws | 1994-03-17 | 3 || Buffy | 1989-05-13 | 5 || Fang | 1990-08-27 | 8 || Bowser | 1989-08-31 | 8 || Chirpy | 1998-09-11 | 9 || Whistler | 1997-12-09 | 12 || Slim | 1996-04-29 | 4 || Puffball | 1999-03-30 | 3 |+----------+------------+--------------+mysql&gt; SELECT name, birth FROM pet WHERE MONTH(birth) = 5;+-------+------------+| name | birth |+-------+------------+| Buffy | 1989-05-13 |+-------+------------+ 模式匹配的规则如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748mysql&gt; SELECT * FROM pet WHERE name LIKE &#x27;b%&#x27;;+--------+--------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+--------+--------+---------+------+------------+------------+| Buffy | Harold | dog | f | 1989-05-13 | NULL || Bowser | Diane | dog | m | 1989-08-31 | 1995-07-29 |+--------+--------+---------+------+------------+------------+mysql&gt; SELECT * FROM pet WHERE name LIKE &#x27;%fy&#x27;;+--------+--------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+--------+--------+---------+------+------------+-------+| Fluffy | Harold | cat | f | 1993-02-04 | NULL || Buffy | Harold | dog | f | 1989-05-13 | NULL |+--------+--------+---------+------+------------+-------+mysql&gt; SELECT * FROM pet WHERE name LIKE &#x27;%w%&#x27;;+----------+-------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+----------+-------+---------+------+------------+------------+| Claws | Gwen | cat | m | 1994-03-17 | NULL || Bowser | Diane | dog | m | 1989-08-31 | 1995-07-29 || Whistler | Gwen | bird | NULL | 1997-12-09 | NULL |+----------+-------+---------+------+------------+------------+mysql&gt; SELECT * FROM pet WHERE name LIKE &#x27;_____&#x27;;+-------+--------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+-------+--------+---------+------+------------+-------+| Claws | Gwen | cat | m | 1994-03-17 | NULL || Buffy | Harold | dog | f | 1989-05-13 | NULL |+-------+--------+---------+------+------------+-------+mysql&gt; SELECT * FROM pet WHERE REGEXP_LIKE(name, &#x27;^b&#x27;);+--------+--------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+--------+--------+---------+------+------------+------------+| Buffy | Harold | dog | f | 1989-05-13 | NULL || Bowser | Diane | dog | m | 1979-08-31 | 1995-07-29 |+--------+--------+---------+------+------------+------------+mysql&gt; SELECT * FROM pet WHERE REGEXP_LIKE(name, &#x27;fy$&#x27;);+--------+--------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+--------+--------+---------+------+------------+-------+| Fluffy | Harold | cat | f | 1993-02-04 | NULL || Buffy | Harold | dog | f | 1989-05-13 | NULL |+--------+--------+---------+------+------------+-------+ 统计查询数目的条数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677mysql&gt; SELECT COUNT(*) FROM pet;+----------+| COUNT(*) |+----------+| 9 |+----------+mysql&gt; SELECT owner, COUNT(*) FROM pet GROUP BY owner;+--------+----------+| owner | COUNT(*) |+--------+----------+| Benny | 2 || Diane | 2 || Gwen | 3 || Harold | 2 |+--------+----------+mysql&gt; SELECT species, COUNT(*) FROM pet GROUP BY species;+---------+----------+| species | COUNT(*) |+---------+----------+| bird | 2 || cat | 2 || dog | 3 || hamster | 1 || snake | 1 |+---------+----------+mysql&gt; SELECT sex, COUNT(*) FROM pet GROUP BY sex;+------+----------+| sex | COUNT(*) |+------+----------+| NULL | 1 || f | 4 || m | 4 |+------+----------+mysql&gt; SELECT species, sex, COUNT(*) FROM pet GROUP BY species, sex;+---------+------+----------+| species | sex | COUNT(*) |+---------+------+----------+| bird | NULL | 1 || bird | f | 1 || cat | f | 1 || cat | m | 1 || dog | f | 1 || dog | m | 2 || hamster | f | 1 || snake | m | 1 |+---------+------+----------+mysql&gt; SELECT species, sex, COUNT(*) FROM pet WHERE species = &#x27;dog&#x27; OR species = &#x27;cat&#x27; GROUP BY species, sex;+---------+------+----------+| species | sex | COUNT(*) |+---------+------+----------+| cat | f | 1 || cat | m | 1 || dog | f | 1 || dog | m | 2 |+---------+------+----------+mysql&gt; SELECT species, sex, COUNT(*) FROM pet WHERE sex IS NOT NULL GROUP BY species, sex;+---------+------+----------+| species | sex | COUNT(*) |+---------+------+----------+| bird | f | 1 || cat | f | 1 || cat | m | 1 || dog | f | 1 || dog | m | 2 || hamster | f | 1 || snake | m | 1 |+---------+------+----------+ 使用多个表格： 123456789101112131415161718192021222324252627mysql&gt; LOAD DATA LOCAL INFILE &#x27;event.txt&#x27; INTO TABLE event;mysql&gt; SELECT pet.name, TIMESTAMPDIFF(YEAR,birth,date) AS age, remark FROM pet INNER JOIN event ON pet.name = event.name WHERE event.type = &#x27;litter&#x27;;+--------+------+-----------------------------+| name | age | remark |+--------+------+-----------------------------+| Fluffy | 2 | 4 kittens, 3 female, 1 male || Buffy | 4 | 5 puppies, 2 female, 3 male || Buffy | 5 | 3 puppies, 3 female |+--------+------+-----------------------------+mysql&gt; SELECT p1.name, p1.sex, p2.name, p2.sex, p1.species FROM pet AS p1 INNER JOIN pet AS p2 ON p1.species = p2.species AND p1.sex = &#x27;f&#x27; AND p1.death IS NULL AND p2.sex = &#x27;m&#x27; AND p2.death IS NULL;+--------+------+-------+------+---------+| name | sex | name | sex | species |+--------+------+-------+------+---------+| Fluffy | f | Claws | m | cat || Buffy | f | Fang | m | dog |+--------+------+-------+------+---------+ 从数据库或者表格中获取相关信息查询当前所有的数据库： 1mysql&gt; SHOW DATABASES; 查询当前数据库中含有的表格： 1mysql&gt; SHOW TABLES; 查询某个表格的表头和属性： 1mysql&gt; DESCRIBE pet; 查询当前所在的数据库： 1mysql&gt; SELECT DATABASE(); 使用脚本运行MySQL指令运行脚本的指令如下： 12shell&gt; mysql -u root -p &lt; batch-fileEnter password: ***** 可以通过如下方法来查看或者将输出保存到文件： 12shell&gt; mysql -u root -p &lt; batch-file | moreshell&gt; mysql -u root -p &lt; batch-file &gt; mysql.out 在batch模式和交互模式输出的内容会有不同，可以使用-t参数来得到交互模式下的输出；想要在输出语句中包含执行的指令，可以使用-v参数。 也可以在连接mysql后通过source或者\\来运行脚本： 12mysql&gt; source batch-filemysql&gt; \\ filename 常用的查询语句首先创建shop表单： 12345mysql&gt; CREATE TABLE shop( -&gt; article INT UNSIGNED DEFAULT &#x27;0000&#x27; NOT NULL, -&gt; dealer CHAR(20) DEFAULT &#x27;&#x27; NOT NULL, -&gt; price DECIMAL(16, 2) DEFAULT &#x27;0.00&#x27; NOT NULL, -&gt; PRIMARY KEY(article, dealer)); 接着插入数据： 123mysql&gt; INSERT INTO shop VALUES -&gt; (1, &#x27;A&#x27;, 3.45), (1, &#x27;B&#x27;, 3.99), (2, &#x27;A&#x27;, 10.99), (3, &#x27;B&#x27;, 1.45), -&gt; (3, &#x27;C&#x27;, 1.69), (3, &#x27;D&#x27;, 1.25), (4, &#x27;D&#x27;, 19.95); 最大的article值： 1mysql&gt; SELECT MAX(ARTICLE) FROM shop; 找到价格最大的记录： 1mysql&gt; SELECT * FROM shop WHERE price = (SELECT MAX(price) FROM shop); 找到每种物品的最大价格： 12mysql&gt; SELECT article, MAX(price) FROM shop -&gt; GROUP BY article; 对每种物品，找到最贵价格的dealer： 123456mysql&gt; SELECT article, dealer, price FROM shop s1 WHERE price=(SELECT MAX(s2.price) FROM shop s2 WHERE s1.article = s2.article) ORDER BY article; 使用用户定义的变量： 12mysql&gt; SELECT @min_price:=MIN(price),@max_price:=MAX(price) FROM shop;mysql&gt; SELECT * FROM shop WHERE price=@min_price OR price=@max_price; 使用AUTO_INCREMENT我们可以创建animals的表格： 12345CREATE TABLE animals ( id MEDIUMINT NOT NULL AUTO_INCREMENT, name CHAR(30) NOT NULL, PRIMARY KEY (id)); 然后在插入的时候可以不用设置id的值： 1INSERT INTO animals VALUES (NULL, &#x27;dog&#x27;), (NULL, &#x27;pig&#x27;); MySQL备份和恢复SQL文件格式备份全部数据库： 1shell&gt; mysqldump -u root -p --all-databases &gt; dump.sql 备份某些特定的数据库： 1shell&gt; mysqldump -u root -p --databases db1 db2 &gt; dump.sql 备份某个数据库的某些表格： 1shell&gt; mysqldump -u root -p db1 tb1 tb2 &gt; dump.sql 恢复文件内容： 1shell&gt; mysql -u root -p &lt; dump.sql 对于那些SQL文件中没有事先选择数据库的，可以先进入mysql，然后使用： 12mysql&gt; USE db1;mysql&gt; source dump.sql; 使用分割文本格式备份数据库： 1shell&gt; mysqldump -u root -p --tab=/dir_name db1 如果报错：Got error: 1290: The MySQL server is running with the –secure-file-priv option so it cannot execute this statement when executing ‘SELECT INTO OUTFILE’，我们可以使用如下方法来解决： mysql&gt; show global variables like &#39;%secure%&#39; 查看secure-file-priv对用的目录，然后将上面的dirname改为对应的目录就可以。 恢复文件内容： 12shell&gt; mysql -u root -p db1 &lt; t1.sqlshell&gt; mysqlimport db1 t1.txt 或者在mysql环境下： 12mysql&gt; USE db1;mysql&gt; LOAD DATA INFILE &#x27;t1.txt&#x27; INTO TABLE t1;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.zsstrike.tech/tags/MySQL/"}]},{"title":"CentOS 7安装常用软件方法","slug":"CentOS-7安装常用软件方法","date":"2020-02-18T08:53:47.000Z","updated":"2022-05-16T07:41:45.992Z","comments":true,"path":"2020/02/18/CentOS-7安装常用软件方法/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/18/CentOS-7%E5%AE%89%E8%A3%85%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E6%96%B9%E6%B3%95/","excerpt":"本文将会在CentOS 7的情况下安装一下常用的开发软件，主要记录在软件安装中遇到的问题和解决问题的方法。","text":"本文将会在CentOS 7的情况下安装一下常用的开发软件，主要记录在软件安装中遇到的问题和解决问题的方法。 概述由于国内的网络等原因，国外的一些资源或者被墙，或者是网络连接的速度慢，这个时候就需要我们使用镜像等网络资源来提高自己获取资源的速度。 实例MySQL 8.0安装CentOS 7中可能已经预安装了Mariadb，我们首先可以查询一下是否安装了Mariadb，如果安装了就直接卸载这个数据库： 12rpm -qa | grep mariadb*rpm -e --nodeps mariadb* 接下来下载MySQL官方的Yum Repository并且进行安装，注意具体的版本可以自己选择： 123wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm #根据版本选择rpm -ivh mysql-community-release-el7-5.noarch.rpmyum install mysql-server # 安装 但是由于网络原因，资源下载速率很慢，这个时候我们可以根据输出信息来决定下载的包。可以在清华镜像源中下载相应的包，然后按照依赖的关系依次安装。 成功安装完成后，我们使用systemctl start mysqld.service来启动MySQL，然后通过下面命令登录： 1mysql -u root -p # 无密码登录，输入密码行回车就行 进入到了mysql后，首先赋予用户密码： 12mysql&gt; ALTER user &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;123456&#x27;;mysql&gt; FLUSH PRIVILEGES; 如果执行第一步报错，说密码太简单：ERROR 1819 (HY000): Your password does not satisfy the current policy requirements。我们可以设置密码的规则： 12mysql&gt; set global validate_password.policy=0;mysql&gt; set global validate_password.length=1; 需要注意的是，在MySQL 5.7中应该按照下列方法设置： 12mysql&gt; set global validate_password_policy=0;mysql&gt; set global validate_password_length=1; Python 3.7安装在CentOS 7中，安装Python 3.7的步骤通常如下： 123456789101112131415# 安装相关编译工具yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel# 下载安装包并且解压wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tar.xztar -xvJf Python-3.7.0.tar.xz# 编译安装cd Python-3.7.0./configuremake &amp;&amp; make install# 检验是否成功安装python3 -Vpip3 -V 问题的关键点在于python.org被GFW墙了，根本不能下载Python源码。为此，我们可以在淘宝镜像上先下载源码包，然后按照上述方法安装就行。 pip2安装CentOS 7中默认安装了Python 2.7，但是没有预安装pip2命令，使用下面的方法安装就行： 123456# 先安装EPEL(Extra Packages for Enterprise Linux)源yum -y install epel-release# 接下来安装pip2yum install python-pip# 检验安装是否成功pip2 -V 总结遇到外网下载资源不佳的情况下，可以考虑使用国内的镜像源，根据自己下载的软件版本和系统的架构选择相应的软件下载下来，然后编译安装就行。","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.zsstrike.tech/tags/Linux/"},{"name":"MySQL","slug":"MySQL","permalink":"http://blog.zsstrike.tech/tags/MySQL/"},{"name":"Python","slug":"Python","permalink":"http://blog.zsstrike.tech/tags/Python/"}]},{"title":"Nginx 使用教程","slug":"Nginx-使用教程","date":"2020-02-17T04:27:05.000Z","updated":"2022-05-16T07:41:46.071Z","comments":true,"path":"2020/02/17/Nginx-使用教程/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/17/Nginx-%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/","excerpt":"本章学习如何在 CentOS 7下使用 Nginx 来搭建反向代理和配置动静分离以及负载均衡过程的步骤。","text":"本章学习如何在 CentOS 7下使用 Nginx 来搭建反向代理和配置动静分离以及负载均衡过程的步骤。 Nginx 基本概念Nginx简介Nginx是以一个高性能的HTTP和反向代理服务器，特点是内存占用小，并发能力强，事实上Nginx的并发能力确实在同类型的网页服务器中表现良好。Nginx专为性能优化而开发，性能是其最重要的考量，实现上非常注重效率，能够经受高负载的考验，有报告表明它能支持高达50000个并发连接数。 反向代理在介绍反向代理之前，我们先介绍一下正向代理。正向代理是一个位于客户端和目标服务器之间的代理服务器(中间服务器)。为了从原始服务器取得内容，客户端向代理服务器发送一个请求，并且指定目标服务器，之后代理向目标服务器转交并且将获得的内容返回给客户端。正向代理的情况下客户端必须要进行一些特别的设置才能使用。正向代理实际上代理的是用户。 反向代理正好相反。对于客户端来说，反向代理就好像目标服务器。并且客户端不需要进行任何设置。客户端向反向代理发送请求，接着反向代理判断请求走向何处，并将请求转交给客户端，使得这些内容就好似他自己一样，一次客户端并不会感知到反向代理后面的服务，也因此不需要客户端做任何设置，只需要把反向代理服务器当成真正的服务器就好了。 负载均衡在因特网中，用户对服务器的访问并发量是很高的，通常单个服务器不可能完成对用户的响应。此时我们可以增加服务器的数量，然后将请求分发到各个服务器上，将原来请求集中到单个服务器上的情况改为将请求分发到多个服务器上，将负载分发到不同的服务器上，这也就是通常所说的负载均衡。 动静分离动静分离是指在web服务器架构中，将静态页面与动态页面或者静态内容接口和动态内容接口分开不同系统访问的架构设计方法，进而提升整个服务访问性能和可维护性。 Nginx基本使用Nginx安装安装Nginx需要先安装该软件的依赖：pcre，openssl，zlib，最后安装Nginx。可通过如下命令安装： 1yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel 接下来安装Nginx。首先下载nginx的源码包： 1wget http://nginx.org/download/nginx-1.16.2.tar.gz 接下来，解压配置编译安装就行： 1234tar -zxvf nginx-1.16.2.tar.gzcd nginx-1.16.2./configuremake &amp;&amp; make install 至此，Nginx安装成功，转到&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin目录下，查看版本号： 1./nginx -v Nginx常用命令 启动Nginx：./nginx 如果没有修改Nginx的配置文件，我们现在就可以在内网中通过访问该服务器的地址获得Nginx的主页，如果没有看到Nginx的主页，此时需要配置防火墙开放80端口。 查看开放的端口：firewall-cmd --list-all 设置开放的端口：firewall-cmd --add-port=80/tcp --permanent 设置开放的服务：firewall-cmd --add-service=http --permanent 设置之后重启防火墙：firewall-cmd --reload 停止Nginx：./nginx -s stop 重加载Nginx：./nginx -s reload Nginx配置文件配置文件位于&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf，配置文件可以划分为三个部分： 全局块：配置服务器整体运行的配置指令 从配置文件开始到events块之间的内容，主要会设置一些影响 nginx 服务器整体运行的配置指令，主要包括配置运行 Nginx 服务器的用户（组）、允许生成的 worker process 数，进程 PID 存放路径、日志存放路径和类型以及配置文件的引入等。 上面的第一行配置的就是worker进程的数目，进程数目越大，相应的并发能力也就也强，通常将它的值设置为CPU的核数目。 events块：影响 Nginx 服务器与用户的网络连接 events 块涉及的指令主要影响 Nginx 服务器与用户的网络连接，常用的设置包括是否开启对多 worker进程下的网络连接进行序列化，是否允许同时接收多个网络连接，选取哪种事件驱动模型来处理连接请求，每个 worker进程可以同时支持的最大连接数等。 上面的例子表示的是一个worker进程支持的最大连接数。 http块 这算是 Nginx 服务器配置中最频繁的部分，代理、缓存和日志定义等绝大多数功能和第三方模块的配置都在这里。需要注意的是http块也可以进一步划分为http全局块核server块。 http全局块：http 全局块配置的指令包括文件引入、MIME-TYPE 定义、日志自定义、连接超时时间、单链接请求数上限等。 server块：这块和虚拟主机有密切关系，虚拟主机从用户角度看，和一台独立的硬件主机是完全一样的，该技术的产生是为了节省互联网服务器硬件成本。每个 http 块可以包括多个 server 块，而每个 server 块就相当于一个虚拟主机。而每个 server 块也分为全局 server 块，以及可以同时包含多个 locaton 块。 全局server块：最常见的配置是本虚拟机主机的监听配置和本虚拟主机的名称或 IP 配置。 location块：一个 server 块可以配置多个 location 块。这块的主要作用是基于 Nginx 服务器接收到的请求字符串（例如 server_name&#x2F;uri-string），对虚拟主机名称（也可以是 IP 别名）之外的字符串（例如 前面的 &#x2F;uri-string）进行匹配，对特定的请求进行处理。地址定向、数据缓存和应答控制等功能，还有许多第三方模块的配置也在这里进行。 Nginx配置实例反向代理 1 实现效果：打开浏览器，在浏览器地址栏输入地址server.test.com，跳转到 liunx 系统 tomcat 主页 面中。 准备工作 在CentOS中安装tomcat，使用默认的端口8080启动服务，即进入tomcat的bin目录中，运行./startup.sh启动tomcat服务器 修改防火墙，使其对外开放8080端口： 12firewall-cmd --add-port=8080/tcp --permanentfirewall-cmd --reload 在windows中通过浏览器ip:8080访问tomcat服务器 具体配置 打开windows的host文件，添加192.168.85.129 server.test.com。 在nginx进行请求转发的配置 接着重启以下nginx：./nginx -s reload。然后在浏览器中输入server.test.com，就可以得到tomcat的网页： 反向代理2 实现效果：使用nginx反向代理，根据访问的路径跳转到不同端口的服务中，nginx监听端口9001，当访问ip:9001/edu直接跳转到127.0.0.1:8080，当访问ip:9001/vod，直接跳转到127.0.0.1:8081。 准备工作 准备两个tomcat服务器，一个配置在8080端口，一个配置在8081端口。修改conf&#x2F;server.xml里面的两个端口，使得两个tomcat能够同时运行起来。 创建文件夹和测试文件。创建的文件夹和文件等会在被解析的时候用到。 具体配置 找到nginx配置文件，进行反向代理配置： 防火墙对外开放9001，8080和8081端口。 测试如下： 负载均衡 实现效果：浏览器地址栏输入地址ip:9002/edu/，使得后台的服务器均匀负载，将请求平均分发到8080和8081端口的服务器上。 准备工作 准备两台服务器，一台8080，另外一台8081 在两台tomcat服务器的webapps目录，创建edu文件夹，同时在edu文件里面创建index.html 具体配置 找到nginx配置文件，进行负载均衡的配置 防火墙开放9002端口。 测试如下： Nginx分配服务器策略 轮询（默认）：每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器 down 掉，能自动剔除。 权重（weight）：weight 代表权重默认为 1,权重越高被分配的客户端越多。 ip_hash：每个请求按访问 ip 的 hash 结果分配，这样每个访客固定访问一个后端服务器。 fair：按后端服务器的响应时间来分配请求，响应时间短的优先分配。 动静分离 动静分离：Nginx 动静分离简单来说就是把动态跟静态请求分开，不能理解成只是单纯的把动态页面和静态页面物理分离。严格意义上说应该是动态请求跟静态请求分开，可以理解成使用 Nginx处理静态页面，Tomcat 处理动态页面。动静分离从目前实现角度来讲大致分为两种，一种是纯粹把静态文件独立成单独的域名，放在独立的服务器上，也是目前主流推崇的方案；另外一种方法就是动态跟静态文件混合在一起发布，通过 nginx 来分开。通过 location 指定不同的后缀名实现不同的请求转发。通过 expires 参数设置，可以使浏览器缓存过期时间，减少与服务器之前的请求和流量。具体 Expires 定义：是给一个资源设定一个过期时间，也就是说无需去服务端验证，直接通过浏览器自身确认是否过期即可，所以不会产生额外的流量。此种方法非常适合不经常变动的资源。（如果经常更新的文件，不建议使用 Expires 来缓存），我这里设置 3d，表示在这 3 天之内访问这个 URL，发送一个请求，比对服务器该文件最后更新时间没有变化，则不会从服务器抓取，返回状态码 304，如果有修改，则直接从服务器重新下载，返回状态码 200。 准备工作：在CentOS中，创建&#x2F;static&#x2F;www，&#x2F;static&#x2F;image文件夹，接着放入静态文件。 Nginx配置： autoindex能够为目录下的文件自动创建索引。 效果： 高可用集群下面将使用keepalived实现服务器的高可用性。详细的讲解见Nginx原理一节。 两台Nginx服务器 准备两台服务器，在此地址是192.168.85.129和192.168.85.130。然后再在两台服务器上安装Nginx软件。 keepalived软件 使用以下命令： 1yum install keepalived -y 安装完成后，可以在&#x2F;etc&#x2F;keepalived中找到配置文件。 完成高可用的配置 修改&#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf 1234567891011121314151617181920212223242526! Configuration File for keepalivedglobal_defs &#123; router_id LVS_DEVEL&#125;vrrp_script chk_http_port &#123; script &quot;/usr/local/src/nginx_check.sh&quot; interval 2 weight 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens33 virtual_router_id 51 priority 90 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; track_script &#123; chk_http_port &#125; virtual_ipaddress &#123; 192.168.85.120 &#125;&#125; 在&#x2F;usr&#x2F;local&#x2F;src&#x2F;中添加nginx_check.sh文件 123456789#!/bin/bashA=`ps -C nginx --no-header | wc -l`if [ $A -eq 0 ];then /usr/local/nginx/sbin/nginx sleep 2 if [`ps -C nginx --no-header | wc -l` -eq 0];then killall keepalived fifi 测试 启动nginx：./nginx 启动keepalived：systemctl start keepalived.service 关闭主服务器上的nginx，再次访问虚拟地址 Nginx原理Nginx在启动时会以daemon形式在后台运行，采用多进程+异步非阻塞IO事件模型来处理各种连接请求。多进程模型包括一个master进程，多个worker进程，一般worker进程个数是根据服务器CPU核数来决定的。master进程负责管理Nginx本身和其他worker进程。 Master进程作用是读取并验证配置文件nginx.conf，管理worker进程；每一个Worker进程都维护一个线程（避免线程切换），处理连接和请求；注意Worker进程的个数由配置文件决定，一般和CPU个数相关（有利于进程切换），配置几个就有几个Worker进程。 Nginx热部署的方式：修改配置文件nginx.conf后，重新生成新的worker进程，当然会以新的配置进行处理请求，而且新的请求必须都交给新的worker进程，至于老的worker进程，等把那些以前的请求处理完毕后，kill掉即可。 Nginx的高并发：Nginx采用了Linux的epoll模型，epoll模型基于事件驱动机制，它可以监控多个事件是否准备完毕，如果OK，那么放入epoll队列中，这个过程是异步的。worker只需要从epoll队列循环处理即可。 Nginx的高可用性：Keepalived是一个高可用解决方案，主要是用来防止服务器单点发生故障，可以通过和Nginx配合来实现Web服务的高可用（其实，Keepalived不仅仅可以和Nginx配合，还可以和很多其他服务配合）。Keepalived+Nginx实现高可用的思路：第一：请求不要直接打到Nginx上，应该先通过Keepalived（这就是所谓虚拟IP，VIP）第二：Keepalived应该能监控Nginx的生命状态（提供一个用户自定义的脚本，定期检查Nginx进程状态，进行权重变化,，从而实现Nginx故障切换）","categories":[],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.zsstrike.tech/tags/Nginx/"}]},{"title":"训练和评估","slug":"训练和评估","date":"2020-02-15T04:18:10.000Z","updated":"2022-05-16T07:41:46.369Z","comments":true,"path":"2020/02/15/训练和评估/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/15/%E8%AE%AD%E7%BB%83%E5%92%8C%E8%AF%84%E4%BC%B0/","excerpt":"本节主要从两方面学习模型的训练和评估：使用内建的API进行训练和评估或者是自定义函数实现训练和评估。不管使用哪种方法，不同方式构建的模型的训练和评估方式是一样的。","text":"本节主要从两方面学习模型的训练和评估：使用内建的API进行训练和评估或者是自定义函数实现训练和评估。不管使用哪种方法，不同方式构建的模型的训练和评估方式是一样的。 使用内建API 当我们使用内建的API来进行训练和评估时，我们传入的数据必须是Numpy arrays或者是tf.data.Dataset对象，在接下来的几个小节里，我们将会使用MNIST数据集作为示例。 内建API总览首先创建一个模型，如下： 123456789from tensorflow import kerasfrom tensorflow.keras import layersinputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x)outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x)model = keras.Model(inputs=inputs, outputs=outputs) 接下来，我们定义一个如下一个数据集： 1234567891011121314(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()# Preprocess the data (these are Numpy arrays)x_train = x_train.reshape(60000, 784).astype(&#x27;float32&#x27;) / 255x_test = x_test.reshape(10000, 784).astype(&#x27;float32&#x27;) / 255y_train = y_train.astype(&#x27;float32&#x27;)y_test = y_test.astype(&#x27;float32&#x27;)# Reserve 10,000 samples for validationx_val = x_train[-10000:]y_val = y_train[-10000:]x_train = x_train[:-10000]y_train = y_train[:-10000] 然后配置训练参数： 12345model.compile(optimizer=keras.optimizers.RMSprop(), # Optimizer # Loss function to minimize loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), # List of metrics to monitor metrics=[&#x27;sparse_categorical_accuracy&#x27;]) 接下来按照小批次的数目（batch_size）来训练这个模型，迭代整个数据集次数通过epochs设置： 1234567891011print(&#x27;# Fit model on training data&#x27;)history = model.fit(x_train, y_train, batch_size=64, epochs=3, # We pass some validation for # monitoring validation loss and metrics # at the end of each epoch validation_data=(x_val, y_val))print(&#x27;\\nhistory dict:&#x27;, history.history)&gt;&gt; history dict: &#123;&#x27;loss&#x27;: [0.34013055738687514, 0.15638909303188325, 0.11687878879904746], &#x27;sparse_categorical_accuracy&#x27;: [0.90308, 0.95404, 0.96512], &#x27;val_loss&#x27;: [0.18770194243788718, 0.13478265590667723, 0.11865641107037664], &#x27;val_sparse_categorical_accuracy&#x27;: [0.9454, 0.9615, 0.9672]&#125; 返回的对象记录了训练过程中损失值（loss value）和度量值（metrics）。下面的代码用于评估和预测： 12345678910# Evaluate the model on the test data using `evaluate`print(&#x27;\\n# Evaluate on test data&#x27;)results = model.evaluate(x_test, y_test, batch_size=128)print(&#x27;test loss, test acc:&#x27;, results)# Generate predictions (probabilities -- the output of the last layer)# on new data using `predict`print(&#x27;\\n# Generate predictions for 3 samples&#x27;)predictions = model.predict(x_test[:3])print(&#x27;predictions shape:&#x27;, predictions.shape) 定义损失函数，评价指标和优化器为了训练模型，我们需要定义损失函数，评价指标和优化器。我们可以再模型编译期间传入这些参数： 123model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#x27;sparse_categorical_accuracy&#x27;]) 注意，metrics参数必须是一个列表，可以传入多个评价指标。对于含有多个输出的模型，我们可以分别为其定义损失函数，评价指标和优化器。同时，一些默认的参数值我们也可以使用字符串。为了重用，我们定义如下代码： 1234567891011121314def get_uncompiled_model(): inputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;) x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs) x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x) outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x) model = keras.Model(inputs=inputs, outputs=outputs) return modeldef get_compiled_model(): model = get_uncompiled_model() model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#x27;sparse_categorical_accuracy&#x27;]) return model 内建的损失函数，评价指标和优化器内建优化器：SGD，RMSprop，Adam；内建损失函数：MeanSquareError，KLDivergence，CosineSimilarity；内建评估指标：AUC，Precision，Recall。 自定义损失函数有两种方法来自定义我们的损失函数，第一种是定义一个函数，接受y_true和y_pred参数： 1234567def basic_loss_function(y_true, y_pred): return tf.math.reduce_mean(tf.abs(y_true - y_pred))model.compile(optimizer=keras.optimizers.Adam(), loss=basic_loss_function)model.fit(x_train, y_train, batch_size=64, epochs=3) 另外一种方法是构造tf.keras.losses.Loss的子类，并且实现以下两个方法： __init__：接受传向损失函数的参数 call(self, y_true, y_pred)：用于计算模型的损失 传向__init__的参数可以被call方法调用。以下方法实现实现了BinaryCrossEntropy损失函数： 12345678910111213141516171819202122class WeightedBinaryCrossEntropy(keras.losses.Loss): &quot;&quot;&quot; Args: pos_weight: Scalar to affect the positive labels of the loss function. weight: Scalar to affect the entirety of the loss function. from_logits: Whether to compute loss from logits or the probability. reduction: Type of tf.keras.losses.Reduction to apply to loss. name: Name of the loss function. &quot;&quot;&quot; def __init__(self, pos_weight, weight, from_logits=False, reduction=keras.losses.Reduction.AUTO, name=&#x27;weighted_binary_crossentropy&#x27;): super().__init__(reduction=reduction, name=name) self.pos_weight = pos_weight self.weight = weight self.from_logits = from_logits def call(self, y_true, y_pred): ce = tf.losses.binary_crossentropy( y_true, y_pred, from_logits=self.from_logits)[:,None] ce = self.weight * (ce*(1-y_true) + self.pos_weight*ce*(y_true)) return ce 由于数据集由10个类别，但我们使用的是二元损失，所以我们只考虑每个类别的预测，这样就能基于二元损失来计算了。首先创建独热码： 1one_hot_y_train = tf.one_hot(y_train.astype(np.int32), depth=10) 接下俩训练模型： 123456789model = get_uncompiled_model()model.compile( optimizer=keras.optimizers.Adam(), loss=WeightedBinaryCrossEntropy( pos_weight=0.5, weight = 2, from_logits=True))model.fit(x_train, one_hot_y_train, batch_size=64, epochs=5) 自定义评估指标可以通过创建Metric来实现自定义的评价指标，需要实现下列四种方法： __init__：用于创建状态变量 update_state(self, y_true, y_pred, sample_weight=None)：用于更新状态 result(self)：使用状态变量计算最终结果 reset_states(self)：重新初始化状态 下面是一个实现了CategoricalTruePositive评价指标： 123456789101112131415161718192021class CategoricalTruePositives(keras.metrics.Metric): def __init__(self, name=&#x27;categorical_true_positives&#x27;, **kwargs): super(CategoricalTruePositives, self).__init__(name=name, **kwargs) self.true_positives = self.add_weight(name=&#x27;tp&#x27;, initializer=&#x27;zeros&#x27;) def update_state(self, y_true, y_pred, sample_weight=None): y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1)) values = tf.cast(y_true, &#x27;int32&#x27;) == tf.cast(y_pred, &#x27;int32&#x27;) values = tf.cast(values, &#x27;float32&#x27;) if sample_weight is not None: sample_weight = tf.cast(sample_weight, &#x27;float32&#x27;) values = tf.multiply(values, sample_weight) self.true_positives.assign_add(tf.reduce_sum(values)) def result(self): return self.true_positives def reset_states(self): # The state of the metric will be reset at the start of each epoch. self.true_positives.assign(0.) 下面是使用评价指标的代码： 123456model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[CategoricalTruePositives()])model.fit(x_train, y_train, batch_size=64, epochs=3) 处理非常规的损失函数和评价指标可以通过y_pred和y_true来计算损失函数和评价指标，然而，并非对所有的损失函数和评价指标都是如此。比如，一个正则化的损失函数可能需要某个层的激励值，而这个激励值并非是模型的输出。处理此类问题，我们可以再自定义层中的call方法中加入self.add_loss(loss_value): 123456789101112131415161718192021222324class ActivityRegularizationLayer(layers.Layer): def call(self, inputs): self.add_loss(tf.reduce_sum(inputs) * 0.1) return inputs # Pass-through layer.inputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs)# Insert activity regularization as a layerx = ActivityRegularizationLayer()(x)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x)outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x)model = keras.Model(inputs=inputs, outputs=outputs)model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))# The displayed loss will be much higher than before# due to the regularization component.model.fit(x_train, y_train, batch_size=64, epochs=1) 同样，对于评价指标也是如此： 12345678910111213141516171819202122232425262728class MetricLoggingLayer(layers.Layer): def call(self, inputs): # The `aggregation` argument defines # how to aggregate the per-batch values # over each epoch: # in this case we simply average them. self.add_metric(keras.backend.std(inputs), name=&#x27;std_of_activation&#x27;, aggregation=&#x27;mean&#x27;) return inputs # Pass-through layer.inputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs)# Insert std logging as a layer.x = MetricLoggingLayer()(x)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x)outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x)model = keras.Model(inputs=inputs, outputs=outputs)model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))model.fit(x_train, y_train, batch_size=64, epochs=1) 再函数式API中，我们可以通过model.add_loss(loss_tensor)和model.add_metric(metric_tensor, name, aggregation)来实现： 1234567891011121314151617inputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;)x1 = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs)x2 = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x1)outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x2)model = keras.Model(inputs=inputs, outputs=outputs)model.add_loss(tf.reduce_sum(x1) * 0.1)model.add_metric(keras.backend.std(x1), name=&#x27;std_of_activation&#x27;, aggregation=&#x27;mean&#x27;)model.compile(optimizer=keras.optimizers.RMSprop(1e-3), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))model.fit(x_train, y_train, batch_size=64, epochs=1) 自动设置验证集再第一个实例中，我们使用validation_data来手动设置验证集。其实我们还可以使用validation_split参数来定义我们验证集的比例，需要注意的是，验证集在fit之前选取原数据集的前$ x% $比例的数据作为验证集。validation_split参数只能在训练Numpy数据集时使用： 12model = get_compiled_model()model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1, steps_per_epoch=1) 从Datasets中训练和评估在TF2中，tf.data下的API用于加载数据和数据预处理。我们可以直接将Dataset的实例传给fit,evaluate,predoct等函数： 1234567891011121314151617181920model = get_compiled_model()# First, let&#x27;s create a training Dataset instance.# For the sake of our example, we&#x27;ll use the same MNIST data as before.train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))# Shuffle and slice the dataset.train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)# Now we get a test dataset.test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))test_dataset = test_dataset.batch(64)# Since the dataset already takes care of batching,# we don&#x27;t pass a `batch_size` argument.model.fit(train_dataset, epochs=3)# You can also evaluate or predict on a dataset.print(&#x27;\\n# Evaluate&#x27;)result = model.evaluate(test_dataset)dict(zip(model.metrics_names, result)) 注意Dataset在每次迭代结束后都会被重置，以此让我们在下次迭代中可以重新使用。如果我们想要定义每次迭代的步数，我们可以使用take参数。在达到指定的步数时，Dataset不会重置，除非它已经被遍历完了： 12345678model = get_compiled_model()# Prepare the training datasettrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)# Only use the 100 batches per epoch (that&#x27;s 64 * 100 samples)model.fit(train_dataset.take(100), epochs=3) 使用测试数据集可以给fit函数传入validation_data: 1234567891011model = get_compiled_model()# Prepare the training datasettrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)# Prepare the validation datasetval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))val_dataset = val_dataset.batch(64)model.fit(train_dataset, epochs=3, validation_data=val_dataset) 同样，如果我们定义每次迭代时使用验证集的次数，可以定义validation_steps: 1234567891011121314model = get_compiled_model()# Prepare the training datasettrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)# Prepare the validation datasetval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))val_dataset = val_dataset.batch(64)model.fit(train_dataset, epochs=3, # Only run validation using the first 10 batches of the dataset # using the `validation_steps` argument validation_data=val_dataset, validation_steps=10) 注意，此时不管测试数据集是否遍历完，都会被重置。 其他输入格式的数据除了Numpy中的数组和TF2中的Dataset对象，我们还可以使用Pandas的dataframs，或者是Python的generator（能够yield小批次数据）。 总体来说，对于少量数据，可以在内存中保存的，推荐使用Numpy中array，否则使用TF2中Dataset对象。 使用样本权重和类标权重我们可以在使用fit方法的时候传入样本的权重和类标的权重： 当使用Numpy数据时：通过sample_weight和class_weight参数 当使用TF2的Dataset时：让其返回一个这样的元组：(input_batch, target_batch, sample_weight_batch) 下面是使用Numpy数据进行训练的例子： 1234567891011121314151617181920212223import numpy as npclass_weight = &#123;0: 1., 1: 1., 2: 1., 3: 1., 4: 1., # Set weight &quot;2&quot; for class &quot;5&quot;, # making this class 2x more important 5: 2., 6: 1., 7: 1., 8: 1., 9: 1.&#125;print(&#x27;Fit with class weight&#x27;)model.fit(x_train, y_train, class_weight=class_weight, batch_size=64, epochs=4)# Here&#x27;s the same example using `sample_weight` instead:sample_weight = np.ones(shape=(len(y_train),))sample_weight[y_train == 5] = 2.print(&#x27;\\nFit with sample weight&#x27;)model = get_compiled_model()model.fit(x_train, y_train, sample_weight=sample_weight, batch_size=64, epochs=4) 下面是使用Dataset数据集的例子： 12345678910111213sample_weight = np.ones(shape=(len(y_train),))sample_weight[y_train == 5] = 2.# Create a Dataset that includes sample weights# (3rd element in the return tuple).train_dataset = tf.data.Dataset.from_tensor_slices( (x_train, y_train, sample_weight))# Shuffle and slice the dataset.train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)model = get_compiled_model()model.fit(train_dataset, epochs=3) 将数据传入多输入输出模型考虑这样一个模型： 12345678910111213141516171819from tensorflow import kerasfrom tensorflow.keras import layersimage_input = keras.Input(shape=(32, 32, 3), name=&#x27;img_input&#x27;)timeseries_input = keras.Input(shape=(None, 10), name=&#x27;ts_input&#x27;)x1 = layers.Conv2D(3, 3)(image_input)x1 = layers.GlobalMaxPooling2D()(x1)x2 = layers.Conv1D(3, 3)(timeseries_input)x2 = layers.GlobalMaxPooling1D()(x2)x = layers.concatenate([x1, x2])score_output = layers.Dense(1, name=&#x27;score_output&#x27;)(x)class_output = layers.Dense(5, name=&#x27;class_output&#x27;)(x)model = keras.Model(inputs=[image_input, timeseries_input], outputs=[score_output, class_output]) 这个模型有两个输入两个输出。在模型编译期间，我们传入不同的损失函数： 1234model.compile( optimizer=keras.optimizers.RMSprop(1e-3), loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy(from_logits=True)]) 同样可以传入不同的评价指标： 1234567model.compile( optimizer=keras.optimizers.RMSprop(1e-3), loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy(from_logits=True)], metrics=[[keras.metrics.MeanAbsolutePercentageError(), keras.metrics.MeanAbsoluteError()], [keras.metrics.CategoricalAccuracy()]]) 由于我们已经为输出层赋予了 名字，我们可以使用字典的方式传递参数： 1234567model.compile( optimizer=keras.optimizers.RMSprop(1e-3), loss=&#123;&#x27;score_output&#x27;: keras.losses.MeanSquaredError(), &#x27;class_output&#x27;: keras.losses.CategoricalCrossentropy(from_logits=True)&#125;, metrics=&#123;&#x27;score_output&#x27;: [keras.metrics.MeanAbsolutePercentageError(), keras.metrics.MeanAbsoluteError()], &#x27;class_output&#x27;: [keras.metrics.CategoricalAccuracy()]&#125;) 同样的，我们可以定义损失权重： 12345678model.compile( optimizer=keras.optimizers.RMSprop(1e-3), loss=&#123;&#x27;score_output&#x27;: keras.losses.MeanSquaredError(), &#x27;class_output&#x27;: keras.losses.CategoricalCrossentropy(from_logits=True)&#125;, metrics=&#123;&#x27;score_output&#x27;: [keras.metrics.MeanAbsolutePercentageError(), keras.metrics.MeanAbsoluteError()], &#x27;class_output&#x27;: [keras.metrics.CategoricalAccuracy()]&#125;, loss_weights=&#123;&#x27;score_output&#x27;: 2., &#x27;class_output&#x27;: 1.&#125;) 我们可以为某个输出定义损失函数，而另外一个输出不定义损失函数： 123456789# List loss versionmodel.compile( optimizer=keras.optimizers.RMSprop(1e-3), loss=[None, keras.losses.CategoricalCrossentropy(from_logits=True)])# Or dict loss versionmodel.compile( optimizer=keras.optimizers.RMSprop(1e-3), loss=&#123;&#x27;class_output&#x27;:keras.losses.CategoricalCrossentropy(from_logits=True)&#125;) 传入训练数据集的方式和上述介绍的方式差不多： 123456789101112131415161718192021model.compile( optimizer=keras.optimizers.RMSprop(1e-3), loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy(from_logits=True)])# Generate dummy Numpy dataimg_data = np.random.random_sample(size=(100, 32, 32, 3))ts_data = np.random.random_sample(size=(100, 20, 10))score_targets = np.random.random_sample(size=(100, 1))class_targets = np.random.random_sample(size=(100, 5))# Fit on listsmodel.fit([img_data, ts_data], [score_targets, class_targets], batch_size=32, epochs=3)# Alternatively, fit on dictsmodel.fit(&#123;&#x27;img_input&#x27;: img_data, &#x27;ts_input&#x27;: ts_data&#125;, &#123;&#x27;score_output&#x27;: score_targets, &#x27;class_output&#x27;: class_targets&#125;, batch_size=32, epochs=3) 而Dataset的使用方式如下： 123456train_dataset = tf.data.Dataset.from_tensor_slices( (&#123;&#x27;img_input&#x27;: img_data, &#x27;ts_input&#x27;: ts_data&#125;, &#123;&#x27;score_output&#x27;: score_targets, &#x27;class_output&#x27;: class_targets&#125;))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)model.fit(train_dataset, epochs=3) 使用回调回调对象可以在不同的时间点（每轮迭代的开始，每个批次的结束，每个迭代的结束）被调用来实现不同的功能。回调对象可以被传入fit: 1234567891011121314151617model = get_compiled_model()callbacks = [ keras.callbacks.EarlyStopping( # Stop training when `val_loss` is no longer improving monitor=&#x27;val_loss&#x27;, # &quot;no longer improving&quot; being defined as &quot;no better than 1e-2 less&quot; min_delta=1e-2, # &quot;no longer improving&quot; being further defined as &quot;for at least 2 epochs&quot; patience=2, verbose=1)]model.fit(x_train, y_train, epochs=20, batch_size=64, callbacks=callbacks, validation_split=0.2) 内建的回调对象 ModelCheckpoint: 定时保存模型检查点 EarlyStopping: 当评估指数没有改进的时候提前停止 TensorBoard: 记录模型的参数值 CSVLogger: 将模型的损失值和评价指标保存到CSV文件中 自建回调对象我们可以通过继承Callback对象实现自定义的回调对象，回调对象可以通过self.model获取相关联的模型，下面是一个实例： 1234567class LossHistory(keras.callbacks.Callback): def on_train_begin(self, logs): self.losses = [] def on_batch_end(self, batch, logs): self.losses.append(logs.get(&#x27;loss&#x27;)) 保存模型的检查点当我们的训练集很大的时候，我们需要定期保存模型的检查点，最简单的方式是使用ModelCheckpoint回调： 123456789101112131415161718model = get_compiled_model()callbacks = [ keras.callbacks.ModelCheckpoint( filepath=&#x27;mymodel_&#123;epoch&#125;&#x27;, # Path where to save the model # The two parameters below mean that we will overwrite # the current checkpoint if and only if # the `val_loss` score has improved. save_best_only=True, monitor=&#x27;val_loss&#x27;, verbose=1)]model.fit(x_train, y_train, epochs=3, batch_size=64, callbacks=callbacks, validation_split=0.2) 使用学习速率表深度学习中一个常见的训练模式是递减我们的学习速率，学习速率递减可以实静态的或者是动态的。 将学习速率表传给优化器我们可以将一个静态的学习速率表通过参数传递给优化器： 12345678initial_learning_rate = 0.1lr_schedule = keras.optimizers.schedules.ExponentialDecay( initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True)optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule) 内建的学习速率表还有：ExponentialDecay, PiecewiseConstantDecay, PolynomialDecay, and InverseTimeDecay。 使用回调实现动态学习速率表由于优化器不能获取评估指标，所以动态的学习速率表不能通过内建的学习速率表实现。但是，我们可以通过回调来实现动态学习速率表，因为回调能够获取所有的评价指标。实际上，这已经内建在ReduceLROnPlateau 这个回调中了。 可视化损失和评估值最好的方法是使用TensorBoard，它可以帮助我们实时可视化损失值和评估值。启动Tensorboard的方法如下： 1tensorboard --logdir=/full_path_to_your_logs 使用Tensorboard回调最简单的方法就是在fit的时候传入Tensorboard回调： 12tensorboard_cbk = keras.callbacks.TensorBoard(log_dir=&#x27;/full_path_to_your_logs&#x27;)model.fit(dataset, epochs=10, callbacks=[tensorboard_cbk]) Tensorboard还有一些其他参数可供选择： 12345keras.callbacks.TensorBoard( log_dir=&#x27;/full_path_to_your_logs&#x27;, histogram_freq=0, # How often to log histogram visualizations embeddings_freq=0, # How often to log embedding visualizations update_freq=&#x27;epoch&#x27;) # How often to write logs (default: once per epoch) 编写自己的训练和评估方法使用GradientTape在GradientTape作用域中调用模型会使你很容易得到相关参数的梯度值。下面是一个MNIST模型： 12345678910111213141516# Get the model.inputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x)outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x)model = keras.Model(inputs=inputs, outputs=outputs)# Instantiate an optimizer.optimizer = keras.optimizers.SGD(learning_rate=1e-3)# Instantiate a loss function.loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)# Prepare the training dataset.batch_size = 64train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size) 训练方法如下： 1234567891011121314151617181920212223242526272829303132epochs = 3for epoch in range(epochs): print(&#x27;Start of epoch %d&#x27; % (epoch,)) # Iterate over the batches of the dataset. for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): # Open a GradientTape to record the operations run # during the forward pass, which enables autodifferentiation. with tf.GradientTape() as tape: # Run the forward pass of the layer. # The operations that the layer applies # to its inputs are going to be recorded # on the GradientTape. logits = model(x_batch_train, training=True) # Logits for this minibatch # Compute the loss value for this minibatch. loss_value = loss_fn(y_batch_train, logits) # Use the gradient tape to automatically retrieve # the gradients of the trainable variables with respect to the loss. grads = tape.gradient(loss_value, model.trainable_weights) # Run one step of gradient descent by updating # the value of the variables to minimize the loss. optimizer.apply_gradients(zip(grads, model.trainable_weights)) # Log every 200 batches. if step % 200 == 0: print(&#x27;Training loss (for one batch) at step %s: %s&#x27; % (step, float(loss_value))) print(&#x27;Seen so far: %s samples&#x27; % ((step + 1) * 64)) 实现自定义评估值接着我们添加自定义的评估值，下面是工作流： 在每次迭代前初始化评价指标 在每个批次结束时调用metric.update_state 在需要展示结果的时候调用metric.result 在需要重置的时候（如每次迭代的末尾）调用metric.reset_states 接下来手动实现SparseCategoricalAccuracy 评价指标，下面是模型创建时的代码： 12345678910111213141516171819202122232425# Get modelinputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x)outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x)model = keras.Model(inputs=inputs, outputs=outputs)# Instantiate an optimizer to train the model.optimizer = keras.optimizers.SGD(learning_rate=1e-3)# Instantiate a loss function.loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)# Prepare the metrics.train_acc_metric = keras.metrics.SparseCategoricalAccuracy()val_acc_metric = keras.metrics.SparseCategoricalAccuracy()# Prepare the training dataset.batch_size = 64train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)# Prepare the validation dataset.val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))val_dataset = val_dataset.batch(64) 自定义训练的迭代如下： 12345678910111213141516171819202122232425262728293031323334epochs = 3for epoch in range(epochs): print(&#x27;Start of epoch %d&#x27; % (epoch,)) # Iterate over the batches of the dataset. for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): with tf.GradientTape() as tape: logits = model(x_batch_train) loss_value = loss_fn(y_batch_train, logits) grads = tape.gradient(loss_value, model.trainable_weights) optimizer.apply_gradients(zip(grads, model.trainable_weights)) # Update training metric. train_acc_metric(y_batch_train, logits) # Log every 200 batches. if step % 200 == 0: print(&#x27;Training loss (for one batch) at step %s: %s&#x27; % (step, float(loss_value))) print(&#x27;Seen so far: %s samples&#x27; % ((step + 1) * 64)) # Display metrics at the end of each epoch. train_acc = train_acc_metric.result() print(&#x27;Training acc over epoch: %s&#x27; % (float(train_acc),)) # Reset training metrics at the end of each epoch train_acc_metric.reset_states() # Run a validation loop at the end of each epoch. for x_batch_val, y_batch_val in val_dataset: val_logits = model(x_batch_val) # Update val metrics val_acc_metric(y_batch_val, val_logits) val_acc = val_acc_metric.result() val_acc_metric.reset_states() print(&#x27;Validation acc: %s&#x27; % (float(val_acc),)) 处理额外的损失值在前面的小节中我们在call方法中调用self.add_loss(values)来正则化损失值，通常来说我们需要将这些额外的损失值也考虑在内，下面是我们实现的其中一个模型： 123456789101112131415class ActivityRegularizationLayer(layers.Layer): def call(self, inputs): self.add_loss(1e-2 * tf.reduce_sum(inputs)) return inputsinputs = keras.Input(shape=(784,), name=&#x27;digits&#x27;)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_1&#x27;)(inputs)# Insert activity regularization as a layerx = ActivityRegularizationLayer()(x)x = layers.Dense(64, activation=&#x27;relu&#x27;, name=&#x27;dense_2&#x27;)(x)outputs = layers.Dense(10, name=&#x27;predictions&#x27;)(x)model = keras.Model(inputs=inputs, outputs=outputs) 当我们调用模型的时候： 1logits = model(x_train) 在前向传播过程中的损失值会被加到model.losses属性中。 为了将额外的损失值考虑在内，我们需要修改我们自定义的训练循环体中的代码： 123456789101112131415161718192021optimizer = keras.optimizers.SGD(learning_rate=1e-3)epochs = 3for epoch in range(epochs): print(&#x27;Start of epoch %d&#x27; % (epoch,)) for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): with tf.GradientTape() as tape: logits = model(x_batch_train) loss_value = loss_fn(y_batch_train, logits) # Add extra losses created during this forward pass: loss_value += sum(model.losses) grads = tape.gradient(loss_value, model.trainable_weights) optimizer.apply_gradients(zip(grads, model.trainable_weights)) # Log every 200 batches. if step % 200 == 0: print(&#x27;Training loss (for one batch) at step %s: %s&#x27; % (step, float(loss_value))) print(&#x27;Seen so far: %s samples&#x27; % ((step + 1) * 64))","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://blog.zsstrike.tech/tags/TensorFlow/"}]},{"title":"Keras函数式API","slug":"Keras函数式API","date":"2020-02-14T06:23:53.000Z","updated":"2022-05-16T07:41:45.992Z","comments":true,"path":"2020/02/14/Keras函数式API/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/14/Keras%E5%87%BD%E6%95%B0%E5%BC%8FAPI/","excerpt":"本章了解Keras的函数式API以及灵活使用它们的方法。","text":"本章了解Keras的函数式API以及灵活使用它们的方法。 简介我们已经熟悉了如何使用keras.Sequential函数来创建我们的层叠模型，而函数式API是比它更加灵活的创建模型的方法：它可以允许我们创建非线性的模型，共用层的模型以及多个输入输出的模型。函数式API的基本思路是深度学习网络是一种有向无环图（DAG），我们可以使用函数式API来创建这些层。 如下一个包含了3个层的模型： 123456789(input: 784-dimensional vectors) ↧[Dense (64 units, relu activation)] ↧[Dense (64 units, relu activation)] ↧[Dense (10 units, softmax activation)] ↧(output: logits of a probability distribution over 10 classes) 为了使用函数式API来创建相同的模型，我们首先创建输入节点： 123from tensorflow import kerasinputs = keras.Input(shape=(784,)) 我们声明了输入的数据是一个784维的向量，注意这里的shape是单个样本的shape，不是批次的shape。对于图片，假设数据是（32，32，3）类型的，我们可以使用以下代码： 12# Just for demonstration purposesimg_inputs = keras.Input(shape=(32, 32, 3)) 我们得到的返回值inputs包含了输入数据的shape和类型。为了创建层节点，我们使用如下方法： 1234from tensorflow.keras import layersdense = layers.Dense(64, activation=&#x27;relu&#x27;)x = dense(inputs) 调用层函数的作用相当于在两个节点之间画一条有向线，我们得到了经过第一层处理后的返回值x，接着，我们创建完剩余的层： 12x = layers.Dense(64, activation=&#x27;relu&#x27;)(x)outputs = layers.Dense(10)(x) 到了这一步，我们现在可以创建我们的模型了： 1model = keras.Model(inputs=inputs, outputs=outputs) 至此，我们的模型就创建成功了。 训练评估和预测训练评估和预测的使用方法其实和在Sequential中创建模型一致。下面是使用我们刚刚创建的模型进行训练评估和预测的代码： 1234567891011121314(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()x_train = x_train.reshape(60000, 784).astype(&#x27;float32&#x27;) / 255x_test = x_test.reshape(10000, 784).astype(&#x27;float32&#x27;) / 255model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=keras.optimizers.RMSprop(), metrics=[&#x27;accuracy&#x27;])history = model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.2)test_scores = model.evaluate(x_test, y_test, verbose=2)print(&#x27;Test loss:&#x27;, test_scores[0])print(&#x27;Test accuracy:&#x27;, test_scores[1]) 序列化同样地，使用函数式APiece创建出来的模型序列化和反序列化和使用Sequential创建的模型一致。最常用的方法是使用save方法，它会保存： 模型的架构 模型权重值（在训练中得到） 模型训练配置（在compile的时候得到） 模型优化配置 1234model.save(&#x27;path_to_my_model&#x27;)del model# Recreate the exact same model purely from the file:model = keras.models.load_model(&#x27;path_to_my_model&#x27;) 使用相同的层来创建多个模型在使用函数式API创建模型时，我们只需要声明模型的输入和输出即可，这也就意味着我们可以使用相同的层来创建多个模型，以下是一个实例： 1234567891011121314151617181920encoder_input = keras.Input(shape=(28, 28, 1), name=&#x27;img&#x27;)x = layers.Conv2D(16, 3, activation=&#x27;relu&#x27;)(encoder_input)x = layers.Conv2D(32, 3, activation=&#x27;relu&#x27;)(x)x = layers.MaxPooling2D(3)(x)x = layers.Conv2D(32, 3, activation=&#x27;relu&#x27;)(x)x = layers.Conv2D(16, 3, activation=&#x27;relu&#x27;)(x)encoder_output = layers.GlobalMaxPooling2D()(x)encoder = keras.Model(encoder_input, encoder_output, name=&#x27;encoder&#x27;)encoder.summary()x = layers.Reshape((4, 4, 1))(encoder_output)x = layers.Conv2DTranspose(16, 3, activation=&#x27;relu&#x27;)(x)x = layers.Conv2DTranspose(32, 3, activation=&#x27;relu&#x27;)(x)x = layers.UpSampling2D(3)(x)x = layers.Conv2DTranspose(16, 3, activation=&#x27;relu&#x27;)(x)decoder_output = layers.Conv2DTranspose(1, 3, activation=&#x27;relu&#x27;)(x)autoencoder = keras.Model(encoder_input, decoder_output, name=&#x27;autoencoder&#x27;)autoencoder.summary() 模型可调用我们可以将模型看作是特殊的层，因为它接收Input或者其他层的输出作为参数。注意，我们调用模型的时候不仅仅只是使用了它的架构，还使用了它的权重： 123456789101112131415161718192021222324252627encoder_input = keras.Input(shape=(28, 28, 1), name=&#x27;original_img&#x27;)x = layers.Conv2D(16, 3, activation=&#x27;relu&#x27;)(encoder_input)x = layers.Conv2D(32, 3, activation=&#x27;relu&#x27;)(x)x = layers.MaxPooling2D(3)(x)x = layers.Conv2D(32, 3, activation=&#x27;relu&#x27;)(x)x = layers.Conv2D(16, 3, activation=&#x27;relu&#x27;)(x)encoder_output = layers.GlobalMaxPooling2D()(x)encoder = keras.Model(encoder_input, encoder_output, name=&#x27;encoder&#x27;)encoder.summary()decoder_input = keras.Input(shape=(16,), name=&#x27;encoded_img&#x27;)x = layers.Reshape((4, 4, 1))(decoder_input)x = layers.Conv2DTranspose(16, 3, activation=&#x27;relu&#x27;)(x)x = layers.Conv2DTranspose(32, 3, activation=&#x27;relu&#x27;)(x)x = layers.UpSampling2D(3)(x)x = layers.Conv2DTranspose(16, 3, activation=&#x27;relu&#x27;)(x)decoder_output = layers.Conv2DTranspose(1, 3, activation=&#x27;relu&#x27;)(x)decoder = keras.Model(decoder_input, decoder_output, name=&#x27;decoder&#x27;)decoder.summary()autoencoder_input = keras.Input(shape=(28, 28, 1), name=&#x27;img&#x27;)encoded_img = encoder(autoencoder_input)decoded_img = decoder(encoded_img)autoencoder = keras.Model(autoencoder_input, decoded_img, name=&#x27;autoencoder&#x27;)autoencoder.summary() 可以发现，模型可以包含子模型，一个常见的用途是用于模型的聚合： 123456789101112131415def get_model(): inputs = keras.Input(shape=(128,)) outputs = layers.Dense(1)(inputs) return keras.Model(inputs, outputs)model1 = get_model()model2 = get_model()model3 = get_model()inputs = keras.Input(shape=(128,))y1 = model1(inputs)y2 = model2(inputs)y3 = model3(inputs)outputs = layers.average([y1, y2, y3])ensemble_model = keras.Model(inputs=inputs, outputs=outputs) 生成复杂模型包含多个输入和输出的模型我们可以使用函数式API生成包含多个输入输出的模型，这在Sequential中是不能被实现的。接下来我们创建一个将用户问题分类并且将其转交给哪个部门的模型，这个模型含有3个输入： 问题的标题 问题的内容 用户添加的问题的标签（分类输入） 含有2个输出： 优先级[0, 1] 这个问题该交给哪个部门 下面是代码实现： 1234567891011121314151617181920212223242526272829num_tags = 12 # Number of unique issue tagsnum_words = 10000 # Size of vocabulary obtained when preprocessing text datanum_departments = 4 # Number of departments for predictionstitle_input = keras.Input(shape=(None,), name=&#x27;title&#x27;) # Variable-length sequence of intsbody_input = keras.Input(shape=(None,), name=&#x27;body&#x27;) # Variable-length sequence of intstags_input = keras.Input(shape=(num_tags,), name=&#x27;tags&#x27;) # Binary vectors of size `num_tags`# Embed each word in the title into a 64-dimensional vectortitle_features = layers.Embedding(num_words, 64)(title_input)# Embed each word in the text into a 64-dimensional vectorbody_features = layers.Embedding(num_words, 64)(body_input)# Reduce sequence of embedded words in the title into a single 128-dimensional vectortitle_features = layers.LSTM(128)(title_features)# Reduce sequence of embedded words in the body into a single 32-dimensional vectorbody_features = layers.LSTM(32)(body_features)# Merge all available features into a single large vector via concatenationx = layers.concatenate([title_features, body_features, tags_input])# Stick a logistic regression for priority prediction on top of the featurespriority_pred = layers.Dense(1, name=&#x27;priority&#x27;)(x)# Stick a department classifier on top of the featuresdepartment_pred = layers.Dense(num_departments, name=&#x27;department&#x27;)(x)# Instantiate an end-to-end model predicting both priority and departmentmodel = keras.Model(inputs=[title_input, body_input, tags_input], outputs=[priority_pred, department_pred]) 至此我们完成的模型的创建，接下里需要完成模型的编译： 1234model.compile(optimizer=keras.optimizers.RMSprop(1e-3), loss=[keras.losses.BinaryCrossentropy(from_logits=True), keras.losses.CategoricalCrossentropy(from_logits=True)], loss_weights=[1., 0.2]) 如上，我们可以为输出赋予不同的误差函数，以帮助我们控制他们两个输出对误差的贡献。由于我们已经为输出层赋予了名字，我们也可以使用如下方式编译： 1234model.compile(optimizer=keras.optimizers.RMSprop(1e-3), loss=&#123;&#x27;priority&#x27;:keras.losses.BinaryCrossentropy(from_logits=True), &#x27;department&#x27;: keras.losses.CategoricalCrossentropy(from_logits=True)&#125;, loss_weights=[1., 0.2]) 接下来进行训练，对于在NumPy产生的数据： 1234567891011121314import numpy as np# Dummy input datatitle_data = np.random.randint(num_words, size=(1280, 10))body_data = np.random.randint(num_words, size=(1280, 100))tags_data = np.random.randint(2, size=(1280, num_tags)).astype(&#x27;float32&#x27;)# Dummy target datapriority_targets = np.random.random(size=(1280, 1))dept_targets = np.random.randint(2, size=(1280, num_departments))model.fit(&#123;&#x27;title&#x27;: title_data, &#x27;body&#x27;: body_data, &#x27;tags&#x27;: tags_data&#125;, &#123;&#x27;priority&#x27;: priority_targets, &#x27;department&#x27;: dept_targets&#125;, epochs=2, batch_size=32) 当我们使用Dataset对象时，它要么yield数组元组：([title_data, body_data, tags_data], [priority_targets, dept_targets])，要么yield字典元组：(&#123;&#39;title&#39;: title_data, &#39;body&#39;: body_data, &#39;tags&#39;: tags_data&#125;, &#123;&#39;priority&#39;: priority_targets, &#39;department&#39;: dept_targets&#125;)。 一个简单的残差网络模型函数式API还可以创建非线性的模型，一个常见的应用是构建残差模型： 123456789101112131415161718192021inputs = keras.Input(shape=(32, 32, 3), name=&#x27;img&#x27;)x = layers.Conv2D(32, 3, activation=&#x27;relu&#x27;)(inputs)x = layers.Conv2D(64, 3, activation=&#x27;relu&#x27;)(x)block_1_output = layers.MaxPooling2D(3)(x)x = layers.Conv2D(64, 3, activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(block_1_output)x = layers.Conv2D(64, 3, activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x)block_2_output = layers.add([x, block_1_output])x = layers.Conv2D(64, 3, activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(block_2_output)x = layers.Conv2D(64, 3, activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x)block_3_output = layers.add([x, block_2_output])x = layers.Conv2D(64, 3, activation=&#x27;relu&#x27;)(block_3_output)x = layers.GlobalAveragePooling2D()(x)x = layers.Dense(256, activation=&#x27;relu&#x27;)(x)x = layers.Dropout(0.5)(x)outputs = layers.Dense(10)(x)model = keras.Model(inputs, outputs, name=&#x27;toy_resnet&#x27;)model.summary() 训练方法如下： 12345678910111213(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()x_train = x_train.astype(&#x27;float32&#x27;) / 255.x_test = x_test.astype(&#x27;float32&#x27;) / 255.y_train = keras.utils.to_categorical(y_train, 10)y_test = keras.utils.to_categorical(y_test, 10)model.compile(optimizer=keras.optimizers.RMSprop(1e-3), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[&#x27;acc&#x27;])model.fit(x_train, y_train, batch_size=64, epochs=1, validation_split=0.2) 共享层函数式API的另外一个优点是我们可以使用共享层。为了创建共享层，我们只需要创建层的实例，然后再不断调用即可： 123456789101112# Embedding for 1000 unique words mapped to 128-dimensional vectorsshared_embedding = layers.Embedding(1000, 128)# Variable-length sequence of integerstext_input_a = keras.Input(shape=(None,), dtype=&#x27;int32&#x27;)# Variable-length sequence of integerstext_input_b = keras.Input(shape=(None,), dtype=&#x27;int32&#x27;)# We reuse the same layer to encode both inputsencoded_input_a = shared_embedding(text_input_a)encoded_input_b = shared_embedding(text_input_b) 提取和重用节点由于我们使用函数式API创建的模型是静态的，所以它容易被存取和检查。这个过程和画图差不多。这也就意味着我们可以获取模型中节点并且重用他们。接下来我们看一下带有权重的VGG19模型： 123from tensorflow.keras.applications import VGG19vgg19 = VGG19() 可以通过模型的结构获取到中间的层（节点）： 1features_list = [layer.output for layer in vgg19.layers] 我们可以通过这些节点来构建一个模型，用于获取通过每个层的中间值： 1234feat_extraction_model = keras.Model(inputs=vgg19.input, outputs=features_list)img = np.random.random((1, 224, 224, 3)).astype(&#x27;float32&#x27;)extracted_features = feat_extraction_model(img) 自定义层来扩展APItf.keras含有大量的内建层： 卷积层：Conv1D, Conv2D, Conv3D, Conv2DTranspose 池化层：MaxPooling1D, MaxPooling2D, MaxPooling3D, AveragePooling1D RNN层：GRU, LSTM, ConvLSTM2D BatchNormalization, Dropout, Embedding，etc. 如果这些都不能满足要求，我们可以创建Layer的子类，每个子类需要实现： call：定义这一层完成的运算 build：创建这一层的权重 下面是Dense层的简单实现： 123456789101112131415161718192021class CustomDense(layers.Layer): def __init__(self, units=32): super(CustomDense, self).__init__() self.units = units def build(self, input_shape): self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer=&#x27;random_normal&#x27;, trainable=True) self.b = self.add_weight(shape=(self.units,), initializer=&#x27;random_normal&#x27;, trainable=True) def call(self, inputs): return tf.matmul(inputs, self.w) + self.binputs = keras.Input((4,))outputs = CustomDense(10)(inputs)model = keras.Model(inputs, outputs) 如果想支持序列化，这时也需要实现get_config方法，该方法将会返回构造器的参数： 1234567891011121314151617181920212223242526272829class CustomDense(layers.Layer): def __init__(self, units=32): super(CustomDense, self).__init__() self.units = units def build(self, input_shape): self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer=&#x27;random_normal&#x27;, trainable=True) self.b = self.add_weight(shape=(self.units,), initializer=&#x27;random_normal&#x27;, trainable=True) def call(self, inputs): return tf.matmul(inputs, self.w) + self.b def get_config(self): return &#123;&#x27;units&#x27;: self.units&#125;inputs = keras.Input((4,))outputs = CustomDense(10)(inputs)model = keras.Model(inputs, outputs)config = model.get_config()new_model = keras.Model.from_config( config, custom_objects=&#123;&#x27;CustomDense&#x27;: CustomDense&#125;) 同样可以实现from_config方法来实现层的重构，默认的from_config方法如下： 12def from_config(cls, config): return cls(**config) 使用函数式API的时机什么时候该使用函数式API构建模型，什么时候使用模型子类构建模型？总体来说，函数式API是一种更加易用安全，多特性的方法，而模型子类则提供了更高的灵活性。 函数式API的优点如下： 简洁的： 1234567891011121314151617181920212223# 函数式APIinputs = keras.Input(shape=(32,))x = layers.Dense(64, activation=&#x27;relu&#x27;)(inputs)outputs = layers.Dense(10)(x)mlp = keras.Model(inputs, outputs)# 模型子类class MLP(keras.Model): def __init__(self, **kwargs): super(MLP, self).__init__(**kwargs) self.dense_1 = layers.Dense(64, activation=&#x27;relu&#x27;) self.dense_2 = layers.Dense(10) def call(self, inputs): x = self.dense_1(inputs) return self.dense_2(x)# Instantiate the model.mlp = MLP()# Necessary to create the model&#x27;s state.# The model doesn&#x27;t have a state until it&#x27;s called at least once._ = mlp(tf.zeros((1, 32))) 在构建模型的时候提供检查：每一层可以根据输入数据的shape和dtype判断是否是合法的输入 模型更易构建：构建模型就像是画图一样简单 模型可以被序列化和克隆 函数式API缺点如下： 不支持动态架构 混合模式构建模型我们可以混合使用函数式API和模型子类方式来构建模型： 123456789101112131415161718192021222324252627282930313233343536units = 32timesteps = 10input_dim = 5# Define a Functional modelinputs = keras.Input((None, units))x = layers.GlobalAveragePooling1D()(inputs)outputs = layers.Dense(1)(x)model = keras.Model(inputs, outputs)class CustomRNN(layers.Layer): def __init__(self): super(CustomRNN, self).__init__() self.units = units self.projection_1 = layers.Dense(units=units, activation=&#x27;tanh&#x27;) self.projection_2 = layers.Dense(units=units, activation=&#x27;tanh&#x27;) # Our previously-defined Functional model self.classifier = model def call(self, inputs): outputs = [] state = tf.zeros(shape=(inputs.shape[0], self.units)) for t in range(inputs.shape[1]): x = inputs[:, t, :] h = self.projection_1(x) y = h + self.projection_2(state) state = y outputs.append(y) features = tf.stack(outputs, axis=1) print(features.shape) return self.classifier(features)rnn_model = CustomRNN()_ = rnn_model(tf.zeros((1, timesteps, input_dim))) 下面是一个使用函数式模型构建RNN网络： 1234567891011121314151617181920212223242526272829303132333435363738units = 32timesteps = 10input_dim = 5batch_size = 16class CustomRNN(layers.Layer): def __init__(self): super(CustomRNN, self).__init__() self.units = units self.projection_1 = layers.Dense(units=units, activation=&#x27;tanh&#x27;) self.projection_2 = layers.Dense(units=units, activation=&#x27;tanh&#x27;) self.classifier = layers.Dense(1) def call(self, inputs): outputs = [] state = tf.zeros(shape=(inputs.shape[0], self.units)) for t in range(inputs.shape[1]): x = inputs[:, t, :] h = self.projection_1(x) y = h + self.projection_2(state) state = y outputs.append(y) features = tf.stack(outputs, axis=1) return self.classifier(features)# Note that we specify a static batch size for the inputs with the `batch_shape`# arg, because the inner computation of `CustomRNN` requires a static batch size# (when we create the `state` zeros tensor).inputs = keras.Input(batch_shape=(batch_size, timesteps, input_dim))x = layers.Conv1D(32, 3)(inputs)outputs = CustomRNN()(x)model = keras.Model(inputs, outputs)rnn_model = CustomRNN()_ = rnn_model(tf.zeros((1, 10, 5)))","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://blog.zsstrike.tech/tags/TensorFlow/"}]},{"title":"Keras概览","slug":"Keras概览","date":"2020-02-13T08:22:59.000Z","updated":"2022-05-16T07:41:45.992Z","comments":true,"path":"2020/02/13/Keras概览/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/13/Keras%E6%A6%82%E8%A7%88/","excerpt":"本节介绍Keras及其相关模块，以帮助我们快速构建人工神经网络。","text":"本节介绍Keras及其相关模块，以帮助我们快速构建人工神经网络。 构建一个简单模型层叠式模型在Keras中，我们使用层（layers）来构建我们的模型，模型通常是一个由多个层构成的流程图，最简单模型类型是层叠式（Sequential）类型。为了构建一个简单全连接的MLP，我们用如下代码： 1234567891011import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersmodel = tf.keras.Sequential()# Adds a densely-connected layer with 64 units to the model:model.add(layers.Dense(64, activation=&#x27;relu&#x27;))# Add another:model.add(layers.Dense(64, activation=&#x27;relu&#x27;))# Add an output layer with 10 output units:model.add(layers.Dense(10)) 调整层参数有很多内建的层，它们都有一些公用的构造参数： activation：设置激活函数 kernel_initializer和bias_initializer：用于初始化权重的方法 kernel_regularizer和bias_regularizer：定义正则化的方法 下面的代码构造使用不同的参数构造层： 12345678910111213141516# Create a relu layer:layers.Dense(64, activation=&#x27;relu&#x27;)# Or:layers.Dense(64, activation=tf.nn.relu)# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l1(0.01))# A linear layer with L2 regularization of factor 0.01 applied to the bias vector:layers.Dense(64, bias_regularizer=tf.keras.regularizers.l2(0.01))# A linear layer with a kernel initialized to a random orthogonal matrix:layers.Dense(64, kernel_initializer=&#x27;orthogonal&#x27;)# A linear layer with a bias vector initialized to 2.0s:layers.Dense(64, bias_initializer=tf.keras.initializers.Constant(2.0)) 训练和评估训练时的设置当模型被构建后，我们可以通过调用compile方法来调整学习过程： 1234567891011model = tf.keras.Sequential([# Adds a densely-connected layer with 64 units to the model:layers.Dense(64, activation=&#x27;relu&#x27;, input_shape=(32,)),# Add another:layers.Dense(64, activation=&#x27;relu&#x27;),# Add an output layer with 10 output units:layers.Dense(10)])model.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;]) compile有以下重要参数： optimizer：定义优化算法 loss：定义误差函数 metrix：用于观察训练的情况 下面的示例展示了调整模型的情况： 123456789# Configure a model for mean-squared error regression.model.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss=&#x27;mse&#x27;, # mean squared error metrics=[&#x27;mae&#x27;]) # mean absolute error# Configure a model for categorical classification.model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;]) 从NumPy数据训练对于小型的数据集，我们可以用如下方法训练： 123456import numpy as npdata = np.random.random((1000, 32))labels = np.random.random((1000, 10))model.fit(data, labels, epochs=10, batch_size=32) fit方法有以下重要的参数： epochs：训练的迭代次数 batch_size：每个批次的样本数量 validation_data：用于定义验证集 12345678910import numpy as npdata = np.random.random((1000, 32))labels = np.random.random((1000, 10))val_data = np.random.random((100, 32))val_labels = np.random.random((100, 10))model.fit(data, labels, epochs=10, batch_size=32, validation_data=(val_data, val_labels)) 从tf.data中的datasets训练使用Datasets中的方法来构建训练数据集： 12345# Instantiates a toy dataset instance:dataset = tf.data.Dataset.from_tensor_slices((data, labels))dataset = dataset.batch(32)model.fit(dataset, epochs=10) Dataset数据会不断yield小批次的数据，因此不需要batch_size参数。 同样，Dataset可以用于验证： 12345678dataset = tf.data.Dataset.from_tensor_slices((data, labels))dataset = dataset.batch(32)val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))val_dataset = val_dataset.batch(32)model.fit(dataset, epochs=10, validation_data=val_dataset) 评估和预测代码如下： 1234567891011# With Numpy arraysdata = np.random.random((1000, 32))labels = np.random.random((1000, 10))model.evaluate(data, labels, batch_size=32)# With a Datasetdataset = tf.data.Dataset.from_tensor_slices((data, labels))dataset = dataset.batch(32)model.evaluate(dataset) 同样，我们可以使用以下代码进行预测： 12result = model.predict(data, batch_size=32)print(result.shape) 构建复杂模型函数式API层叠式模型是一种将多个层之间连接的简单模型，我们可以使用Keras中的函数式API来构建复杂的模型： 多个输入模型 多个输出模型 包含共享层（同一个层被多次调用）的模型 不包含顺序流的模型（如残差模型） 接下来我们使用函数式API来构建这样的一个模型： 一个层实例是可以被调用的并且可以返回一个张量 输入输出张量可以被用来定义模型实例 该模型训练方法和层叠模型一致 下面的代码用于构建一个简单全连接的网络： 123456inputs = tf.keras.Input(shape=(32,)) # Returns an input placeholder# A layer instance is callable on a tensor, and returns a tensor.x = layers.Dense(64, activation=&#x27;relu&#x27;)(inputs)x = layers.Dense(64, activation=&#x27;relu&#x27;)(x)predictions = layers.Dense(10)(x) 接下来实例化模型： 123456789model = tf.keras.Model(inputs=inputs, outputs=predictions)# The compile step specifies the training configuration.model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;])# Trains for 5 epochsmodel.fit(data, labels, batch_size=32, epochs=5) 模型子类化为了构建一个高度定制的模型，我们可以构造模型的子类。我们可以在__init__方法中定义层和层的属性，同时在call方法中定义前向传播。代码示例如下： 1234567891011121314class MyModel(tf.keras.Model): def __init__(self, num_classes=10): super(MyModel, self).__init__(name=&#x27;my_model&#x27;) self.num_classes = num_classes # Define your layers here. self.dense_1 = layers.Dense(32, activation=&#x27;relu&#x27;) self.dense_2 = layers.Dense(num_classes) def call(self, inputs): # Define your forward pass here, # using layers you previously defined (in `__init__`). x = self.dense_1(inputs) return self.dense_2(x) 接下来定义新的模型子类： 123456789model = MyModel(num_classes=10)# The compile step specifies the training configuration.model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;])# Trains for 5 epochs.model.fit(data, labels, batch_size=32, epochs=5) 自定义层自定义的层可以通过构建tf.keras.layers.Layer的子类来进行，需要实现如下方法： __init__：可选，用于定义这一层中将要使用子层 build：创建层的权重，可以通过add_weight方法来添加权重 call：定义前向传播 可选，可以通过实现get_config和from_config方法实现序列化和反序列化 下面代码实现了一个矩阵相乘的层： 123456789101112131415161718192021222324class MyLayer(layers.Layer): def __init__(self, output_dim, **kwargs): self.output_dim = output_dim super(MyLayer, self).__init__(**kwargs) def build(self, input_shape): # Create a trainable weight variable for this layer. self.kernel = self.add_weight(name=&#x27;kernel&#x27;, shape=(input_shape[1], self.output_dim), initializer=&#x27;uniform&#x27;, trainable=True) def call(self, inputs): return tf.matmul(inputs, self.kernel) def get_config(self): base_config = super(MyLayer, self).get_config() base_config[&#x27;output_dim&#x27;] = self.output_dim return base_config @classmethod def from_config(cls, config): return cls(**config) 使用自定义层来构建模型： 12345678910model = tf.keras.Sequential([ MyLayer(10)])# The compile step specifies the training configurationmodel.compile(optimizer=tf.keras.optimizers.RMSprop(0.001), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;])# Trains for 5 epochs.model.fit(data, labels, batch_size=32, epochs=5) Callbackscallback对象用于传给模型以此在训练期间被使用，可以使用自定义的callback，同样可以使用内建的callback： tf.keras.callbacks.ModelCheckpoint: 每次迭代保存检验点 tf.keras.callbacks.LearningRateScheduler: 动态改变学习速率 tf.keras.callbacks.EarlyStopping: 如果模型没有改经就停止训练 tf.keras.callbacks.TensorBoard: 监视模型的行为 使用方法如下： 12345678callbacks = [ # Interrupt training if `val_loss` stops improving for over 2 epochs tf.keras.callbacks.EarlyStopping(patience=2, monitor=&#x27;val_loss&#x27;), # Write TensorBoard logs to `./logs` directory tf.keras.callbacks.TensorBoard(log_dir=&#x27;./logs&#x27;)]model.fit(data, labels, batch_size=32, epochs=5, callbacks=callbacks, validation_data=(val_data, val_labels)) 保存和恢复保存权重值使用方法： 123456# Save weights to a TensorFlow Checkpoint filemodel.save_weights(&#x27;./weights/my_model&#x27;)# Restore the model&#x27;s state,# this requires a model with the same architecture.model.load_weights(&#x27;./weights/my_model&#x27;) 同样，可以将文件格式保存为HDF5类型： 12345# Save weights to a HDF5 filemodel.save_weights(&#x27;my_model.h5&#x27;, save_format=&#x27;h5&#x27;)# Restore the model&#x27;s statemodel.load_weights(&#x27;my_model.h5&#x27;) 保存模型配置参数一个模型的配置参数可以被保存（但是不保存权重），即使是在没有代码定义，一个模型的配置可以用于创建和初始化同样的模型。Keras支持JSON和YAML的两种序列化的格式： 1234567# Serialize a model to JSON formatjson_string = model.to_json() # savefresh_model = tf.keras.models.model_from_json(json_string) # restore# Serialize a model to YAML formatyaml_string = model.to_yaml()fresh_model = tf.keras.models.model_from_yaml(yaml_string) 保存整个模型到一个文件整个模型的权重值，模型配置参数和优化配置参数可以被保存在一个文件中，这样可以让我们的模型在检查点处保存和恢复： 12345678910111213141516# Create a simple modelmodel = tf.keras.Sequential([ layers.Dense(10, activation=&#x27;relu&#x27;, input_shape=(32,)), layers.Dense(10)])model.compile(optimizer=&#x27;rmsprop&#x27;, loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;])model.fit(data, labels, batch_size=32, epochs=5)# Save entire model to a HDF5 filemodel.save(&#x27;my_model&#x27;)# Recreate the exact same model, including weights and optimizer.model = tf.keras.models.load_model(&#x27;my_model&#x27;) 分布式运行在GPUs上运行首先需要在分布策略范围内定义模型： 12345678910111213strategy = tf.distribute.MirroredStrategy()with strategy.scope(): model = tf.keras.Sequential() model.add(layers.Dense(16, activation=&#x27;relu&#x27;, input_shape=(10,))) model.add(layers.Dense(1)) optimizer = tf.keras.optimizers.SGD(0.2) model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=optimizer)model.summary() 接下来，按照平常的方式进行训练： 1234567x = np.random.random((1024, 10))y = np.random.randint(2, size=(1024, 1))x = tf.cast(x, tf.float32)dataset = tf.data.Dataset.from_tensor_slices((x, y))dataset = dataset.shuffle(buffer_size=1024).batch(32)model.fit(dataset, epochs=1)","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://blog.zsstrike.tech/tags/TensorFlow/"}]},{"title":"TF2初始教程","slug":"TF2初始教程","date":"2020-02-12T06:38:17.000Z","updated":"2022-05-16T07:41:46.120Z","comments":true,"path":"2020/02/12/TF2初始教程/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/12/TF2%E5%88%9D%E5%A7%8B%E6%95%99%E7%A8%8B/","excerpt":"本节我们学习一些TensorFlow的基本使用方法，包括使用TensorFlow构建神经网络来对MNIST数据集进行划分以及学习一下数据的加载方法。","text":"本节我们学习一些TensorFlow的基本使用方法，包括使用TensorFlow构建神经网络来对MNIST数据集进行划分以及学习一下数据的加载方法。 使用TensorFlow对MNIST数据集进行划分首先，我们加载MNIST数据集，同时将数据映射到$ [0, 1] $上： 12345import tensorflow as tfmnist = tf.keras.datasets.mnist(X_train, y_train), (X_test, y_test) = mnist.load_data()X_train, X_test = X_train / 255.0, X_test / 255.0 接下来将各层堆叠起来，来搭建tf.keras.Sequential模型： 123456model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation=&#x27;relu&#x27;), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10, activation=&#x27;softmax&#x27;)]) 接下来我们将已经搭建的模型进行编译： 12model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;]) 接下来，训练并且验证模型： 12model.fit(X_train, y_train, epochs=5)model.evaluate(X_test, y_test, verbose=2) 得到的结果如下： 12345678910111213Train on 60000 samplesEpoch 1/560000/60000 [==============================] - 5s 87us/sample - loss: 0.2942 - accuracy: 0.9140Epoch 2/560000/60000 [==============================] - 5s 75us/sample - loss: 0.1416 - accuracy: 0.9582Epoch 3/560000/60000 [==============================] - 4s 75us/sample - loss: 0.1056 - accuracy: 0.9681Epoch 4/560000/60000 [==============================] - 4s 73us/sample - loss: 0.0888 - accuracy: 0.9724Epoch 5/560000/60000 [==============================] - 4s 73us/sample - loss: 0.0752 - accuracy: 0.976110000/1 - 1s - loss: 0.0385 - accuracy: 0.9779[0.07606992674819194, 0.9779] 现在，我们得到的照片分类器的准确率已经达到了98%。相较于之前我们实现的分类器，这个分类器的准确率更加优良。 对Fashion MNIST数据集划分这一节我们会构建一个神经网络模型来区分关于衣物的图片，首先导入我们需要的库： 1234567import tensorflow as tffrom tensorflow import kerasimport numpy as npimport matplotlib.pyplot as pltprint(tf.__version__)&gt;&gt; 2.0.0 接下来导入Fashion MNIST数据集，这个数据集包含了共70000张10个类别的图片，每个图片用$ 28 \\times 28 $的矩阵来表示。我们将60000张图片用作是训练，10000章图片用作是评估。代码如下： 12fashion_mnist = keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() 该数据集的类标对应关系如下： Label Class 0 T-shirt&#x2F;top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot 我们可以构建一个列表，来映射相应类标对应的类别： 12class_names = [&#x27;T-shirt/top&#x27;, &#x27;Trouser&#x27;, &#x27;Pullover&#x27;, &#x27;Dress&#x27;, &#x27;Coat&#x27;, &#x27;Sandal&#x27;, &#x27;Shirt&#x27;, &#x27;Sneaker&#x27;, &#x27;Bag&#x27;, &#x27;Ankle boot&#x27;] 下面我们看一下训练集中的第一张图片： 12345plt.figure()plt.imshow(train_images[0])plt.colorbar()plt.grid(False)plt.show() 得到的图像如下： 接下来我们需要进行特征缩放： 12train_images = train_images / 255.0test_images = test_images / 255.0 然后，看一下训练集中的前25张图片： 12345678910# first 25 picplt.figure(figsize=(10, 10))for i in range(25): plt.subplot(5, 5, i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i], cmap=plt.cm.binary) plt.xlabel(class_names[train_labels[i]])plt.show() 图像如下： 至此，我们来构建并且编译模型： 1234567891011# build model## set up layersmodel = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(128, activation=&#x27;relu&#x27;), keras.layers.Dense(10)])## compile the modelmodel.compile(optimizer=&#x27;adam&#x27;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;]) 然后，训练这个模型： 1model.fit(train_images, train_labels, epochs=10) 运行结果如下： 12345678910111213141516171819202122Train on 60000 samplesEpoch 1/1060000/60000 [==============================] - 5s 76us/sample - loss: 0.5019 - accuracy: 0.8227Epoch 2/1060000/60000 [==============================] - 4s 67us/sample - loss: 0.3747 - accuracy: 0.8639Epoch 3/1060000/60000 [==============================] - 4s 68us/sample - loss: 0.3375 - accuracy: 0.8772Epoch 4/1060000/60000 [==============================] - 4s 68us/sample - loss: 0.3132 - accuracy: 0.8858Epoch 5/1060000/60000 [==============================] - 4s 72us/sample - loss: 0.2913 - accuracy: 0.8926Epoch 6/1060000/60000 [==============================] - 4s 73us/sample - loss: 0.2791 - accuracy: 0.8977Epoch 7/1060000/60000 [==============================] - 4s 74us/sample - loss: 0.2660 - accuracy: 0.9014Epoch 8/1060000/60000 [==============================] - 5s 82us/sample - loss: 0.2538 - accuracy: 0.9048Epoch 9/1060000/60000 [==============================] - 5s 78us/sample - loss: 0.2454 - accuracy: 0.9086Epoch 10/1060000/60000 [==============================] - 5s 90us/sample - loss: 0.2362 - accuracy: 0.9113 接下来，我们看一下模型在评估集上面的准确率： 123test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)print(&#x27;Acc: %.2f&#x27; % test_acc)&gt;&gt; Acc: 0.88 可以发现我们的模型在评估集上面的准确率比在训练集上面的准确率低，说明我们的模型过拟合了。 接下来，我们来进行预测： 12345predictions = model.predict(test_images)predictions[0]&gt;&gt; array([-10.688818 , -11.685984 , -11.544111 , -15.445654 , -9.708677 ,&gt;&gt; -1.1382349, -10.859651 , 2.193298 , -12.344756 , 6.487081 ],&gt;&gt; dtype=float32) 可以发现一个预测的结果是一个包含10个数字的数组，数字代表着这张图片属于某个类标的可信度。同样可以使用argmax函数来得到最高可信度对应的类标： 12np.argmax(predictions[0])&gt;&gt; 9 同样的，当我们需要对单独一个未知的数据进行预测的时候，需要将其转换为$ (n,28,28) $的shape： 1234img = test_images[1]img = np.expand_dims(img, 0)print(img.shape)&gt;&gt; (1, 28, 28) 接下里就可以进行预测了： 1234predictions_single = model.predict(img)print(predictions_single)&gt;&gt; [[ -2.5079174 -16.936686 8.989066 -12.332449 2.5185988&gt;&gt; -2.7389777 -0.61303186 -22.695055 -13.934152 -30.008425 ]] 相应的类标如下： 12np.argmax(predictions_single[0])&gt;&gt; 2 加载CSV数据本节学习如何将CSV文件加载到tf.data.Dataset中，将要使用的是泰坦尼克号乘客的数据，模型会根据乘客的年龄，性别，票务舱和是否独立旅行等特征来预测乘客生还的可能性。 首先，导入必要的库： 1234import functoolsimport numpy as npimport tensorflow as tfimport tensorflow_datasets as tfds 接下来，下载数据文件： 12345TRAIN_DATA_URL = &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;TEST_DATA_URL = &quot;https://storage.googleapis.com/tf-datasets/titanic/eval.csv&quot;train_file_path = tf.keras.utils.get_file(&quot;train.csv&quot;, TRAIN_DATA_URL)test_file_path = tf.keras.utils.get_file(&quot;eval.csv&quot;, TEST_DATA_URL) 同时设置一下numpy的输出设置，让他的输出精度是3位： 1np.set_printoptions(precision=3, suppress=True) 首先，来看一下CSV文件的前面几行： 1!head &#123;train_file_path&#125; 得到的输出如下： 12345678910survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone0,male,22.0,1,0,7.25,Third,unknown,Southampton,n1,female,38.0,1,0,71.2833,First,C,Cherbourg,n1,female,26.0,0,0,7.925,Third,unknown,Southampton,y1,female,35.0,1,0,53.1,First,C,Southampton,n0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y0,male,2.0,3,1,21.075,Third,unknown,Southampton,n1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n1,female,4.0,1,1,16.7,Third,G,Southampton,n 可以发现，CSV文件的每列都有一个列名。dataset构造函数会自动识别这些列名。如果某个CSV文件不包含列名，我们可以自己手动设置： 12345678CSV_COLUMNS = [&#x27;survived&#x27;, &#x27;sex&#x27;, &#x27;age&#x27;, &#x27;n_siblings_spouses&#x27;, &#x27;parch&#x27;, &#x27;fare&#x27;, &#x27;class&#x27;, &#x27;deck&#x27;, &#x27;embark_town&#x27;, &#x27;alone&#x27;]dataset = tf.data.experimental.make_csv_dataset( ..., column_names=CSV_COLUMNS, ...) 这个示例使用了所有的列，当然我们也可以只使用某些选中的列： 123456dataset = tf.data.experimental.make_csv_dataset( ..., select_columns = columns_to_use, ...) 对于包含模型需要预测的值的列是需要显式指定的： 12LABEL_COLUMN = &#x27;survived&#x27;LABELS = [0, 1] 现在从文件中读取CSV数据并创建dataset： 123456789101112def get_dataset(file_path): dataset = tf.data.experimental.make_csv_dataset( file_path, batch_size=12, # 为了示例更容易展示，手动设置较小的值 label_name=LABEL_COLUMN, na_value=&quot;?&quot;, num_epochs=1, ignore_errors=True) return datasetraw_train_data = get_dataset(train_file_path)raw_test_data = get_dataset(test_file_path) dataset中的每个条目都是一个批次，用一个元组表示（多个样本，多个标签）。样本中的数据组织形式是以列为主的张量，每个条目中包含的元素个数就是批次大小（本例中是12）。 我们首先获取第一个条目的数据： 123examples, labels = next(iter(raw_train_data)) # 第一个批次print(&quot;EXAMPLES: \\n&quot;, examples, &quot;\\n&quot;)print(&quot;LABELS: \\n&quot;, labels) 输出： 1234567891011121314151617181920212223EXAMPLES: OrderedDict([(&#x27;sex&#x27;, &lt;tf.Tensor: id=170, shape=(12,), dtype=string, numpy=array([b&#x27;male&#x27;, b&#x27;male&#x27;, b&#x27;female&#x27;, b&#x27;female&#x27;, b&#x27;female&#x27;, b&#x27;male&#x27;, b&#x27;male&#x27;, b&#x27;male&#x27;, b&#x27;male&#x27;, b&#x27;male&#x27;, b&#x27;male&#x27;, b&#x27;male&#x27;], dtype=object)&gt;), (&#x27;age&#x27;, &lt;tf.Tensor: id=162, shape=(12,), dtype=float32, numpy=array([19., 17., 42., 22., 9., 24., 28., 36., 37., 32., 28., 28.], dtype=float32)&gt;), (&#x27;n_siblings_spouses&#x27;, &lt;tf.Tensor: id=168, shape=(12,), dtype=int32, numpy=array([0, 0, 1, 1, 4, 1, 0, 0, 2, 0, 1, 0], dtype=int32)&gt;), (&#x27;parch&#x27;, &lt;tf.Tensor: id=169, shape=(12,), dtype=int32, numpy=array([0, 2, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0], dtype=int32)&gt;), (&#x27;fare&#x27;, &lt;tf.Tensor: id=167, shape=(12,), dtype=float32, numpy=array([ 6.75 , 110.883, 26. , 29. , 31.275, 16.1 , 13.863, 512.329, 7.925, 7.896, 19.967, 26.55 ], dtype=float32)&gt;), (&#x27;class&#x27;, &lt;tf.Tensor: id=164, shape=(12,), dtype=string, numpy=array([b&#x27;Third&#x27;, b&#x27;First&#x27;, b&#x27;Second&#x27;, b&#x27;Second&#x27;, b&#x27;Third&#x27;, b&#x27;Third&#x27;, b&#x27;Second&#x27;, b&#x27;First&#x27;, b&#x27;Third&#x27;, b&#x27;Third&#x27;, b&#x27;Third&#x27;, b&#x27;First&#x27;], dtype=object)&gt;), (&#x27;deck&#x27;, &lt;tf.Tensor: id=165, shape=(12,), dtype=string, numpy=array([b&#x27;unknown&#x27;, b&#x27;C&#x27;, b&#x27;unknown&#x27;, b&#x27;unknown&#x27;, b&#x27;unknown&#x27;, b&#x27;unknown&#x27;, b&#x27;unknown&#x27;, b&#x27;B&#x27;, b&#x27;unknown&#x27;, b&#x27;unknown&#x27;, b&#x27;unknown&#x27;, b&#x27;C&#x27;], dtype=object)&gt;), (&#x27;embark_town&#x27;, &lt;tf.Tensor: id=166, shape=(12,), dtype=string, numpy=array([b&#x27;Queenstown&#x27;, b&#x27;Cherbourg&#x27;, b&#x27;Southampton&#x27;, b&#x27;Southampton&#x27;, b&#x27;Southampton&#x27;, b&#x27;Southampton&#x27;, b&#x27;Cherbourg&#x27;, b&#x27;Cherbourg&#x27;, b&#x27;Southampton&#x27;, b&#x27;Southampton&#x27;, b&#x27;Southampton&#x27;, b&#x27;Southampton&#x27;], dtype=object)&gt;), (&#x27;alone&#x27;, &lt;tf.Tensor: id=163, shape=(12,), dtype=string, numpy=array([b&#x27;y&#x27;, b&#x27;n&#x27;, b&#x27;n&#x27;, b&#x27;n&#x27;, b&#x27;n&#x27;, b&#x27;n&#x27;, b&#x27;y&#x27;, b&#x27;n&#x27;, b&#x27;n&#x27;, b&#x27;y&#x27;, b&#x27;n&#x27;, b&#x27;y&#x27;], dtype=object)&gt;)]) LABELS: tf.Tensor([0 1 1 1 0 0 1 1 0 0 0 1], shape=(12,), dtype=int32) 接下来，我们进行数据的预处理。 CSV数据中有些列是分类的列，也就是这些列中的值只能在有限的集合中取值。使用tf.feature_columnAPI创建一个tf.feture_column.indicator_column集合，集合中每个元素对应着一个分类的列。我们先将其转换： 123456789101112131415CATEGORIES = &#123; &#x27;sex&#x27;: [&#x27;male&#x27;, &#x27;female&#x27;], &#x27;class&#x27; : [&#x27;First&#x27;, &#x27;Second&#x27;, &#x27;Third&#x27;], &#x27;deck&#x27; : [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;, &#x27;G&#x27;, &#x27;H&#x27;, &#x27;I&#x27;, &#x27;J&#x27;], &#x27;embark_town&#x27; : [&#x27;Cherbourg&#x27;, &#x27;Southhampton&#x27;, &#x27;Queenstown&#x27;], &#x27;alone&#x27; : [&#x27;y&#x27;, &#x27;n&#x27;]&#125;categorical_columns = []for feature, vocab in CATEGORIES.items(): cat_col = tf.feature_column.categorical_column_with_vocabulary_list( key=feature, vocabulary_list=vocab) categorical_columns.append(tf.feature_column.indicator_column(cat_col) categorical_columns 得到的输出如下： 12345[IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key=&#x27;sex&#x27;, vocabulary_list=(&#x27;male&#x27;, &#x27;female&#x27;), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key=&#x27;class&#x27;, vocabulary_list=(&#x27;First&#x27;, &#x27;Second&#x27;, &#x27;Third&#x27;), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key=&#x27;deck&#x27;, vocabulary_list=(&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;, &#x27;G&#x27;, &#x27;H&#x27;, &#x27;I&#x27;, &#x27;J&#x27;), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key=&#x27;embark_town&#x27;, vocabulary_list=(&#x27;Cherbourg&#x27;, &#x27;Southhampton&#x27;, &#x27;Queenstown&#x27;), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key=&#x27;alone&#x27;, vocabulary_list=(&#x27;y&#x27;, &#x27;n&#x27;), dtype=tf.string, default_value=-1, num_oov_buckets=0))] 这是后续构建模型时处理输入数据的一部分。 而对于连续数据，我们需要将其进行标准化，写一个函数标准化这些值，然后将这些值改造成2维德张量： 1234def process_continuous_data(mean, data): # 标准化数据 data = tf.cast(data, tf.float32) * 1/(2*mean) return tf.reshape(data, [-1, 1]) 现在创建一个数值列的集合。tf.feature_columns.numeric_column API 会使用 normalizer_fn 参数。在传参的时候使用 functools.partial，functools.partial 由使用每个列的均值进行标准化的函数构成。 12345678910111213MEANS = &#123; &#x27;age&#x27; : 29.631308, &#x27;n_siblings_spouses&#x27; : 0.545455, &#x27;parch&#x27; : 0.379585, &#x27;fare&#x27; : 34.385399&#125;numerical_columns = []for feature in MEANS.keys(): num_col = tf.feature_column.numeric_column(feature, normalizer_fn=functools.partial(process_continuous_data, MEANS[feature])) numerical_columns.append(num_col) 接下来创建预处理层： 1preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numerical_columns) 然后基于预处理层构建并编译模型： 1234567891011model = tf.keras.Sequential([ preprocessing_layer, tf.keras.layers.Dense(128, activation=&#x27;relu&#x27;), tf.keras.layers.Dense(128, activation=&#x27;relu&#x27;), tf.keras.layers.Dense(1, activation=&#x27;sigmoid&#x27;),])model.compile( loss=&#x27;binary_crossentropy&#x27;, optimizer=&#x27;adam&#x27;, metrics=[&#x27;accuracy&#x27;]) 接下来，我们就可以实例化和训练模型： 123train_data = raw_train_data.shuffle(500)test_data = raw_test_datamodel.fit(train_data, epochs=20) 训练完成后，我们可以在测试集上检查准确性： 123test_loss, test_accuracy = model.evaluate(test_data)print(&#x27;\\n\\nTest Loss &#123;&#125;, Test Accuracy &#123;&#125;&#x27;.format(test_loss, test_accuracy))&gt;&gt; Test Loss 0.44521663270213385, Test Accuracy 0.814393937587738 可以发现，该模型的预测准确率是81%。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://blog.zsstrike.tech/tags/TensorFlow/"}]},{"title":"SSR更新PAC文件","slug":"SSR更新PAC文件","date":"2020-02-11T05:03:23.000Z","updated":"2022-05-16T07:41:46.120Z","comments":true,"path":"2020/02/11/SSR更新PAC文件/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/11/SSR%E6%9B%B4%E6%96%B0PAC%E6%96%87%E4%BB%B6/","excerpt":"SSR项目已经不再维护，它的PAC文件更新功能已经失效，本文我们将gfwlist.txt转换为pac.txt给SSR软件使用。","text":"SSR项目已经不再维护，它的PAC文件更新功能已经失效，本文我们将gfwlist.txt转换为pac.txt给SSR软件使用。 虽然原来的PAC地址已经失效了，但是gfwlist项目组维护了被墙的网站，GitHub地址：https://github.com/gfwlist/gfwlist/ 。首先，我们下载gfwlist.txt： 1curl https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt &gt; gfwlist.txt 接下来，我们需要安装Python的包genpac： 1pip install genpac 安装完成后，使用genpac将文件gfwlist.txt转换为pac.txt： 1genpac --pac-proxy=&quot;SOCKS 127.0.0.1:1080&quot; --gfwlist-local=&quot;./gfwlist.txt&quot; -o pac.txt 接下来将生成的pac.txt文件覆盖掉原来SSR软件的pac.txt即可。","categories":[],"tags":[{"name":"SSR","slug":"SSR","permalink":"http://blog.zsstrike.tech/tags/SSR/"}]},{"title":"实现多层人工神经网络","slug":"实现多层人工神经网络","date":"2020-02-09T08:05:41.000Z","updated":"2022-05-16T07:41:46.294Z","comments":true,"path":"2020/02/09/实现多层人工神经网络/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/09/%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%B1%82%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","excerpt":"本章中，我们将会学习人工神经网络的基本概念以帮助我们学习后面几章中的内容。","text":"本章中，我们将会学习人工神经网络的基本概念以帮助我们学习后面几章中的内容。 使用人工神经网络对复杂函数建模我们在第二章中从人工神经元入手，开始了机器学习算法的探索。对于本章中将要讨论的多层人工神经网络来说，人工神经元是其构建的基石。 单层神经网络回顾先来回顾一下自适应线性神经元（Adaline）算法： 我们实现了二分类类别的Adaline算法，并通过梯度下降优化算法来学习模型的权重系数：$$w:&#x3D;w+\\Delta w,其中\\Delta w &#x3D; -\\eta \\nabla J(w)$$在梯度下降优化过程中，我们在每次迭代后同时更新所有权重。此外，将激励函数定义为：$$\\phi(z)&#x3D;z&#x3D;a$$其中，净输入z时输入和权重的线性组合，使用激励函数来计算梯度更新时，我们定义了一个阈值函数将连续的输出值转换为二类别分类的预测类标：$$\\hat{y}&#x3D;\\begin{cases}1 &amp; 若g(z) \\ge 0\\-1 &amp; 其他\\end{cases}$$ 多层神经网络架构简介本节中，我们将会看到如何将多个单独的神经元连接为一个多层前反馈神经网络。这种特殊的网络也被称作是多层感知器（MLP）。MLP的示例图如下： MLP包含一个输入层，一个隐层以及一个输出层。如果这样的网络中包含不只一个隐层，我们称其为深度神经网络。如图所示，我们将第l层中第i个激励单元记作$ a_i^l $，同时我们将激励单元$ a_0^{in} $和$ a_0^{h} $为偏置单元（bias unit），我们均设定为1。输入层各单元的激励为输入加上偏置单元：$$a^{in} &#x3D; \\begin{bmatrix}a^{in}_0 \\a^{in}_1 \\\\vdots \\a^{in}m\\end{bmatrix}$$对于第l层的各单元，均通过一个权重系数连接到$ l+1 $层中的所有单元上。如连接第l层中第k个单元与第$ l+1 $层中第j个单元的连接可记为$ w{j,k}^l $。下图是一个3-4-3多层感知器： 通过正向传播构造神经网络本节中，我们将使用正向传播来计算多层感知器（MLP）模型的输出。我们将多层感知器的学习过程总结为三个步骤： 从输入层开始，通过网络向前传播（也就是正向传播）训练数据中的模式，以生成输出 基于网络的输出，通过一个代价函数计算所需最小化的误差 反向传播误差，计算其对于网络中每个权重的导数，并且更新模型 最终通过多层感知器模型权重的多次迭代和学习，我们使用正向传播来计算输出，并使用阈值函数获得独热法所表示的预测类标。 现在，我们根据正向传播算法逐步从训练数据的模式中生成一个输出。由于隐层每个节点均完全连接到所有输入层节点，我们首先通过以下公式计算$ a_1^2 $的激励：$$z_1^2 &#x3D; a_0^1w_{1,0}^1+a_1^1w_{1,1}^1+\\cdots+a_m^1w_{1,m}^1\\a_1^2 &#x3D; \\phi(z_1^2)$$激励函数可以使用sigmoid激励函数以解决图像分类等复杂问题。 多层感知器是一个典型的前馈人工神经网络，此处的前馈指的是每一层的输出都直接作为下一层的输入。为了提高代码的执行效率和可读性，我们将使用线性代数中的基本概念：$$Z^2 &#x3D; W^1[A^1]^T$$接下来我们可以将激励函数$ \\phi(\\cdot) $应用于净输入矩阵中的每个值，便于获取下一个激励矩阵$ A^2 $:$$A^2 &#x3D; \\phi(Z^2)$$类似地，我们以向量的形式重写输入层的激励：$$Z^3 &#x3D; W^2A^2$$最后，通过sigmoid激励函数，我们可以得到神经网络的连续型输出：$$A^3 &#x3D; \\phi(Z^3)$$ 手写数字的识别接下来我们看一下神经网络在实际中的应用，通过MNIST数据集上对手写数字的识别，来完成我们第一个多层神经网络的训练。MNIST是机器学习算法中常用的一个基准数据集。 获取MNIST数据集MNIST数据集可以通过链接http://yann.lecun.com/exdb/mnist/下载，包含下列四个部分： 训练集图像：train-images-idx3-ubyte.gz 训练集类标：train-labels-idx1-ubyte.gz 测试集图像：t10k-images-idx3-ubyte.gz 测试集类标：t10k-labels-idx1-ubyte.gz 下载完数据后并解压，接下来将其读入数组并且用于训练感知器模型： 1234567891011121314import osimport structimport numpy as npdef load_mnist(path, kind=&#x27;train&#x27;): labels_path = os.path.join(path, &#x27;%s-labels-idx1-ubyte&#x27; % kind) images_path = os.path.join(path, &#x27;%s-images-idx3-ubyte&#x27; % kind) with open(labels_path, &#x27;rb&#x27;) as lbpath: magic, c = struct.unpack(&#x27;&gt;II&#x27;, lbpath.read(8)) labels = np.fromfile(lbpath, dtype=np.uint8) with open(images_path, &#x27;rb&#x27;) as imgpath: magic, num, rows, cols = struct.unpack(&#x27;&gt;IIII&#x27;, imgpath.read(16)) images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784) return images, labels load_mnist函数返回值返回两个数组，第一个是$ n\\times m $维NumPy数组（存储图像），返回的第二个数组（类标）包含对应的目标变量，也即手写数字对应的类标，struct.unpack函数中的fmt参数的实参值：&gt;II。&gt;这是表示大端字节序，I表示一个无符号整数。 接下来我们读取数据： 123456X_train, y_train = load_mnist(&#x27;mnist&#x27;, kind=&#x27;train&#x27;)print(&#x27;Rows: %d, columns: %d&#x27; % (X_train.shape[0], X_train.shape[1]))X_test, y_test = load_mnist(&#x27;mnist&#x27;, kind=&#x27;t10k&#x27;)print(&#x27;Rows: %d, columns: %d&#x27; % (X_test.shape[0], X_test.shape[1]))&gt;&gt; Rows: 60000, columns: 784&gt;&gt; Rows: 10000, columns: 784 为了解MNIST数据集中图像的样子，我们可以将特征矩阵中的784像素向量还原为$ 28 \\times 28 $图像： 12345678910import matplotlib.pyplot as pltfig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)ax = ax.flatten()for i in range(10): img = X_train[y_train==i][0].reshape(28, 28) ax[i].imshow(img, cmap=&#x27;Greys&#x27;, interpolation=&#x27;nearest&#x27;)ax[0].set_xticks([])ax[0].set_yticks([])plt.tight_layout()plt.show() 图像如下： 此外，我们绘制一下相同数字的多个示例： 123456789fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)ax = ax.flatten()for i in range(25): img = X_train[y_train==7][i].reshape(28, 28) ax[i].imshow(img, cmap=&#x27;Greys&#x27;, interpolation=&#x27;nearest&#x27;)ax[0].set_xticks([])ax[0].set_yticks([])plt.tight_layout()plt.show() 图像如下： 实现一个多层感知器接下来，我们实现一个包含一个输入层，一个隐层和一个输出层的多层感知器，并且将其用来识别MNIST数据集中的图像，整体代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596import numpy as npimport sysclass NeuralNetMLP(object): def __init__(self, n_hidden=30, l2=0., epochs=100, eta=0.001, shuffle=True, minibatch_size=1, seed=None): self.random = np.random.RandomState(seed) self.n_hidden = n_hidden self.l2 = l2 self.epochs = epochs self.eta = eta self.shuffle = shuffle self.minibatch_size = minibatch_size def _onehot(self, y, n_classes): onehot = np.zeros((n_classes, y.shape[0])) for idx, val in enumerate(y.astype(int)): onehot[val, idx] = 1 return onehot.T def _sigmoid(self, Z): return 1. / (1. + np.exp(-np.clip(z, -250, 250))) def _forward(self, X): # step 1: net input of hidden layer z_h = np.dot(X, self.w_h) + self.b_h # step 2: activation of hidden layer a_h = self._sigmoid(z_h) # step 3: net input of output layer z_out = np.dot(a_h, self.w_out) + self.b_out # step 4: activation output layer a_out = self._sigmoid(z_out) return z_h, a_h, z_out, a_out def _compute_cost(self, y_enc, output): L2_term = (self.l2 * (np.sum(self.w_h ** 2.) + np.sum(self.w_out ** 2.))) term1 = -y_enc * (np.log(output)) term2 = (1. - y_enc) * np.log(1. - output) cost = np.sum(term1 - term2) + L2_term return cost def predict(self, X): z_h, a_h, z_out, a_out = self._forward(X) y_pred = np.argmax(z_out, axis=1) return y_pred def fit(self, X_train, y_train, X_valid, y_valid): n_output = np.unique(y_train).shape[0] n_features = X_train.shape[1] # weight initialization # weights for input -&gt; hidden self.b_h = np.zeros(self.n_hidden) self.w_h = self.random.normal(loc=0.0, scale=0.1, size=(n_features, self.n_hidden)) # weights for hidden -&gt; output self.b_out = np.zeros(n_output) self.w_out = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden, n_output)) epoch_strlen = len(str(self.epochs)) self.eval_ = &#123;&#x27;cost&#x27;: [], &#x27;train_acc&#x27;: [], &#x27;valid_acc&#x27;: []&#125; y_train_enc = self._onehot(y_train, n_output) # iteration over training epochs for i in range(self.epochs): indices = np.arange(X_train.shape[0]) if self.shuffle: self.random.shuffle(indices) for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size): batch_idx = indices[start_idx:start_idx + self.minibatch_size] z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx]) # Backpropagation sigma_out = a_out - y_train_enc[batch_idx] sigmoid_derivative_h = a_h * (1. - a_h) sigma_h = (np.dot(sigma_out, self.w_out.T) * sigmoid_derivative_h) grad_w_h = np.dot(a_h.T, sigma_out) grad_b_out = np.sum(sigma_out, axis=0) delta_w_h = (grad_w_h + self.l2*self.w_h) delta_b_h = grad_b_h # bias is not regularized self.w_h -= self.eta * delta_w_h self.b_h -= self.eta * delta_b_h delta_w_out = (grad_w_out + self.l2*self.w_out) delta_b_out = grad_b_out # bias is not regularized self.w_out -= self.eta * delta_w_out self.b_out -= self.eta * delta_b_out # evaluation z_h, a_h, z_out, a_out = self._forward(X_train) cost = self._compute_cost(y_enc=y_train_enc, output=a_out) y_train_pred = self.predict(X_train) y_valid_pred = self.predict(X_valid) train_acc = ((np.sum(y_train ==y_train_pred)).astype(np.float) / X_train.shape[0]) valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) / X_valid.shape[0]) sys.stderr.write(&#x27;\\r%0*d/%d | Cost: %.2f &#x27;&#x27;| Train/Valid Acc.: %.2f%%/%.2f%% &#x27; % (epoch_strlen, i+1, self.epochs, cost, train_acc*100, valid_acc*100)) sys.stderr.flush() self.eval_[&#x27;cost&#x27;].append(cost) self.eval_[&#x27;train_acc&#x27;].append(train_acc) self.eval_[&#x27;valid_acc&#x27;].append(valid_acc) return self 接下来我们初始化一下784-100-10的MLP： 12nn = NeuralNetMLP(n_hidden=100, l2=0.01, epochs=200, eta=0.0005, minibatch_size=100, shuffle=True, seed=1) 首先看一下参数的含义： l2:：l2正则化系数$ \\lambda $ epochs：遍历训练集的次数（遍历次数） eta：学习速率$ \\eta $ shuffle：每次迭代前打乱训练集的数据 seed：打乱数据和权重初始化的随机种子 minibatch_size：在每个小批次中训练样本的数目 梯度每个批次分别计算，而不是在整个训练数据集上进行计算，这样做是为了加快学习的速率。 接下来进行训练： 123nn.fit(X_train=X_train[:55000], y_train=y_train[:55000], X_valid=X_train[55000:], y_valid=y_train[55000:])&gt;&gt; 200/200 | Cost: 15345.39 | Train/Valid Acc.: 96.10%/96.40% 我们在上述实现中，我们也定义了eval_用来保存每次迭代后的代价值，我们将其绘制出来： 12345import matplotlib.pyplot as pltplt.plot(range(nn.epochs), nn.eval_[&#x27;cost&#x27;])plt.xlabel(&#x27;Cost&#x27;)plt.ylabel(&#x27;Epochs&#x27;)plt.show() 得到的图像如下： 可以得到前100次cost的值下降得很快，之后随着迭代次数增加，cost值下降不明显。 接下来看一下训练和验证率得变化： 123456plt.plot(range(nn.epochs), nn.eval_[&#x27;train_acc&#x27;], label=&#x27;training&#x27;)plt.plot(range(nn.epochs), nn.eval_[&#x27;valid_acc&#x27;], label=&#x27;validation&#x27;, linestyle=&#x27;--&#x27;)plt.xlabel(&#x27;Accuracy&#x27;)plt.ylabel(&#x27;Epochs&#x27;)plt.legend()plt.show() 图像如下： 可以发现在迭代次数175之前，拟合模型有点欠拟合。最后我们看一下预测准确率： 1234y_test_pred = nn.predict(X_test)acc = (np.sum(y_test == y_test_pred)).astype(np.float) / X_test.shape[0]print(&#x27;Acc: %.3f&#x27; % acc)&gt;&gt; Acc: 0.959 可以发现我们的模型在测试集上准确率差不多是96%，在数值上接近训练集中验证的准确率，表明模型拟合程度较好。 最后，看一下一些图片和我们MLP预测结果的示例图： 12345678910111213miscl_img = X_test[y_test != y_test_pred][125:150]correct_lab = y_test[y_test != y_test_pred][125:150]miscl_lab = y_test_pred[y_test != y_test_pred][25:50]fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)ax = ax.flatten()for i in range(25): img = miscl_img[i].reshape(28, 28) ax[i].imshow(img, cmap=&#x27;Greys&#x27;, interpolation=&#x27;nearest&#x27;) ax[i].set_title(&#x27;%d) t: %d p: %d&#x27; % (i+1, correct_lab[i], miscl_lab[i]))ax[0].set_xticks([])ax[0].set_yticks([])plt.tight_layout()plt.show() 得到的图像如下： 图片上的第二个数字表示的是正确的类标（true class），第三个数字表示的是预测的类标（predicted class）。可以发现，某些图像即便让人工分类也存在一定的困难度。 训练人工神经网络接下来我们看一下人工神经网络的一些深层的概念，如用于权值更新过程中的逻辑斯蒂代价函数和反向传播算法。 计算逻辑斯蒂代价函数在_compute_cost方法中实现的逻辑斯蒂代价函数如下：$$J(w) &#x3D; -\\sum_{i&#x3D;1}^{n}y^ilog(a^i)+(1-y^i)log(1-a^i)$$其中，$ a^i $是前向传播过程中，用来计算第i个单元的sigmoid激励函数：$$a^i &#x3D; \\phi(z^i)$$接下来，我们添加一个正则化项，它可以降低过拟合的程度，L2正则化定义如下：$$L2：&#x3D;\\lambda ||w||^2_2 &#x3D; \\lambda\\sum_{j&#x3D;1}^{m}w_j^2$$通过在逻辑斯蒂代价函数中加入L2正则化项，得到：$$J(w) &#x3D; -\\sum_{i&#x3D;1}^{n}y^ilog(a^i)+(1-y^i)log(1-a^i) &#x3D;\\lambda ||w||^2_2$$我们已经实现了一个用于多分类的MLP，它返回一个包含t个元素的输出向量，我们需要将这个输出向量和使用独热编码表示的 $ t \\times 1 $维目标向量进行比较。例如，对于一个样本，它在第三层的激励和目标类别（此处是2）可能如下：$$a^3 &#x3D; \\begin{bmatrix}0.1 \\0.9 \\\\vdots \\0.3\\end{bmatrix},y &#x3D; \\begin{bmatrix}0 \\1 \\\\vdots \\0\\end{bmatrix}$$由此，我们需要逻辑斯蒂函数应用到网络中的所有激励单元j中。因此代价函数（未增加正则化项）：$$J(w) &#x3D; -\\sum_{i&#x3D;1}^{n}\\sum_{j&#x3D;1}^{t}y^i_jlog(a^i_j)+(1-y^i_j)log(1-a^i_j)$$这里，上标i表示的是第在训练集中的第i个样本。加入正则化项的公式如下：$$J(w) &#x3D; -\\left[\\sum_{i&#x3D;1}^{n}\\sum_{j&#x3D;1}^{t}y^i_jlog(a^i_j)+(1-y^i_j)log(1-a^i_j)\\right] \\frac{\\lambda}{2}\\sum_{l&#x3D;1}^{L-1}\\sum_{i&#x3D;1}^{u_l}\\sum_{j&#x3D;1}^{u_l+1}(w_{j,i}^l)^2$$在这里，$ u_l $表示第$ l $层的数目。我们的目标是最小化$ j(W) $代价函数，因此我们需要计算出网络中各层权重的偏导：$$\\frac{\\partial}{\\partial{w_{j,i}^l}}J(W)$$注意$ W $包含多个矩阵，在一个仅仅包含一个隐层单元的MLP中，$ W^h $连接输入层和隐层，$ W^{out} $连接隐层和输出层。下图对$ W $进行可视化： 通过反向传播来训练神经网络回忆本章中介绍的内容，我们需要通过正向传播来获得输出层的激励：$$Z^h &#x3D; A^{in}W^h (隐层的净输入)\\A^h &#x3D; \\phi(Z^h) (隐层的激励)\\Z^{out} &#x3D; A^hW^{out} (输出层的净输出)\\A^{out} &#x3D; \\phi(Z^{out})(输出层的激励)$$简单说，我们按照下图处理输入： 后向传播中，我们将误差从右向左传递。首先计算输出层的误差向量：$$\\delta^{out} &#x3D; a^{out} - y$$其中，$ y $是真实类标的向量。接下来，我们计算隐层的误差项：$$\\delta^{h} &#x3D; \\delta^{out}(W^{out})^T \\odot \\frac{\\partial\\phi(z^h)}{\\partial z^h}$$这里，$ \\frac{\\partial\\phi(z^h)}{\\partial z^h} $计算公式如下：$$\\frac{\\partial\\phi(z^h)}{\\partial z^h} &#x3D; \\left(a^h \\odot (1-a^h)\\right)$$在这里，$ \\odot $表示的是数组元素依次相乘符号。 相应的，$ \\delta^h $计算公式如下：$$\\delta^{h} &#x3D; \\delta^{out}(W^{out})^T \\odot \\left(a^h \\odot (1-a^h)\\right)$$在得到$ \\delta $后，我们可以将代价函数的偏导记作：$$\\frac{\\partial}{\\partial w_{i, j}^{out}}J(W) &#x3D; a_j^h\\delta_i^{out}\\\\frac{\\partial}{\\partial w_{i, j}^h}J(W) &#x3D; a_j^{in}\\delta_i^h$$综上，我们通过下图进行反向传播总结： 神经网络的收敛性在前面实现的训练手写数字的神经网络过程中，没有使用传统的梯度下降，而是使用小批次样本学习来替代。随机梯度下降每次仅使用一个样本（k&#x3D;1）更新权重来进行，虽然这是一种随机的方法，但相较于传统梯度下降，它通常嗯能得到精度极高的训练结果，并且收敛速度更快。子批次学习是随机梯度下降的一个特例：从包含n个样本的训练数据中随机抽取k个用于训练，其中1&lt;k&lt;n。 神经网络的输出函数的曲线并不平滑，而且容易陷入局部最优值，如下图：","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"聚类分析之处理无类标数据","slug":"聚类分析之处理无类标数据","date":"2020-02-09T03:36:44.000Z","updated":"2022-05-16T07:41:46.364Z","comments":true,"path":"2020/02/09/聚类分析之处理无类标数据/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/09/%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E4%B9%8B%E5%A4%84%E7%90%86%E6%97%A0%E7%B1%BB%E6%A0%87%E6%95%B0%E6%8D%AE/","excerpt":"前面几章中，我们使用的数据都是事先已经直到预测结果的，即训练数据中已提供了数据的类标。在本章中，我们转而研究聚类分析，它是一种无监督学习技术，可以在事先不知道正确结果的情况下，发现数据本身所蕴含的结构等信息。","text":"前面几章中，我们使用的数据都是事先已经直到预测结果的，即训练数据中已提供了数据的类标。在本章中，我们转而研究聚类分析，它是一种无监督学习技术，可以在事先不知道正确结果的情况下，发现数据本身所蕴含的结构等信息。 使用k-means算法对相似对象进行分组本节讨论最流行的聚类算法：k-means算法，它在学术邻域及业界都得到了广泛应用。聚类是一种可以找到相似对象群组的技术，与组间对象相比，组内对象之间具有更高的相似度。 尽管k-means算法适用于高维数据，但出于可视化需要，我们使用一个二维数据集的例子演示： 123456789101112from sklearn.datasets import make_blobsimport matplotlib.pyplot as pltX, y = make_blobs(n_samples=150, n_features=2, centers=3, cluster_std=0.5, shuffle=True, random_state=0)plt.scatter(X[:, 0], X[:, 1], c=&#x27;blue&#x27;, marker=&#x27;o&#x27;, s=50)plt.grid()plt.show() 图像如下： k-means算法具体有四个步骤： 从样本点随机选择k个点作为初始簇中心 将每个样本点划分到距离它最近的中心点$ \\mu^j, j\\in{1,\\cdots ,k} $所代表的簇中 用各簇中所有样本的中心点替代原有的中心点 重复步骤2和3，直到中心点不变或者达到预定迭代次数时，算法终止 度量对象之间的相似性可以用欧几里得距离的平方：$$d(x, y)^2 &#x3D; \\sum_{j&#x3D;1}^{m}(x_j-y_j)^2&#x3D;||x-y||^2_2$$基于欧几里得标准，我们可以将k-means算法描述为一个简单的优化问题，也就是使得簇内误差平方和（SSE）最小：$$SSE &#x3D; \\sum_{j&#x3D;1}^n\\sum_{j&#x3D;1}^{k}w^{i,j}&#x3D;||x^i-\\mu^j||_2^2$$现在借助scikit-learn中的KMeans类将k-means算法应用于我们的示例数据集： 12345678from sklearn.cluster import KMeanskm = KMeans(n_clusters=3, init=&#x27;random&#x27;, n_init=10, max_iter=300, tol=1e-04, random_state=0)y_km = km.fit_predict(X) 在k-means算法的某次迭代中，可能会发生无法收敛的问题，特别是我们设置了较大的max_iter。解决这个问题的方法是为tol参数设置一个较大的值，上述容忍度为1e-04。 k-means++我们讨论了经典的k-means算法，它使用随机点作为初始中心点，但是初始中心点选择不当，就会导致收敛速度慢的问题。解决此问题的方法是在数据集上多次运行k-mean算法，并且根据SSE选择性能更好的模型。另外一种方案是使用k-means++算法让初始中心点彼此尽可能远离，相比传统的k-means算法，它能够产生更好的结果。k-means++算法的初始化过程如下： 初始化一个空的集合M，用于存储选定的k个中心点 从输入样本中随机选定第一个中心点$ \\mu^j $，并且将其加入到集合M中 对于集合M之外的任一样本点$ x^i $，通过计算找到与其平方距离最小的样本$ d(x^i, M)^2 $ 使用加权概率分布$ \\frac{d(\\mu, M)^2}{\\sum_id(x^i, M)^2} $来随机选择下一个中心点$ \\mu^p $ 重复步骤2，3，直到选定k个中心点 基于选定的中心点执行k-means算法 现在对k-means算法的结果做可视化展示： 123456789101112131415161718192021222324252627plt.scatter(X[y_km==0, 0], X[y_km==0, 1], s=50, c=&#x27;lightgreen&#x27;, marker=&#x27;s&#x27;, label=&#x27;cluster 1&#x27;)plt.scatter(X[y_km==1, 0], X[y_km==1, 1], s=50, c=&#x27;orange&#x27;, marker=&#x27;o&#x27;, label=&#x27;cluster 2&#x27;)plt.scatter(X[y_km==2, 0], X[y_km==2, 1], s=50, c=&#x27;lightblue&#x27;, marker=&#x27;v&#x27;, label=&#x27;cluster 3&#x27;)plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=250, marker=&#x27;*&#x27;, c=&#x27;red&#x27;, label=&#x27;centroids&#x27;)plt.legend()plt.grid()plt.show() 图像如下： 散点图显示的结果中3个中心点位于各个簇的中心，分组结果看起来是合理的。 k-means算法的一个缺点是我们必须先指定一个簇数量k，但是在实际应用中，簇数量并不总是显而易见的。 硬聚类与软聚类硬聚类指每个样本只能划至一个簇的算法，如k-means算法；软聚类算法可以将样本划分到一个或多个簇，如FCM算法。 FCM算法和k-means算法十分相似，k-means算法某个样本预测结果是$ [0,1,0] $，表明该样本属于簇2。FCM中可以允许预测的结果中含有分数，如$ [0.7, 0.2, 0.1] $表明该样本属于簇1的概率是0.7，簇2的概率是0.2。FCM的步骤如下： 指定k个中心点，并随机将样本点划分至某个簇 计算各个簇的中心$ \\mu^j ,j\\in{1, \\cdots,k}$ 更新各样本点所属簇的成员隶属度 重复步骤2，3，直到各个样本点所属簇成员隶属度不变或者是达到最大的迭代次数 FCM的目标函数如下：$$J_m &#x3D; \\sum_{i&#x3D;1}^{n}\\sum_{j&#x3D;1}^{m}w^m(i,j)||x^i-\\mu^j||^2_2$$ 使用肘方法确定簇的最佳数量簇内误差平方和可以通过inertia访问，基于簇内误差平方和，我们可以使用图形工具，即所谓的肘方法，针对特定任务估计出最优的簇数量k。 12345678910111213distortions = []for i in range(1, 11): km = KMeans(n_clusters=i, init=&#x27;k-means++&#x27;, n_init=10, max_iter=300, random_state=0) km.fit(X) distortions.append(km.inertia_)plt.plot(range(1, 11), distortions, marker=&#x27;o&#x27;)plt.xlabel(&#x27;Number of clusters&#x27;)plt.ylabel(&#x27;Distortion&#x27;)plt.show() 图像如下： 如图，当k&#x3D;3时团呈现肘形，这表明对于此数据来说，k&#x3D;3是一个好的选择。 通过轮廓图定量分析聚类质量另外一种聚类质量的评估方法时轮廓分析，此方法用于k-means算法之外的其他聚类方法。我们通过以下步骤计算轮廓系数： 将某样本$ x^i $与簇内其他点之间的平均距离看作是簇的内聚度$ a^i $ 将样本$ x^i $与其最近簇中所有点之间的平均距离看作是与下一最近簇的分离度$ b^i $ 轮廓系数如下：$$s^i &#x3D; \\frac{b^i - a^i}{max{b^i, a^i}}$$ 可以发现，理想的轮廓系数时1，轮廓系数代码如下： 1234567891011121314151617181920212223242526272829303132333435363738km = KMeans(n_clusters=3, init=&#x27;k-means++&#x27;, n_init=10, max_iter=300, tol=1e-04, random_state=0)y_km = km.fit_predict(X)import numpy as npfrom matplotlib import cmfrom sklearn.metrics import silhouette_samplescluster_labels = np.unique(y_km)n_clusters = cluster_labels.shape[0]silhouette_vals = silhouette_samples(X, y_km, metric=&#x27;euclidean&#x27;)y_ax_lower, y_ax_upper = 0, 0yticks = []for i, c in enumerate(cluster_labels): c_silhouette_vals = silhouette_vals[y_km == c] c_silhouette_vals.sort() y_ax_upper += len(c_silhouette_vals) color = cm.jet(float(i) / n_clusters) plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor=&#x27;none&#x27;, color=color) yticks.append((y_ax_lower + y_ax_upper) / 2.) y_ax_lower += len(c_silhouette_vals)silhouette_avg = np.mean(silhouette_vals)plt.axvline(silhouette_avg, color=&quot;red&quot;, linestyle=&quot;--&quot;)plt.yticks(yticks, cluster_labels + 1)plt.ylabel(&#x27;Cluster&#x27;)plt.xlabel(&#x27;Silhouette coefficient&#x27;)plt.show() 图像如下： 从上图可见，轮廓系数未接近0点，此指标显示聚类效果不错。为了解聚类效果不佳的轮廓图的形状，我们使用两个中心点来初始化k-means算法： 1234567891011121314151617181920212223242526272829km = KMeans(n_clusters=2, init=&#x27;k-means++&#x27;, n_init=10, max_iter=300, tol=1e-04, random_state=0)y_km = km.fit_predict(X)plt.scatter(X[y_km==0, 0], X[y_km==0, 1], s=50, c=&#x27;lightgreen&#x27;, marker=&#x27;s&#x27;, label=&#x27;cluster 1&#x27;)plt.scatter(X[y_km==1, 0], X[y_km==1, 1], s=50, c=&#x27;orange&#x27;, marker=&#x27;o&#x27;, label=&#x27;cluster 2&#x27;)plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=250, marker=&#x27;*&#x27;, c=&#x27;red&#x27;, label=&#x27;centroids&#x27;)plt.legend()plt.grid()plt.show() 图像如下： 接下来，我们绘制轮廓图来对聚类结果进行评估： 123456789101112131415161718192021222324252627cluster_labels = np.unique(y_km)n_clusters = cluster_labels.shape[0]silhouette_vals = silhouette_samples(X, y_km, metric=&#x27;euclidean&#x27;)y_ax_lower, y_ax_upper = 0, 0yticks = []for i, c in enumerate(cluster_labels): c_silhouette_vals = silhouette_vals[y_km == c] c_silhouette_vals.sort() y_ax_upper += len(c_silhouette_vals) color = cm.jet(float(i) / n_clusters) plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor=&#x27;none&#x27;, color=color) yticks.append((y_ax_lower + y_ax_upper) / 2.) y_ax_lower += len(c_silhouette_vals)silhouette_avg = np.mean(silhouette_vals)plt.axvline(silhouette_avg, color=&quot;red&quot;, linestyle=&quot;--&quot;)plt.yticks(yticks, cluster_labels + 1)plt.ylabel(&#x27;Cluster&#x27;)plt.xlabel(&#x27;Silhouette coefficient&#x27;)plt.show() 由结果可见，轮廓图由明显不同的长度和宽度，这说明该聚类并非最优结果： 层次聚类本节中，我们将学习另外一种基于原型的聚类：层次聚类。层次聚类算法的优势在于：他能够使我们绘制出树状图，这有助于我们使用有意义的分类解释聚类结果。层次聚类的另外一个优势在于我们无需指定簇数量。 层次聚类有两种主要方法：凝聚层次聚类和分裂层次聚类。在凝聚层次聚类中，判定簇间距离的两个标准方法分别是单连接和全连接。单连接方法计算每一对簇中最相似两个样本的距离，并且合并距离最近的两个样本所属簇。与之相反，全连接方法是通过比较找到分布于两个簇中最不相似的样本（距离最远的样本），进而完成簇的合并。 本节中，我们主要关注基于全连接方法的凝聚层次聚类，迭代过程如下： 计算得到所有样本间的距离矩阵 将每个数据点看作是一个单独的簇 基于最不相似（距离最远）样本的距离，合并两个最接近的簇 更新相似矩阵 重复步骤2到4，直到所有样本都合并到一个簇为止 计算距离矩阵的方式如下： 12345678import pandas as pdimport numpy as npnp.random.seed(123)variables = [&#x27;X&#x27;, &#x27;Y&#x27;, &#x27;Z&#x27;]labels = [&#x27;ID_0&#x27;, &#x27;ID_1&#x27;, &#x27;ID_2&#x27;, &#x27;ID_3&#x27;, &#x27;ID_4&#x27;]X = np.random.random_sample([5, 3])*10df = pd.DataFrame(X, columns=variables, index=labels)df 得到的数据如下： 基于距离矩阵进行层次聚类我们基于SciPy来计算距离矩阵： 123from scipy.spatial.distance import pdist, squareformrow_dist = pd.DataFrame(squareform(pdist(df, metric=&#x27;euclidean&#x27;)), columns=labels, index=labels)row_dist 得到的数据如下： 接下来我们使用linkage函数，此函数以全连接作为距离判定标准： 1234from scipy.cluster.hierarchy import linkagerow_clusters = linkage(df.values, method=&#x27;complete&#x27;, metric=&#x27;euclidean&#x27;)pd.DataFrame(row_clusters, columns=[&#x27;row label 1&#x27;, &#x27;row label 2&#x27;, &#x27;distance&#x27;, &#x27;no. of items&#x27;], index=[&#x27;cluster %d&#x27; % (i+1) for i in range(row_clusters.shape[0])]) 得到的数据如下： 接下来采用树状图的形式对聚类结果进行可视化展示： 12345from scipy.cluster.hierarchy import dendrogramrow_dendr = dendrogram(row_clusters, labels=labels)plt.tight_layout()plt.ylabel(&#x27;Euclidean distance&#x27;)plt.show() 得到的图像如下： 此树状图采用了凝聚层次聚类合并生成不同簇的过程，从图中可见，首先ID_0和ID_4合并，解下来是ID_1和ID_2合并。 树状图与热度图的关联实际应用中，层次聚类的树状图与热度图结合使用，本节中我们讨论如何将树状图附加到热度图上： 123456789101112131415fig = plt.figure(figsize=(8, 8))axd = fig.add_axes([0.09, 0.1, 0.2, 0.6])row_dendr = dendrogram(row_clusters, orientation=&#x27;left&#x27;)df_rowclust = df.ix[row_dendr[&#x27;leaves&#x27;][::-1]]axm = fig.add_axes([0.23, 0.1, 0.6, 0.6])cax = axm.matshow(df_rowclust, interpolation=&#x27;nearest&#x27;, cmap=&#x27;hot_r&#x27;)axd.set_xticks([])axd.set_yticks([])for i in axd.spines.values(): i.set_visible(False)fig.colorbar(cax)axm.set_xticklabels([&#x27;&#x27;] + list(df_rowclust.columns))axm.set_yticklabels([&#x27;&#x27;] + list(df_rowclust.index))plt.show() 得到图像可得： 通过scikit-learn进行凝聚聚类本节使用scikit-learn进行基于凝聚的层次聚类： 12345from sklearn.cluster import AgglomerativeClusteringac = AgglomerativeClustering(n_clusters=2, affinity=&#x27;euclidean&#x27;, linkage=&#x27;complete&#x27;)labels = ac.fit_predict(X)print(&#x27;Cluster labels: %s&#x27; % labels)&gt;&gt; Cluster labels: [0 1 1 0 0] 通过对簇类标的预测结果进行分析，我们可以看出第一第四第五样本被划分至第一个簇，第二第三样本被划分到第二个簇。 使用DBSCAN划分高密度区域接下来我们介绍另外一种聚类方法：基于密度空间的聚类算法。在DBSCAN中，基于一下标准，每个样本都被赋予了一个特殊的标签： 如果一个点周边的指定半径$ \\epsilon $内，其他样本点的数量不小于指定数量（MinPts），则此样本点称为核心点（core point） 在指定半径$ \\epsilon $内，如果一个点的邻居数量小于MinPts时，但是却包含一个核心点，则此点称为边界点（border point） 除了核心点和边界点外的其他样本点称为噪声点（noise point） 完成对核心点，边界点和噪声点的标记后，DBSCAN算法可以总结为两个简单的步骤： 基于每个核心点或者一组相连的核心点形成一个单独的簇 将每个边界点划分到对应核心点所在的簇中 为了给出一个更能说明问题的例子，我们创建一个半月形的数据集，以及k-means聚类，层次聚类和DBSCAN聚类进行比较，首先得到半月形数据集： 1234from sklearn.datasets import make_moonsX, y = make_moons(n_samples=200, noise=0.05, random_state=0)plt.scatter(X[:, 0], X[:, 1])plt.show() 得到的图像如下： 下面首先使用前面讨论过的k-means算法和基于全连接的层次聚类算法，算法如下： 1234567891011121314151617181920212223242526272829303132333435363738f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))km = KMeans(n_clusters=2, random_state=0)y_km = km.fit_predict(X)ax1.scatter(X[y_km==0,0], X[y_km==0,1], c=&#x27;lightblue&#x27;, edgecolor=&#x27;black&#x27;, marker=&#x27;o&#x27;, s=40, label=&#x27;cluster 1&#x27;)ax1.scatter(X[y_km==1,0], X[y_km==1,1], c=&#x27;red&#x27;, edgecolor=&#x27;black&#x27;, marker=&#x27;s&#x27;, s=40, label=&#x27;cluster 2&#x27;)ax1.set_title(&#x27;K-means clustering&#x27;)ac = AgglomerativeClustering(n_clusters=2, affinity=&#x27;euclidean&#x27;, linkage=&#x27;complete&#x27;)y_ac = ac.fit_predict(X)ax2.scatter(X[y_ac==0,0], X[y_ac==0,1], c=&#x27;lightblue&#x27;, edgecolor=&#x27;black&#x27;, marker=&#x27;o&#x27;, s=40, label=&#x27;cluster 1&#x27;)ax2.scatter(X[y_ac==1,0], X[y_ac==1,1], c=&#x27;red&#x27;, edgecolor=&#x27;black&#x27;, marker=&#x27;s&#x27;, s=40, label=&#x27;cluster 2&#x27;)ax2.set_title(&#x27;Agglomerative clustering&#x27;)plt.legend()plt.show() 得到的图像如下： 可以发现，上述两个算法无法有效分开两个数组。最后我们试一下DBSCAN算法在此数据集上的效果： 123456789101112131415161718192021from sklearn.cluster import DBSCANdb = DBSCAN(eps=0.2, min_samples=5, metric=&#x27;euclidean&#x27;)y_db = db.fit_predict(X)plt.scatter(X[y_db==0,0], X[y_db==0,1], c=&#x27;lightblue&#x27;, edgecolor=&#x27;black&#x27;, marker=&#x27;o&#x27;, s=40, label=&#x27;cluster 1&#x27;)plt.scatter(X[y_db==1,0], X[y_db==1,1], c=&#x27;red&#x27;, edgecolor=&#x27;black&#x27;, marker=&#x27;s&#x27;, s=40, label=&#x27;cluster 2&#x27;)plt.legend()plt.show() 得到的图像如下： 可以发现，DBSCAN算法可以成功地对半月形数据进行划分，这也是DBSCAN算法的优势。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"使用回归分析预测连续型目标变量","slug":"使用回归分析预测连续型目标变量","date":"2020-02-08T04:32:51.000Z","updated":"2022-05-16T07:41:46.287Z","comments":true,"path":"2020/02/08/使用回归分析预测连续型目标变量/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/08/%E4%BD%BF%E7%94%A8%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%E8%BF%9E%E7%BB%AD%E5%9E%8B%E7%9B%AE%E6%A0%87%E5%8F%98%E9%87%8F/","excerpt":"本章将会介绍监督学习的另外一个分支，回归分析（regression analysis）。回归模型可以用于连续型目标变量的预测分析，这使得它在探寻变量间关系，评估趋势，做出预测等领域极具吸引力。具体的例子如预测公司在未来几个月的销售情况等。","text":"本章将会介绍监督学习的另外一个分支，回归分析（regression analysis）。回归模型可以用于连续型目标变量的预测分析，这使得它在探寻变量间关系，评估趋势，做出预测等领域极具吸引力。具体的例子如预测公司在未来几个月的销售情况等。 简单线性回归模型初探简单（单变量）线性回归的目标是：通过模型来描述某一特征（解释变量x）与输出变量（目标变量y）之间的关系。当只有一个解释变量时，线性模型函数定义如下：$$y &#x3D; w_0 + w_1x$$其中，$ w_0 $为函数在y轴上的截距，$ w_1 $为解释变量的系数。 基于前面定义的线性方程，线性回归可以看作是求解样本点的最佳拟合直线，如下图： 这条最佳拟合线称作是回归线，回归线和样本点之间的垂直连线就是偏移或残差。 多元线性回归函数定义如下：$$y &#x3D; w_0x_0+w_1x_1+\\cdots+w_mx_m$$其中，$ w_0 $时$ x_0 &#x3D;1 $时在y轴上的截距。 波士顿房屋数据集在本章的后续内容中，我们将会使用房屋价格（MEDV）作为目标变量，使用其他13个变量中的一个或多个值作为解释变量对其进行预测： 12345import pandas as pddf = pd.read_csv(&#x27;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data&#x27;, header=None, sep=&#x27;\\s+&#x27;)df.columns = [&#x27;CRIM&#x27;, &#x27;ZN&#x27;, &#x27;INDUS&#x27;, &#x27;CHAS&#x27;, &#x27;NOX&#x27;, &#x27;RM&#x27;, &#x27;AGE&#x27;, &#x27;DIS&#x27;, &#x27;RAD&#x27;, &#x27;TAX&#x27;, &#x27;PTRATIO&#x27;, &#x27;B&#x27;, &#x27;LSTAT&#x27;, &#x27;MEDV&#x27;]df.head() 输出如下： 搜索性数据分析（EDA）是机器学习模型训练前的一个重要步骤。首先，借助散点图矩阵，我们可以以可视化的方法汇总显示各不同特征两两之间的关系： 123456import matplotlib.pyplot as pltimport seaborn as snssns.set(style=&#x27;whitegrid&#x27;, context=&#x27;notebook&#x27;)cols = [&#x27;LSTAT&#x27;, &#x27;INDUS&#x27;, &#x27;NOX&#x27;, &#x27;RM&#x27;, &#x27;MEDV&#x27;]sns.pairplot(df[cols], size=2.5)plt.show() 得到的图像如下： 通过此散点图矩阵，我们可以快速了解数据是如何分布的，以及其中是否包含异常值。从右下角子图可以发现：MEDV看似呈正态分布，但是包含几个异常值。 为了量化特征之间的关系，我们创建一个相关系数矩阵。相关系数矩阵是一个包含皮尔逊积矩相关系数，它是用来衡量两两特征间的线性依赖关系。计算公式如下：$$r &#x3D; \\frac{\\sum_{i&#x3D;1}^{n}[(x^i - \\mu_x)(y^i-\\mu_y)]}{\\sqrt{\\sum_{i&#x3D;1}^n(x^i-\\mu_x)^2}\\sqrt{\\sum_{i&#x3D;1}^n(y^i-\\mu_y)^2}}&#x3D;\\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}$$其中，$ \\mu $为样本特征的均值，$ \\sigma_{xy} $为相应的协方差，$ \\sigma_x,\\sigma_y $分别为两个特征的标准差。 通过以下代码，我们计算前5个特征间的相关系数矩阵： 1234567891011121314import numpy as npcm = np.corrcoef(df[cols].values.T)# sns.set(font_scale=1.5)hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt=&#x27;.2f&#x27;, annot_kws=&#123;&#x27;size&#x27;: 15&#125;, yticklabels=cols, xticklabels=cols)hm.set_ylim([5, 0])plt.show() 得到的图像如下： 为了拟合线性回归模型，我们主要关注那些跟目标变量MEDV高度相关的特征。观察前面的相关系数矩阵，可以发现MEDV与变量LSTAT的相关性最大（-0.74）。另一方面，RM和MEDV间的相关性也较高（0.70）。 基于最小二乘法构建线性回归模型接下来我们需要对最优拟合做出判断，在此使用最小二乘法（Ordinary Least Square，OLS）估计回归曲线的参数，使得回归曲线到样本点垂直距离的平方和最小。 通过梯度下降计算回归参数在第二章中介绍的Adaline中使用了一个线性激励函数，同时定义了一个激励函数，可以通过梯度下降（GD），随机梯度下降（SGD）等优化算法使得代价函数最小，从而得到相应的权重。Adaline中的代价函数就是误差平方和（SSE），他等同于我们定义的OLS代价函数：$$J(w) &#x3D; \\frac{1}{2}\\sum_{i&#x3D;1}^n(y^i - \\hat{y^i})^2$$本质上，OLS线性回归可以理解为无单位阶跃函数的Adaline，这样我们的得到的是连续型的输出值，而不是-1或者1的类标。接下来可以看一下线性回归梯度下降代码： 123456789101112131415161718192021222324class LinearRegressionGD(object): def __init__(self, eta=0.001, n_iter=20): self.eta = eta self.n_iter = n_iter def fit(self, X, y): self.w_ = np.zeros(1 + X.shape[1]) self.cost_ = [] for i in range(self.n_iter): output = self.net_input(X) errors = (y - output) self.w_[1:] += self.eta * X.T.dot(errors) self.w_[0] +=self.eta * errors.sum() cost = (errors**2).sum() / 2.0 self.cost_.append(cost) return self def net_input(self, X): return np.dot(X, self.w_[1:] + self.w_[0]) def predict(self, X): return self.net_input(X) 接下来我们使用房屋数据集中的RM（房间数量）作为解释变量来训练模型以预测MEDV（房屋价格）。此外，为了使得梯度下降算法收敛性更佳，在此对相关变量做了标准化处理： 123456789X = df[[&#x27;RM&#x27;]].valuesy = df[&#x27;MEDV&#x27;].valuesfrom sklearn.preprocessing import StandardScalersc_x = StandardScaler()sc_y = StandardScaler()X_std = sc_x.fit_transform(X)y_std = sc_y.fit_transform(y[:, np.newaxis]).flatten()lr = LinearRegressionGD()lr.fit(X_std, y_std) 接下来看一下代价函数和迭代次数的图像： 1234plt.plot(range(1, lr.n_iter+1), lr.cost_)plt.ylabel(&#x27;SSE&#x27;)plt.xlabel(&#x27;Epoch&#x27;)plt.show() 得到的图像如下： 接下来，绘制房间数和房屋价格的关系： 12345678def lin_regplot(X, y, model): plt.scatter(X, y, c=&#x27;b&#x27;) plt.plot(X, model.predict(X), color=&#x27;r&#x27;) return Nonelin_regplot(X_std, y_std, lr)plt.xlabel(&#x27;Average Number&#x27;)plt.ylabel(&#x27;Price&#x27;)plt.show() 得到的图像如下： 从图中可知，随着房间数的增加，房价呈现上涨趋势。但是从图中可以看到，房间数在很多的情况下并不能很好解释房价。对于经过标准化处理的变量，它们的截距必定是0： 12print(&#x27;Slope: %.3f&#x27; % lr.w_[1])print(&#x27;Intercept: %.3f&#x27; % lr.w_[0]) 使用scikit-learn估计回归模型的系数下面，我们使用scikit-learn中的库实现回归分析： 1234567from sklearn.linear_model import LinearRegressionslr = LinearRegression()slr.fit(X, y)print(&#x27;Slope: %.3f&#x27; % slr.coef_[0])print(&#x27;Intercept: %.3f&#x27; % slr.intercept_)&gt;&gt; Slope: 9.102&gt;&gt; Intercept: -34.671 执行代码发现得到了不同的模型系数，现在绘制出图像： 1234lin_regplot(X, y, slr)plt.xlabel(&#x27;Average Number&#x27;)plt.ylabel(&#x27;Price&#x27;)plt.show() 得到的图像如下： 从图中可以看出，总体结果与GD算法实现的模型是一致的。 使用RANSAC拟合高鲁棒性回归模型作为清除异常值的一种高鲁棒性回归方法，在此我们将学习随机抽样一致性（RANSAC）算法，使用数据的一个子集来进行回归模型的拟合。该算法流程如下： 从数据集中随机抽取样本构建内点集合类拟合模型 使用剩余数据对上一步得到的模型进行测试，并将落在预定公差范围内的样本点增至内带你集合中 使用全部内点集合数据再次进行模型的拟合 使用内点集合来估计模型的误差 如果模型性能达到了特定阈值或者迭代达到了预定次数，则算法中止，否则跳转到第1步 首先我们用RANSACRegressor对象来实现我们的线性模型： 12345678from sklearn.linear_model import RANSACRegressorfrom sklearn.linear_model import LinearRegressionransac = RANSACRegressor(LinearRegression(), max_trials=100, min_samples=50, residual_threshold=5.0, random_state=0)ransac.fit(X, y) 完成拟合后，我们接着来绘制内点和异常值图像： 1234567891011inlier_mask = ransac.inlier_mask_outlier_mask = np.logical_not(inlier_mask)line_X = np.arange(3, 10, 1)line_y_ransac = ransac.predict(line_X[:, np.newaxis])plt.scatter(X[inlier_mask], y[inlier_mask], c=&#x27;b&#x27;, marker=&#x27;o&#x27;, label=&#x27;Inliers&#x27;)plt.scatter(X[outlier_mask], y[outlier_mask], c=&#x27;r&#x27;, marker=&#x27;s&#x27;, label=&#x27;Outliers&#x27;)plt.plot(line_X, line_y_ransac, color=&#x27;g&#x27;)plt.xlabel(&#x27;Average Number&#x27;)plt.ylabel(&#x27;Price&#x27;)plt.legend()plt.show() 图像如下： 接下来看一下模型的截距和斜率： 1234print(&#x27;Slope: %.3f&#x27; % ransac.estimator_.coef_[0])print(&#x27;Intercept: %.3f&#x27; % ransac.estimator_.intercept_)&gt;&gt; Slope: 10.735&gt;&gt; Intercept: -44.089 线性回归模型性能的评估现在我们使用数据集中的所有变量训练多元回归模型： 12345678from sklearn.model_selection import train_test_splitX = df.iloc[:, :-1].valuesy = df[&#x27;MEDV&#x27;].valuesX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)slr = LinearRegression()slr.fit(X_train, y_train)y_train_pred = slr.predict(X_train)y_test_pred = slr.predict(X_test) 使用如下代码，我们会绘制出残差图： 12345678plt.scatter(y_train_pred, y_train_pred - y_train, c=&#x27;b&#x27;, marker=&#x27;o&#x27;, label=&#x27;Training data&#x27;)plt.scatter(y_test_pred, y_test_pred - y_test, c=&#x27;r&#x27;, marker=&#x27;s&#x27;, label=&#x27;Test data&#x27;)plt.xlabel(&#x27;Predicted values&#x27;)plt.ylabel(&#x27;Residuals&#x27;)plt.legend()plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color=&#x27;red&#x27;)plt.xlim([-10, 50])plt.show() 得到的残差图如下： 完美的预测结果其残差为0，但是在实际的应用中，这种情况不会出现。不过，对于一个好的回归模型，我们期望误差是随机分布在中心线附近的。 另外一种对模型性能进行定量评估的方法是均方误差（Mean Squared Error，MSE），计算公式如下：$$MSE &#x3D; \\frac{1}{n}\\sum_{i&#x3D;1}^{n}(y^i - \\hat{y^i})^2$$执行如下代码： 123from sklearn.metrics import mean_squared_errorprint(&#x27;MSE train: %.3f, test: %.3f&#x27; % (mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)))&gt;&gt; MSE train: 19.958, test: 27.196 从结果而知，训练集上的MSE值为19.96，测试集上的MSE值骤升为27.20，这意味着我们的模型过拟合于训练数据。 某些情况下也可以使用决定系数来进行评估，它的计算公式如下：$$R^2 &#x3D; 1 - \\frac{MSE}{Var(y)}$$可以使用如下代码来计算$ R^2 $： 123from sklearn.metrics import r2_scoreprint(&#x27;R^2 train: %.3f, test: %.3f&#x27; % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))&gt;&gt; R^2 train: 0.765, test: 0.673 回归中的正则化方法正则化是通过在模型中加入额外信息来解决过拟合问题的一种方法，引入罚项增加了模型的复杂度但却降低了模型了模型参数的影响。最常见的正则化线性回归方法就是所谓的岭回归（Ridge Regression），最小绝对收缩及算子选择（LASSO）以及弹性网络（Elastic Net）。 岭回归是基于L2罚项的模型，我们只是在最小二乘代价函数中加入了权重的平方和：$$J(w){Ridge} &#x3D; \\sum{i&#x3D;1}^{n}(y^i-\\hat{y^i})^2+\\lambda||w||^2_2$$其中：$$L2： \\lambda||w||^2_2&#x3D;\\lambda\\sum_{j&#x3D;1}^{m}w_j^2$$对于稀疏数据训练的模型，还可以使用LASSO：$$J(w){LASSO}&#x3D; &#x3D; \\sum{i&#x3D;1}^{n}(y^i-\\hat{y^i})^2+\\lambda||w||1$$其中：$$L1： \\lambda||w||1 &#x3D; \\lambda\\sum{j&#x3D;1}^{m}|w_j|$$弹性网络如下：$$J(w){ElasticNet} &#x3D; \\sum_{i&#x3D;1}^{n}(y^i-\\hat{y^i})^2+\\lambda_1\\sum_{j&#x3D;1}^{m}w_j^2+\\lambda_2\\sum_{j&#x3D;1}^{m}|w_j|$$scikit-learn中岭回归模型的初始化方法如下： 12from sklearn.linear_model import Ridgeridge = Ridge(alpha=1.0) 正则化强度通过alpha参数来调节，类似于参数$ \\lambda $。 LASSO对象的初始化如下： 12from sklearn.linear_model import Lassolasso = Lasso(alpha=1.0) 最后，scikit-learn下面的ElasticNet允许我们调整L1与L2的比率： 12from sklearn.linear_model import ElasticNetlasso = ElasticNet(alpha=1.0, l1_ratio=0.5) 线性回归模型的曲线化—-多项式回归对于不符合线性假设的问题，一种常用的解释方法就是：$$y &#x3D; w_0+w_1x+w_2x^2+\\cdots+w_dx^d$$接下来我们讨论一下如何使用scikit-learn中的PolynomialFeatures转化类在只含有一个解释变量的简单回归问题中加入二次项。步骤如下： 增加一个二次多项式： 12345678from sklearn.preprocessing import PolynomialFeaturesX = np.array([ 258.0, 270.0, 294.0, 320.0, 342.0, 368.0, 396.0, 446.0, 480.0, 586.0])[:, np.newaxis]y = np.array([ 236.4, 234.4, 252.8, 298.6, 314.2, 342.2, 360.8, 368.0, 391.2, 390.8])lr = LinearRegression()pr = LinearRegression()quadratic = PolynomialFeatures(degree=2)X_quad = quadratic.fit_transform(X) 拟合一个用于对比的简单线性回归模型： 123lr.fit(X, y)X_fit = np.arange(250, 600, 10)[:, np.newaxis]y_lin_fit = lr.predict(X_fit) 使用经过转换后的特征针对多项式回归拟合一个多元线性回归模型： 1234567pr.fit(X_quad, y)y_quad_fit = pr.predict(quadratic.fit_transform(X_fit))plt.scatter(X, y, label=&#x27;training points&#x27;)plt.plot(X_fit, y_lin_fit, label=&#x27;linear fit&#x27;, linestyle=&#x27;--&#x27;)plt.plot(X_fit, y_quad_fit, label=&#x27;quadratic fit&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show() 得到的图像如下： 从图像可以看出，和线性拟合相比，多项式拟合可以更好地捕捉到解释变量和响应变量之间的关系。 12345678910y_lin_pred = lr.predict(X)y_quad_pred = pr.predict(X_quad)print(&#x27;Training MSE linear: %.3f, quadratic: %.3f&#x27; % ( mean_squared_error(y, y_lin_pred), mean_squared_error(y, y_quad_pred)))print(&#x27;Training R^2 linear: %.3f, quadratic: %.3f&#x27; % ( r2_score(y, y_lin_pred), r2_score(y, y_quad_pred)))&gt;&gt; Training MSE linear: 569.780, quadratic: 61.330&gt;&gt; Training R^2 linear: 0.832, quadratic: 0.982 执行上述代码后，MSE的值由线性拟合的570下降到了61。同时和线性拟合结果相比，二次模型的判定系数结果更好，说明二次拟合的效果更好。 房屋数据中的非线性关系建模接下来，我们将会使用二次核三次多项式对房屋价格核LSTAT之间的关系进行建模，并且和线性拟合进行对比： 1234567891011121314151617181920212223242526272829303132333435X = df[[&#x27;LSTAT&#x27;]].valuesy = df[&#x27;MEDV&#x27;].valuesregr = LinearRegression()# create polynomial featuresquadratic = PolynomialFeatures(degree=2)cubic = PolynomialFeatures(degree=3)X_quad = quadratic.fit_transform(X)X_cubic = cubic.fit_transform(X)# linear fitX_fit = np.arange(X.min(), X.max(), 1)[:, np.newaxis]regr = regr.fit(X, y)y_lin_fit = regr.predict(X_fit)linear_r2 = r2_score(y, regr.predict(X))# quadratic fitregr = regr.fit(X_quad, y)y_quad_fit = regr.predict(quadratic.fit_transform(X_fit))quadratic_r2 = r2_score(y, regr.predict(X_quad))# cubic fitregr = regr.fit(X_cubic, y)y_cubic_fit = regr.predict(cubic.fit_transform(X_fit))cubic_r2 = r2_score(y, regr.predict(X_cubic))# plot resultsplt.scatter(X, y, label=&#x27;training points&#x27;, color=&#x27;lightgray&#x27;)plt.plot(X_fit, y_lin_fit, label=&#x27;linear(d=1), R^2 = %.2f&#x27; % linear_r2, color=&#x27;b&#x27;, lw=2, linestyle=&#x27;:&#x27;)plt.plot(X_fit, y_quad_fit, label=&#x27;quadratic(d=2), R^2 = %.2f&#x27; % quadratic_r2, color=&#x27;g&#x27;, lw=2, linestyle=&#x27;-&#x27;)plt.plot(X_fit, y_cubic_fit, label=&#x27;cubic(d=3), R^2 = %.2f&#x27; % cubic_r2, color=&#x27;r&#x27;, lw=2, linestyle=&#x27;--&#x27;)plt.xlabel(&#x27;LSTAT&#x27;)plt.ylabel(&#x27;Price&#x27;)plt.legend()plt.show() 图像如下： 从图像而知，相较于线性拟合和二次拟合，三次拟合更好地捕获了房屋价格与LSTAT之间的关系。不过，加入越来越多的多项式特征会增加模型复杂度，容易造成过拟合。 此外，多项式特征并非总是非线性关系建模的最佳选择。例如，我们仅就MEDV-LSTAT的散点图来说，我们可以将LSTAT特征变量的对数值以及MEDV的平方根映射到一个特征空间，并用线性回归进行拟合： 1234567891011121314151617# transform featuresX_log = np.log(X)y_sqrt = np.sqrt(y)# fit featuresX_fit = np.arange(X_log.min()-1, X_log.max()+1, 1)[:, np.newaxis]regr = regr.fit(X_log, y_sqrt)y_lin_fit = regr.predict(X_fit)linear_r2 = r2_score(y_sqrt, regr.predict(X_log))# plot resultsplt.scatter(X_log, y_sqrt, label=&#x27;training points&#x27;,color=&#x27;lightgray&#x27;)plt.plot(X_fit, y_lin_fit, label=&#x27;linear(d=1), R^2=%.2f&#x27; % linear_r2, color=&#x27;blue&#x27;, lw=2)plt.xlabel(&#x27;LSTAT&#x27;)plt.ylabel(&#x27;Price&#x27;)plt.legend()plt.show() 得到的图像如下： 从$ R^2 $的值可以看出，这种拟合形式优于前面的任何一种多项式回归。 使用随机森林处理非线性关系本节中，我们将会学习随机森林回归，他从概念上异于本章中介绍的其他回归模型。随机森林是多颗决策树的集合，它可以被理解成分段线性函数的集成。 决策树回归 决策树算法的一个优点是我们无需对数据进行特征转换。在scikit-learn中对其进行建模： 12345678910from sklearn.tree import DecisionTreeRegressorX = df[[&#x27;LSTAT&#x27;]].valuesy = df[&#x27;MEDV&#x27;].valuestree = DecisionTreeRegressor(max_depth=3)tree.fit(X, y)sort_idx = X.flatten().argsort()lin_regplot(X[sort_idx], y[sort_idx], tree)plt.xlabel(&#x27;LSTAT&#x27;)plt.ylabel(&#x27;Price&#x27;)plt.show() 图像如下： 在此例中，深度为3的树看起来是比较合适的。 随机森林回归 随机森林算法是组合多颗决策树的一种集成技术。随机森林的一个优势是：它对数据集中的异常值不敏感，且无需过多的参数调优。接下来使用scikit-learn中的库来拟合一个随机森林回归模型： 123456789101112X = df.iloc[:, :-1].valuesy = df[&#x27;MEDV&#x27;].valuesX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)from sklearn.ensemble import RandomForestRegressorforest = RandomForestRegressor(n_estimators=1000, criterion=&#x27;mse&#x27;, random_state=1, n_jobs=-1)forest.fit(X_train, y_train)y_train_pred = forest.predict(X_train)y_test_pred = forest.predict(X_test)print(&#x27;MSE train: %.3f, test: %.3f&#x27; % (mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)))print(&#x27;R^2 train: %.3f, test: %.3f&#x27; % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))&gt;&gt; MSE train: 1.641, test: 11.056&gt;&gt; R^2 train: 0.979, test: 0.878 遗憾的是，我们发现随机森林对于训练数据有些过拟合，接下来看一下预测的残差图： 12345678plt.scatter(y_train_pred, y_train_pred - y_train, c=&#x27;black&#x27;, marker=&#x27;o&#x27;, s=35, alpha=0.5, label=&#x27;Training data&#x27;)plt.scatter(y_test_pred, y_test_pred - y_test, c=&#x27;lightgreen&#x27;, marker=&#x27;s&#x27;, s=35, alpha=0.7, label=&#x27;Test data&#x27;)plt.xlabel(&#x27;Predicted values&#x27;)plt.ylabel(&#x27;Residuals&#x27;)plt.legend()plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color=&#x27;red&#x27;)plt.xlim([-10, 50])plt.show() 图像如下： 可以发现，随机森林的残差图相比线性拟合产生的残差图有了很大的改进。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"在Web中嵌入机器学习模型","slug":"在Web中嵌入机器学习模型","date":"2020-02-08T01:39:29.000Z","updated":"2022-05-16T07:41:46.294Z","comments":true,"path":"2020/02/08/在Web中嵌入机器学习模型/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/08/%E5%9C%A8Web%E4%B8%AD%E5%B5%8C%E5%85%A5%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/","excerpt":"本章中，我们将学习如何将机器学习模型嵌入到Web应用中，不仅仅是分类，还包括从实时数据中学习。","text":"本章中，我们将学习如何将机器学习模型嵌入到Web应用中，不仅仅是分类，还包括从实时数据中学习。 序列化通过scikit-laern拟合的模型正如我们上一章所述，训练机器模型会带来很高的计算成本。当然，我们不希望每次进行预测分析都需要训练模型。模型持久化的一个方法是使用Python内嵌的pickle模块，它使得我们可以在Python对象与字节码之间进行转换（序列化和反序列化），这样我们就可以将分类器当前的状态保存下来。当需要对新的数据进行分类时，可以直接加载已经保存的分类器，而不必再次用训练数据对模型进行训练： 123456789101112import pickleimport osdest = os.path.join(os.getcwd(), &#x27;movieclassifier&#x27;, &#x27;pkl_objects&#x27;)if not os.path.exists(dest): os.makedirs(dest)pickle.dump(stop, open(os.path.join(dest, &#x27;stopwords.pkl&#x27;), &#x27;wb&#x27;), protocol=4)pickle.dump(clf, open(os.path.join(dest, &#x27;classifier.pkl&#x27;), &#x27;wb&#x27;), protocol=4) 由于无需拟合HashingVectorizer，也就不必对其进行持久化操作。相反，我们创建一个新的脚本文件，通过此脚本可以将向量数据导入到当前Python会话中，下面代码以vectorizer.py作为文件名，保存在movieclassifier目录下： 1234567891011121314151617from sklearn.feature_extraction.text import HashingVectorizerimport reimport osimport picklecur_dir = os.path.dirname(__file__)stop = pickle.load(open(os.path.join(cur_dir, &#x27;pkl_objects&#x27;, &#x27;stopwords.pkl&#x27;), &#x27;rb&#x27;))def tokenizer(text): text = re.sub(&#x27;&lt;[^&gt;]*&gt;&#x27;, &#x27;&#x27;, text) text = re.sub(&#x27;[\\W]+&#x27;, &#x27; &#x27;, text.lower()) tokenized = [w for w in text.split() if w not in stop] return tokenizedvect = HashingVectorizer(decode_error=&#x27;ignore&#x27;, n_features=2**21, preprocessor=None, tokenizer=tokenizer) 接下来定位到movieclassifer目录，就可以导入vectorizer及对分类器进行持久化处理： 12345import pickleimport reimport osfrom vectorizer import vectclf = pickle.load(open(os.path.join(&#x27;pkl_objects&#x27;, &#x27;classifier.pkl&#x27;), &#x27;rb&#x27;)) 在成功加载vectorizer以及反序列化分类器后，我们现在使用这些对象对文档样本进行预处理，并且对其进行预测： 1234567import numpy as nplabel = &#123;0: &#x27;negative&#x27;, 1: &#x27;postive&#x27;&#125;example = [&#x27;I love this movie&#x27;]X = vect.transform(example)print(&#x27;Prediction: %s\\nProbability: %.3f%%&#x27; % (label[clf.predict(X)[0]], np.max(clf.predict_proba(X))*100))&gt;&gt; Prediction: postive&gt;&gt; Probability: 81.483% 使用SQLite数据库存储数据本节中，我们将创建一个简单的SQLite数据库以收集Web应用的用户对于预测结果的反馈。SQLite是一个进程内的库，实现了自给自足的、无服务器的、零配置的、事务性的 SQL 数据库引擎。它是一个零配置的数据库，这意味着与其他数据库一样，我们不需要在系统中配置。就像其他数据库，SQLite 引擎不是一个独立的进程，可以按应用程序需求进行静态或动态连接。SQLite 可以直接访问其存储文件。 通过如下代码，我们将在movieclassifier所在目录创建一个新的SQLite数据库，并且向其中插入两条电影评论的示例数据： 12345678910111213141516import sqlite3import osconn = sqlite3.connect(&#x27;reviews.sqlite&#x27;)c = conn.cursor()c.execute(&quot;CREATE TABLE review_db&quot;\\ &quot;(review TEXT, sentiment INTEGER, date TEXT)&quot;)example1 = &#x27;I love this movie&#x27;c.execute(&quot;INSERT INTO review_db&quot;\\ &quot;(review, sentiment, date) VALUES&quot;\\ &quot;(?, ?, DATETIME(&#x27;now&#x27;))&quot;, (example1, 1))example2 = &#x27;I dislike this movie&#x27;c.execute(&quot;INSERT INTO review_db&quot;\\ &quot;(review, sentiment, date) VALUES&quot;\\ &quot;(?, ?, DATETIME(&#x27;now&#x27;))&quot;, (example2, 0))conn.commit()conn.close() 使用Flask开发Web应用上一节中完成了用于电影评论分类的代码，现在来讨论使用Flask框架开发Web应用的基础知识。 第一个Flask Web应用首先，按照如下目录结构创建Web应用的框架： 12341st_flask_app_1/ -app.py -templates/ -first_app.html app.py文件中包含了运行Flask Web应用程序而需要在Python解释器中执行的入口代码。templates目录下面是Flask用到的静态HTML文件。首先，看一下app.py的内容： 12345678910from flask import Flask, render_templateapp = Flask(__name__)@app.route(&#x27;/&#x27;)def index(): return render_template(&#x27;first_app.html&#x27;)if __name__ == &#x27;__main__&#x27;: app.run() 其中需要注意的是路由注解（@app.route(‘&#x2F;‘)）指定触发index函数的URL路径。接下里通过终端窗口执行下列命令启动Web应用： 12python3 app.py&gt;&gt; Running on http://127.0.0.1:5000/ 接下来打开对应的网站，如果一切正常，将会看到如下内容网页： “Hi, this is my first Flask Web app!”。 表单验证本节中，我们使用HTML表单升级Flask Web应用，以及学习如何使用WTForms库收集数据。 新的应用程序所需的目标结构看起来如下： 123456781st_flask_app_1/ -app.py -static -style.css -templates/ -_formhelpers.html -first_app.html -hello.html 以下为修改后的app.py文件内容： 1234567891011121314151617181920212223from flask import Flask, render_template, requestfrom wtforms import Form, TextAreaFiled, validatorsapp = Flask(__name__)class HelloForm(Form): sayhello = TextAreaFiled(&#x27;&#x27;, [validators.DateRequired()])@app.route(&#x27;/&#x27;)def index(): form = HelloForm(request.form) return render_template(&#x27;first_app.html&#x27;, form=form)@app.route(&#x27;/hello&#x27;, method=[&#x27;POST&#x27;])def hello(): form = HelloForm(request.form) if request.method == &#x27;POST&#x27; and form.validate(): name = request.form[&#x27;sayhello&#x27;] return render_template(&#x27;hello.html&#x27;, name=name) return render_template(&#x27;first_app.html&#x27;, form=form)if __name__ == &#x27;__main__&#x27;: app.run(debug=True) 现在通过Jinjia2模板引擎，在_formhelper.html文件中实现一个通用宏，后续它会被导入到first_app.html文件中用来渲染文本： 123456789101112&#123;% macro render_field(field) %&#125; &lt;dt&gt;&#123;&#123; field.label &#125;&#125; &lt;dd&gt;&#123;&#123; field(**kwargs)|safe &#125;&#125; &#123;% if field.errors %&#125; &lt;ul class=errors&gt; &#123;% for error in field.errors %&#125; &lt;li&gt;&#123;&#123; error &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt; &#123;% endif %&#125; &lt;/dd&gt;&#123;% endmacro %&#125; 接下来，我们创建一个style.css文件，用于控制样式： 123body &#123; font-size: 2em;&#125; 下面是修改后的first_app.html文件内容： 1234567891011121314151617&lt;!doctype html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;First app&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;&#123; url_for(&#x27;static&#x27;, filename=&#x27;style.css&#x27;) &#125;&#125;&quot;&gt; &lt;/head&gt; &lt;body&gt; &#123;% from &quot;_formhelpers.html&quot; import render_field %&#125; &lt;div&gt;What&#x27;s your name?&lt;/div&gt; &lt;form method=post action=&quot;/hello&quot;&gt; &lt;dl&gt; &#123;&#123; render_field(form.sayhello) &#125;&#125; &lt;/dl&gt; &lt;input type=submit value=&#x27;Say Hello&#x27; name=&#x27;submit_btn&#x27;&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 最后我们创建一个hello.html的文件： 12345678910&lt;!doctype html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;First app&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;&#123; url_for(&#x27;static&#x27;, filename=&#x27;style.css&#x27;) &#125;&#125;&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt;Hello &#123;&#123; name &#125;&#125;&lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 接下来通过如下代码来运行我们的Web应用： 1python3 app.py 将电影分类器嵌入Web应用下面更进一步，将电影分类器嵌入到Web应用中。 首先，看一下此电影评论分类应用的目录结构，如下图： 在本章前面的小节中，我们已经创建了vectorizer.py文件，reviews.sqlite以及pkl_objects对象。 由于app.py文件较长，我们分两步来分析。首先导入所需的Python模块和对象，并且通过反序列化恢复我们的分类模型： 12345678910111213141516171819202122232425262728293031323334from flask import Flask, render_template, requestfrom wtforms import Form, TextAreaField, validatorsimport pickleimport sqlite3import osimport numpy as np# import HashingVectorizer from local dirfrom vectorizer import vectapp = Flask(__name__)######## Preparing the Classifiercur_dir = os.path.dirname(__file__)clf = pickle.load(open(os.path.join(cur_dir, &#x27;pkl_objects/classifier.pkl&#x27;), &#x27;rb&#x27;))db = os.path.join(cur_dir, &#x27;reviews.sqlite&#x27;)def classify(document): label = &#123;0: &#x27;negative&#x27;, 1: &#x27;positive&#x27;&#125; X = vect.transform([document]) y = clf.predict(X)[0] proba = np.max(clf.predict_proba(X)) return label[y], probadef train(document, y): X = vect.transform([document]) clf.partial_fit(X, [y]) def sqlite_entry(path, document, y): conn = sqlite3.connect(path) c = conn.cursor() c.execute(&quot;INSERT INTO review_db (review, sentiment, date)&quot;\\ &quot; VALUES (?, ?, DATETIME(&#x27;now&#x27;))&quot;, (document, y)) conn.commit() conn.close() app.py的第二部分如下： 12345678910111213141516171819202122232425262728293031323334353637app = Flask(__name__)class ReviewForm(Form): moviereview = TextAreaField(&#x27;&#x27;, [validators.DataRequired(), validators.length(min=15)])@app.route(&#x27;/&#x27;)def index(): form = ReviewForm(request.form) return render_template(&#x27;reviewform.html&#x27;, form=form)@app.route(&#x27;/results&#x27;, methods=[&#x27;POST&#x27;])def results(): form = ReviewForm(request.form) if request.method == &#x27;POST&#x27; and form.validate(): review = request.form[&#x27;moviereview&#x27;] y, proba = classify(review) return render_template(&#x27;results.html&#x27;, content=review, prediction=y, probability=round(proba*100, 2)) return render_template(&#x27;reviewform.html&#x27;, form=form)@app.route(&#x27;/thanks&#x27;, methods=[&#x27;POST&#x27;])def feedback(): feedback = request.form[&#x27;feedback_button&#x27;] review = request.form[&#x27;review&#x27;] prediction = request.form[&#x27;prediction&#x27;] inv_label = &#123;&#x27;negative&#x27;: 0, &#x27;positive&#x27;: 1&#125; y = inv_label[prediction] if feedback == &#x27;Incorrect&#x27;: y = int(not(y)) train(review, y) sqlite_entry(db, review, y) return render_template(&#x27;thanks.html&#x27;)if __name__ == &#x27;__main__&#x27;: app.run(debug=True) 接下来，看一下reviewform.html模板： 123456789101112131415161718&lt;!doctype html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;Movie Classification&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;Please enter your movie review:&lt;/h2&gt; &#123;% from &quot;_formhelpers.html&quot; import render_field %&#125; &lt;form method=post action=&quot;/results&quot;&gt; &lt;dl&gt; &#123;&#123; render_field(form.moviereview, cols=&#x27;30&#x27;, rows=&#x27;10&#x27;) &#125;&#125; &lt;/dl&gt; &lt;div&gt; &lt;input type=submit value=&#x27;Submit review&#x27; name=&#x27;submit_btn&#x27;&gt; &lt;/div&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 下一个模板是result.html，看上去很有趣： 123456789101112131415161718192021222324252627&lt;!doctype html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;Movie Classification&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;&#123; url_for(&#x27;static&#x27;, filename=&#x27;style.css&#x27;) &#125;&#125;&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;h3&gt;Your movie review:&lt;/h3&gt; &lt;div&gt;&#123;&#123; content &#125;&#125;&lt;/div&gt; &lt;h3&gt;Prediction:&lt;/h3&gt; &lt;div&gt;This movie review is &lt;strong&gt;&#123;&#123; prediction &#125;&#125;&lt;/strong&gt; (probability: &#123;&#123; probability &#125;&#125;%).&lt;/div&gt; &lt;div id=&#x27;button&#x27;&gt; &lt;form action=&quot;/thanks&quot; method=&quot;post&quot;&gt; &lt;input type=submit value=&#x27;Correct&#x27; name=&#x27;feedback_button&#x27;&gt; &lt;input type=submit value=&#x27;Incorrect&#x27; name=&#x27;feedback_button&#x27;&gt; &lt;input type=hidden value=&#x27;&#123;&#123; prediction &#125;&#125;&#x27; name=&#x27;prediction&#x27;&gt; &lt;input type=hidden value=&#x27;&#123;&#123; content &#125;&#125;&#x27; name=&#x27;review&#x27;&gt; &lt;/form&gt; &lt;/div&gt; &lt;div id=&#x27;button&#x27;&gt; &lt;form action=&quot;/&quot;&gt; &lt;input type=submit value=&#x27;Submit another review&#x27;&gt; &lt;/form&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 此外，style.css文件如下： 123456body&#123; width:600px;&#125;#button&#123; padding-top: 20px;&#125; 同样，thanks.html的内容如下： 1234567891011121314&lt;!doctype html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;Movie Classification&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h3&gt;Thank you for your feedback!&lt;/h3&gt; &lt;div id=&#x27;button&#x27;&gt; &lt;form action=&quot;/&quot;&gt; &lt;input type=submit value=&#x27;Submit another review&#x27;&gt; &lt;/form&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 同样，最后我们启动Web应用： 1python3 app.py 接下来，我们就可以访问网站了。 在公共服务器上部署Web应用测试完Web应用后，我们可以将其托管到PythonAnywhere服务器上。托管到PythonAnywhere网站后，我们可以通过访问&lt;username&gt;.pythonanywhere.com。 当收到用户的反馈后，模型会自动即时更新，但是如果服务器崩溃或者重启，clfd对象的更新就会重置。使得更新能够持久化保存的一个方法就是：模型一旦被更新就立即序列化新的clf对象。但是随着用户的增多，此方案的效率会逐渐底下。另外一种解决方案就是使用SQLite数据库保存的反馈信息更新预测模型。为了更新clf对象，我们创建一个update.py脚本文件： 12345678910111213141516171819202122232425262728293031323334353637383940import pickleimport sqlite3import numpy as npimport os# import HashingVectorizer from local dirfrom vectorizer import vectdef update_model(db_path, model, batch_size=10000): conn = sqlite3.connect(db_path) c = conn.cursor() c.execute(&#x27;SELECT * from review_db&#x27;) results = c.fetchmany(batch_size) while results: data = np.array(results) X = data[:, 0] y = data[:, 1].astype(int) classes = np.array([0, 1]) X_train = vect.transform(X) clf.partial_fit(X_train, y, classes=classes) results = c.fetchmany(batch_size) conn.close() return Nonecur_dir = os.path.dirname(__file__)clf = pickle.load(open(os.path.join(cur_dir, &#x27;pkl_objects&#x27;, &#x27;classifier.pkl&#x27;), &#x27;rb&#x27;))db = os.path.join(cur_dir, &#x27;reviews.sqlite&#x27;)update_model(db_path=db, model=clf, batch_size=10000)# Uncomment the following lines if you are sure that# you want to update your classifier.pkl file# permanently.# pickle.dump(clf, open(os.path.join(cur_dir,# &#x27;pkl_objects&#x27;, &#x27;classifier.pkl&#x27;), &#x27;wb&#x27;)# , protocol=4) 创建好update.py的脚本中，我们需要在app.py开头增加一行导入update.py脚本中update_model函数的代码： 12# import update function from local_dirfrom update import update_model 然后在应用程序的主脚本中调用update_model函数： 1234...if __name__ == &#x27;__main__&#x27;: update_model(filepath=db, model=clf, batch_size=10000)...","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"使用机器学习进行情感分析","slug":"使用机器学习进行情感分析","date":"2020-02-07T07:01:11.000Z","updated":"2022-05-16T07:41:46.294Z","comments":true,"path":"2020/02/07/使用机器学习进行情感分析/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/07/%E4%BD%BF%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%BF%9B%E8%A1%8C%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/","excerpt":"本章我们将深入研究自然语言处理（natural language processing，NLP）领域的一个分支—-情感分析（sentiment analysis），还将学习如何使用机器学习算法基于文档的情感倾向对文档进行分类。","text":"本章我们将深入研究自然语言处理（natural language processing，NLP）领域的一个分支—-情感分析（sentiment analysis），还将学习如何使用机器学习算法基于文档的情感倾向对文档进行分类。 获取IMDB电影评论数据集情感分析，又是也称作是观点挖掘，是NLP领域一个非常流行的分支，它分析的是文档的情感倾向。本章中，我们将要使用的是互联网电影数据库中的大量电影评论数据。可以访问http://ai.stanford.edu/~amaas/data/sentiment/来下载电影评论。 在下载完成后对文档进行解压，接下来我们着手将从压缩文件中得到的各文本文档组合为一个CSV文件，在下面的代码中，我们把电影的评论读取到pandas的DataFrame对象中。同时使用PyPrid（Python Progress Indicator）包来预测剩余处理时间： 12345678910111213141516import pyprindimport pandas as pdimport ospbar = pyprind.ProgBar(50000)labels = &#123;&#x27;pos&#x27;: 1, &#x27;neg&#x27;: 0&#125;df = pd.DataFrame()for s in (&#x27;test&#x27;, &#x27;train&#x27;): for l in (&#x27;pos&#x27;, &#x27;neg&#x27;): path = &#x27;./aclImdb/%s/%s&#x27; % (s, l) for file in os.listdir(path): with open(os.path.join(path, file), &#x27;r&#x27;) as infile: txt = infile.read() df = df.append([[txt, labels[l]]], ignore_index=True) pbar.update()df.columns = [&#x27;review&#x27;, &#x27;sentiment&#x27;] 由于集成处理过后数据集中的对应类标是经过排序的，我们现在使用np.random子模块下的permutation函数对DataFrame对象进行重排，并且将其存储为CSV文件： 12345import numpy as npnp.random.seed(0)df = df.reindex(np.random.permutation(df.index))df.to_csv(&#x27;./movie_data.csv&#x27;, index=False) 现在读取前三个样本的摘要： 12df = pd.read_csv(&#x27;./movie_data.csv&#x27;)df.head(3) 词袋模型简介本节中，我们介绍词袋模型，它将文本以数值特征向量的形式来表示。词袋模型的理念很简单，可描述如下： 我们在整个文档上为每个词汇创建了唯一的标记，如单词 我们为每个文档构建一个特征向量，其中包含每个单词在此文档中出现的次数 下面讲解创建简单词袋模型的过程。 将单词转换为特征向量我们可以使用scikit-learn中的CountVector类来根据每个文档中的单词数量构建词袋模型： 123456789import numpy as npfrom sklearn.feature_extraction.text import CountVectorizercount = CountVectorizer()docs = np.array([&#x27;The sun is shining&#x27;, &#x27;The weather is sweet&#x27;, &#x27;The sun is shining and the weather is sweet&#x27;])bag = count.fit_transform(docs)print(count.vocabulary_)&gt;&gt; &#123;&#x27;the&#x27;: 5, &#x27;sun&#x27;: 3, &#x27;is&#x27;: 1, &#x27;shining&#x27;: 2, &#x27;weather&#x27;: 6, &#x27;sweet&#x27;: 4, &#x27;and&#x27;: 0&#125; 由上述命令的运行结果可见，词汇以Python字典的格式存储，将单个单词映射为一个整数索引。接下来看一下之前创建的特征向量： 1234print(bag.toarray())&gt;&gt; [[0 1 1 1 0 1 0]&gt;&gt; [0 1 0 0 1 1 1]&gt;&gt; [1 2 1 1 1 2 1]] 出现在特征向量中的值也称作是原始词频：$ tf(t, d):&#x3D;词汇t在文档d中出现的次数 $。 通过词频–逆文档频率计算单词关联度当我们分析文档数据时，经常遇到的问题就是：一个单词出现在两种类型的多个文档中，这种频繁出现的单词通常不包含有用或具备辨识度的信息。本节中，我们将会学习词频–逆文档频率：$$tf-idf(t, d) &#x3D; tf(t, d) \\times idf(t, d)$$其中，逆文档频率计算公式如下：$$idf(t, d) &#x3D; log\\frac{n_d}{1+df(d, t)}$$这里的$ n_d $问文档的总数，$ df(d, f) $为词汇t在文档d中的数量。分母中的1是为了防止分母为0；取对数是为了出现频率较低的词汇不会被赋予过大的权重。 scikit-learn中还实现了TfidfTransformer转换器： 12345678from sklearn.feature_extraction.text import TfidfTransformer​tfidf = TfidfTransformer()np.set_printoptions(precision=2)print(tfidf.fit_transform(count.fit_transform(docs)).toarray())&gt;&gt; [[0. 0.43 0.56 0.56 0. 0.43 0. ]&gt;&gt; [0. 0.43 0. 0. 0.56 0.43 0.56]&gt;&gt; [0.4 0.48 0.31 0.31 0.31 0.48 0.31]] 可以发现，is在第三个文档中具有较高的词频，但是在将特征向量转换为$ tf-idf $后，单词is在第三个文档中只得到了一个相对较小的$ tf-idf $。 scikit-learn中计算$ tf-idf $之前都会对原始词频进行归一化处理。 清洗文本数据在构建词袋模型之前，最重要的一步就是去除所有不需要的字符对文本数据进行清洗。我们先展示一下经过重排后数据集中第一个文档的最后50个字符： 12df.loc[0, &#x27;review&#x27;][-50:]&gt;&gt; &#x27;is seven.&lt;br /&gt;&lt;br /&gt;Title (Brazil): Not Available&#x27; 接下来，我们将会去除标点符号和HTML标签： 12345import redef preprocessor(text): text = re.sub(&#x27;&lt;[^&gt;]*&gt;&#x27;, &#x27;&#x27;, text) text = re.sub(&#x27;[\\W]+&#x27;, &#x27; &#x27;, text.lower()) return text 接下来我们看一下该函数是否能正常工作： 12preprocessor(df.loc[0, &#x27;review&#x27;][-50:])&gt;&gt; &#x27;is seven title brazil not available&#x27; 最后，我们在下一节中将会反复使用在此经过清洗的文本数据，现在通过preprocessor函数清洗所有的电影评论： 1df[&#x27;review&#x27;] = df[&#x27;review&#x27;].apply(preprocessor) 标记文档准备好电影评论数据集后，我们需要将文本语料拆分为单独的元素。标记（tokenize）文档的一个常用方法是通过文档的空白字符将其拆分为单独的单词： 1234def tokenizer(text): return text.split()tokenizer(&#x27;runner likes running and thus he run&#x27;)&gt;&gt; [&#x27;runner&#x27;, &#x27;likes&#x27;, &#x27;running&#x27;, &#x27;and&#x27;, &#x27;thus&#x27;, &#x27;he&#x27;, &#x27;run&#x27;] 在对文本标记的过程中，另外一种有用的技术就是词干提取（word stemming），这是一个提取单词原型的过程，这样，我们就能将一个单词映射到对应的词干上。Python的自然语言工具包（NLPK）实现了Porter Stemming算法： 123456from nltk.stem import PorterStemmerporter = PorterStemmer()def tokenizer_porter(text): return [porter.stem(word) for word in text.split()]tokenizer_porter(&#x27;runner likes running and thus he run&#x27;)&gt;&gt; [&#x27;runner&#x27;, &#x27;like&#x27;, &#x27;run&#x27;, &#x27;and&#x27;, &#x27;thu&#x27;, &#x27;he&#x27;, &#x27;run&#x27;] 可以发现，running被修改为run，但是thus被修改为不存在的单词thu。在实际应用中中，这种结果造成的影响不大。 另外，还有一种有用的技术：停用词移除（stop-word removal）。停用词在英文中太常见了，它们包含很少的有用信息，因此可以将他们删除： 123456import nltkfrom nltk.corpus import stopwordsnltk.download(&#x27;stopwords&#x27;)stop = stopwords.words(&#x27;english&#x27;)[w for w in tokenizer_porter(&#x27;a runner likes running and runs a lot&#x27;)[-10:] if w not in stop]&gt;&gt; [&#x27;runner&#x27;, &#x27;like&#x27;, &#x27;run&#x27;, &#x27;and&#x27;, &#x27;thu&#x27;, &#x27;he&#x27;, &#x27;run&#x27;] 训练用于文档分类的逻辑斯蒂回归模型本节中，我们将会使用逻辑斯蒂回归模型将电影评论分为正面评价和负面评价。首先，我们将文本对象划分为测试数据和训练数据： 1234X_train = df.loc[:25000, &#x27;review&#x27;].valuesy_train = df.loc[:25000, &#x27;sentiment&#x27;].valuesX_test = df.loc[25000:, &#x27;review&#x27;].valuesy_test = df.loc[25000:, &#x27;sentiment&#x27;].values 接着我们使用Grid Search CV对象，并且使用5折分层交叉验证找到最佳参数： 123456789101112131415161718192021from sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import Pipelinefrom sklearn.linear_model import LogisticRegressionfrom sklearn.feature_extraction.text import TfidfVectorizertfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)param_grid = [&#123;&#x27;vect__ngram_range&#x27;: [(1, 1)], &#x27;vect__stop_words&#x27;: [stop, None], &#x27;vect__tokenizer&#x27;: [tokenizer, tokenizer_porter], &#x27;clf__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;], &#x27;clf__C&#x27;: [1.0, 10.0, 100.0]&#125;, &#123;&#x27;vect__ngram_range&#x27;: [(1, 1)], &#x27;vect__stop_words&#x27;: [stop, None], &#x27;vect__tokenizer&#x27;: [tokenizer, tokenizer_porter], &#x27;vect__use_idf&#x27;: [False], &#x27;vect__norm&#x27;: [None], &#x27;clf__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;], &#x27;clf__C&#x27;: [1.0, 10.0, 100.0]&#125;]lr_tfidf = Pipeline([(&#x27;vect&#x27;, tfidf), (&#x27;clf&#x27;, LogisticRegression(random_state=0))])gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring=&#x27;accuracy&#x27;, cv=5, verbose=1, n_jobs=-1)gs_lr_tfidf.fit(X_train, y_train) 在网格搜索结束后，我们可以输出最佳的参数集： 12print(&#x27;Best params: %s&#x27; % gs_lr_tfidf.best_params_)&gt;&gt; Best params: &#123;&#x27;clf__C&#x27;: 10.0, &#x27;clf__penalty&#x27;: &#x27;l2&#x27;, &#x27;vect__ngram_range&#x27;: (1, 1), &#x27;vect__stop_words&#x27;: None, &#x27;vect__tokenizer&#x27;: &lt;function tokenizer at 0x000002517B0C5F78&gt;&#125; 使用网格搜索得到的最佳模型，我们分别输出5折交叉验证的准确率得分，以及在测试数据集上的分类准确率： 12345print(&#x27;CV acc: %s&#x27; % gs_lr_tfidf.best_score_)&gt;&gt; CV acc: 0.8974041038358466clf = gs_lr_tfidf.best_estimator_print(&#x27;Test acc: %s&#x27; % clf.score(X_test, y_test))&gt;&gt; Test acc: 0.89844 结果表明，我们的机器学习模型针对电影评论是正面评论还是负面评论的分类准确率为90%。 使用大数据之在线算法与外存学习在上一节中，使用网格搜索最佳参数的算法计算成本很高。回顾一下第2章中的随机梯度下降（stochastic gradient descent， SGD）概念，此优化算法每次使用一个样本来更新模型的权重信息。在本节中，我们将使用scikit-learn中SGDClassifier的partial_fit函数来读取本地存储设备，并且使用小型子批次（minibatches）文档来训练一个逻辑斯蒂回归模型。 首先，我们定义一个tokenizer函数来清理movie_data.csv文件中未经处理的文本数据： 123456789import numpy as npimport refrom nltk.corpus import stopwordsstop = stopwords.words(&#x27;english&#x27;)def tokenizer(text): text = re.sub(&#x27;&lt;[^&gt;]*&gt;&#x27;, &#x27;&#x27;, text) text = re.sub(&#x27;[\\W]+&#x27;, &#x27; &#x27;, text.lower()) tokenized = [w for w in text.split() if w not in stop] return tokenized 接下来我们定义一个生成器函数：stream_docs，它每次读取且返回一个文档的内容： 123456def stream_docs(path): with open(path, &#x27;r&#x27;) as scv: next(csv) for line in csv: text, label = line[:-3], int(line[-2]) yield text, label 定义一个get_minibatch函数，它以stream_doc函数得到的文档数据流作为输入，并且通过size返回指定数量的文档内容： 12345678910def get_minibatch(doc_stream, size): docs, y = [], [] try: for _ in range(size): text, label = next(doc_stream) docs.append(text) y.append(label) except StopIteration: return None, None return docs, y 不幸的是，由于需要将所有的词汇加载到内存中，我们无法通过CountVectorizer来使用外存学习方法。另外，TfidfVectorizer需要将所有训练数据集中的特征向量加载到内存以计算逆文档频率。不过，scikit-learn提供了另外一个处理文本信息的向量处理器：HashingVectorizer。HashingVectorizer是独立数据的： 12345678from sklearn.feature_extraction.text import HashingVectorizerfrom sklearn.linear_model import SGDClassifiervect = HashingVectorizer(decode_error=&#x27;ignore&#x27;, n_features=2**21, preprocessor=None, tokenizer=tokenizer)clf = SGDClassifier(loss=&#x27;log&#x27;, random_state=1, n_iter=1)doc_stream = stream_docs(path=&#x27;./movie_data.csv&#x27;) 接下来我们可以通过下述代码使用外存学习： 12345678910import pyprindpbar = pyprind.ProgBar(45)classes = np.array([0, 1])for _ in range(45): X_train, y_train = get_minibatch(doc_stream, size=1000) if not X_train: break X_train = vect.transform(X_train) clf.partial_fit(X_train, y_train, classes=classes) pbar.update() 完成增量学习后，我们将使用剩余的5000个文档来评估模型的性能： 1234X_test, y_test = get_minibatch(doc_stream, size=5000)X_test = vect.transform(X_test)print(&#x27;Acc: %.3f&#x27; % clf.score(X_test, y_test))&gt;&gt; Acc: 0.868 可以看到，模型的准确率约为87%，略微低于我们上一节我们使用网格搜索进行超参调优得到的模型。不过外存学习的存储效率高，只用了不到一分钟的实践就完成了。最后，我们可以通过剩下的5000个文档进行升级： 1clf = clf.partial_fit(X_test, y_test)","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"集成学习之组合不同的模型","slug":"集成学习之组合不同的模型","date":"2020-02-06T07:18:53.000Z","updated":"2022-05-16T07:41:46.386Z","comments":true,"path":"2020/02/06/集成学习之组合不同的模型/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/06/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BB%84%E5%90%88%E4%B8%8D%E5%90%8C%E7%9A%84%E6%A8%A1%E5%9E%8B/","excerpt":"本章中，我们将会学习如何构建一组分类器的集合，使得整体分类效果优于其中任意一个单独的分类器。","text":"本章中，我们将会学习如何构建一组分类器的集合，使得整体分类效果优于其中任意一个单独的分类器。 集成学习集成方法（ensemble method）的目标是：将不同的分类器组合成一个元分类器，与包含于其中的单个分类器相比，元分类器具有更好的泛化性能。 本章中介绍的几种流行的集成方法，它们都使用了多数投票（majority voting）。多数投票原则是将大多数分类器预测的结果作为最终的预测指标。基于训练集，我们首先训练m个不同的成员分类器（$ C_1, \\cdots, C_m $），接着我们将新的未知数据$ x $输入，然后对所有分类器$ C_j $的预测类标进行汇总，选择出得票率最高的类标$ \\hat{y} $：$$\\hat{y} &#x3D; mode{C_1(x), \\cdots, C_m(x)}$$ mode函数：众数函数，返回出现次数最多的值。 另外，由统计学知识得到，当成员分类器出错率低于$ 50% $时，集成分类器的出错率要低于单个分类器。 实现一个简单的多数投票分类器集成算法允许我们使用单独的权重对不同分类算法进行组合，可以将加权多数投票记为：$$\\hat{y} &#x3D; argmax_i\\sum_{j&#x3D;1}^{m}w_j\\chi_A(C_j(x)&#x3D;i)$$其中，$ w_j $是$ C_j $分类器的权重。 argmax是一种函数，是对函数求参数(集合)的函数。当我们有另一个函数$ y&#x3D;f(x) $时，若有结果$ x_0&#x3D; argmax(f(x)) $，则表示当函数$ argmax(f(x)) $取$ x&#x3D;x_0 $的时候，得到f(x)取值范围的最大值；若有多个点使得f(x)取得相同的最大值，那么$ argmax(f(x)) $的结果就是一个点集。 为了使用Python代码实现加权多数投票，我们可以使用NumPy中的argmax和bincount函数： 123import numpy as npnp.argmax(np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6]))&gt;&gt; 1 在实际应用中，我们可以将原来的类标转换为预测类表的概率，这样修正公式如下：$$\\hat{y} &#x3D; argmax_i\\sum_{j&#x3D;1}^mw_jp_{ij}$$其中，$ p_{ij} $是第j个分类器预测样本类标为i的概率。 为实现基于类别预测概率的加权多数投票，我们可以使用如下代码： 123456ex = np.array([[0.9, 0.1], [0.8, 0.2], [0.4, 0.6]])p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6])print(p, np.argmax(p))&gt;&gt; [0.58 0.42] 0 综上，我们可以实现MajorityVoteClassifier: 12345678910111213141516171819202122232425from sklearn.base import BaseEstimatorfrom sklearn.base import ClassifierMixinfrom sklearn.preprocessing import LabelEncoderfrom sklearn.externals import sixfrom sklearn.base import clonefrom sklearn.pipeline import _name_estimatorsimport numpy as npimport operatorclass MajorityVoteClassifier(BaseEstimator, ClassifierMixin): def __init__(self, classifiers, vote=&#x27;classlabel&#x27;, weights=None): self.classifiers = classifiers self.named_classifiers = &#123;key: value for key, value in _name_estimators(classifiers)&#125; self.vote = vote self.weights = weights def fit(self, X, y): self.labelenc_ = LabelEncoder() self.labelenc_.fit(y) self.classes_ = self.labelenc_.classes_ self.classifiers_ = [] for clf in self.classifiers: fitted_clf = clone(clf).fit(X, self.labelenc_.transform(y)) self.classifiers_.append(fitted_clf) return self 在这里，我们使用了两个基类BaseEstimator和ClassifierMixIn获取某些基本方法，包括设定分类器参数的set_params和返回参数的get_params方法，以及用于计算预测准确度的score方法。此外，导入six包是为了使得MajorityVoteClassifier与Python 2.7兼容。 接下来，我们加入predict方法： 1234567891011121314151617181920212223242526def predict(self, X): if self.vote == &#x27;probability&#x27;: maj_vote = np.argmax(self.predict_proba(X), axis=1) else: predictions = np.asarray([clf.predict(X) for clf in self.classifiers_]).T maj_vote = np.apply_along_axis(lambda x: np.argmax(np.bincount(x, weights=self.weights)), axis=1, arr=predictions) maj_vote = self.labelenc_.inverse_transform(maj_vote) return maj_votedef predict_proba(self, X): probas = np.asarray([clf.predict_proba(X) for clf in self.classifiers_]) avg_proba = np.average(probas, axis=0, weights=self.weights) return avg_probadef get_params(self, deep=True): if not deep: return super(MajorityVoteClassifier, self).get_params(deep=False) else: out = self.named_classifiers.copy() for name, step in six.iteritems(self.named_classifiers): for key, value in six.iteritems(step.get_params(deep=True)): out[&#x27;%s__%s&#x27; % (name, key)] = value return out 接下来我们可以将上述算法用于实战了。我们导入鸢尾花数据集，并且只是用其中的两个特征：萼片宽度和花瓣长度。同时我们只区分两个类别的样本：Iris-Versicolor和Iris-Virginica，并且绘制ROC AUC曲线： 123456789from sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.preprocessing import LabelEncoderiris = datasets.load_iris()X, y = iris.data[50:, [1, 2]], iris.target[50:]le = LabelEncoder()y = le.fit_transform(y) 下面划分数据集为测试数据集和训练数据集： 1X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1) 接下来将数据集训练三种不同类型的分类器：逻辑斯蒂回归分类器，决策树分类器和k-近邻分类器各一个： 12345678910111213141516171819from sklearn.model_selection import cross_val_scorefrom sklearn.linear_model import LogisticRegressionfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.pipeline import Pipelineimport numpy as pyclf1 = LogisticRegression(penalty=&#x27;l2&#x27;, C=0.01, random_state=0)clf2 = DecisionTreeClassifier(max_depth=1, criterion=&#x27;entropy&#x27;, random_state=0)clf3 = KNeighborsClassifier(n_neighbors=1, p=2, metric=&#x27;minkowski&#x27;)pipe1 = Pipeline([[&#x27;sc&#x27;, StandardScaler()], [&#x27;clf&#x27;, clf1]])pipe3 = Pipeline([[&#x27;sc&#x27;, StandardScaler()], [&#x27;clf&#x27;, clf3]])clf_labels = [&#x27;LR&#x27;, &#x27;DT&#x27;, &#x27;KNN&#x27;]for clf, label in zip([pipe1, clf2, pipe3], clf_labels): scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring=&#x27;roc_auc&#x27;) print(&#x27;ROC AUC: %.3f +/- %.3f [%s]&#x27; % (scores.mean(), scores.std(), label))&gt;&gt; ROC AUC: 0.917 +/- 0.201 [LR]&gt;&gt; ROC AUC: 0.917 +/- 0.154 [DT]&gt;&gt; ROC AUC: 0.933 +/- 0.104 [KNN] 在此，为什么将逻辑斯蒂回归和k-近邻分类器的训练作为流水线的一部分？不同于决策树，逻辑斯蒂回归和k-近邻算法对数据缩放不敏感，需要对其进行数据标准化处理。 接下来我们基于多数投票原则，在其中组合成员分类器： 12345678910mv_clf = MajorityVoteClassifier(classifiers=[pipe1, clf2, pipe3])clf_labels += [&#x27;MV&#x27;]all_clf = (pipe1, clf2, pipe3, mv_clf)for clf, label in zip(all_clf, clf_labels): scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring=&#x27;roc_auc&#x27;) print(&#x27;ROC AUC: %.3f +/- %.3f [%s]&#x27; % (scores.mean(), scores.std(), label))&gt;&gt; ROC AUC: 0.917 +/- 0.201 [LR]&gt;&gt; ROC AUC: 0.917 +/- 0.154 [DT]&gt;&gt; ROC AUC: 0.933 +/- 0.104 [KNN]&gt;&gt; ROC AUC: 0.967 +/- 0.100 [MV] 从上述输出来看，以10折交叉验证作为评估标准，MajorityVotingClassifier的性能与单个成员分类器相比有着质的提高。 评估与调优集成分类器接下来，我们将在测试数据上计算多数投票的ROC曲线，以验证其在未知数据上的泛化性能： 123456789101112131415161718from sklearn.metrics import roc_curvefrom sklearn.metrics import aucfrom matplotlib import pyplot as pltcolors = [&#x27;black&#x27;, &#x27;orange&#x27;, &#x27;blue&#x27;, &#x27;green&#x27;]linestyles = [&#x27;:&#x27;, &#x27;--&#x27;, &#x27;-.&#x27;, &#x27;-&#x27;]for clf, label, clr, ls in zip(all_clf, clf_labels, colors, linestyles): y_pred = clf.fit(X_train, y_train).predict_proba(X_test)[:, 1] fpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_pred) roc_auc = auc(x=fpr, y=tpr) plt.plot(fpr, tpr, color=clr, linestyle=ls, label=&#x27;%s (auc = %.3f)&#x27; % (label, roc_auc))plt.legend()plt.xlim([-0.1, 1.1])plt.ylim([-0.1, 1.1])plt.grid()plt.xlabel(&#x27;False Positive Rate&#x27;)plt.ylabel(&#x27;True Postive Rate&#x27;)plt.show() 得到的图像如下： 由ROC结果我们可以得到，继承分类器在测试集上表现优秀（ROC AUC &#x3D; 0.95），而KNN分类器对于训练数据有些过拟合。 在学习集成分类的成员分类器调优之前，我们调用一下get_param方法： 1mv_clf.get_params() 输出如下： 12345678910111213141516171819&#123;&#x27;pipeline-1&#x27;: Pipeline(memory=None, steps=[(&#x27;sc&#x27;, StandardScaler(copy=True, with_mean=True, with_std=True)), [&#x27;clf&#x27;, LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#x27;warn&#x27;, n_jobs=None, penalty=&#x27;l2&#x27;, random_state=0, solver=&#x27;warn&#x27;, tol=0.0001, verbose=0, warm_start=False)]], verbose=False),... &#x27;pipeline-2__clf__leaf_size&#x27;: 30, &#x27;pipeline-2__clf__metric&#x27;: &#x27;minkowski&#x27;, &#x27;pipeline-2__clf__metric_params&#x27;: None, &#x27;pipeline-2__clf__n_jobs&#x27;: None, &#x27;pipeline-2__clf__n_neighbors&#x27;: 1, &#x27;pipeline-2__clf__p&#x27;: 2, &#x27;pipeline-2__clf__weights&#x27;: &#x27;uniform&#x27;&#125; 接下来我们先通过网格搜索来调整逻辑斯蒂回归分类器的正则化系数倒数C以及决策深度： 12345678from sklearn.model_selection import GridSearchCVparams = &#123;&#x27;decisiontreeclassifier__max_depth&#x27;: [1, 2], &#x27;pipeline-1__clf__C&#x27;: [0.001, 0.1, 100.0]&#125;grid = GridSearchCV(estimator=mv_clf, param_grid=params, cv=10, scoring=&#x27;roc_auc&#x27;)grid.fit(X_train, y_train)res = grid.cv_results_for params, mean_score, std_score in zip(res[&#x27;params&#x27;], res[&#x27;mean_test_score&#x27;], res[&#x27;std_test_score&#x27;]): print(&#x27;%.3f +/- %.3f %r&#x27; % (mean_score, std_score, params)) 得到的结果如下： 1234560.967 +/- 0.100 &#123;&#x27;decisiontreeclassifier__max_depth&#x27;: 1, &#x27;pipeline-1__clf__C&#x27;: 0.001&#125;0.967 +/- 0.100 &#123;&#x27;decisiontreeclassifier__max_depth&#x27;: 1, &#x27;pipeline-1__clf__C&#x27;: 0.1&#125;1.000 +/- 0.000 &#123;&#x27;decisiontreeclassifier__max_depth&#x27;: 1, &#x27;pipeline-1__clf__C&#x27;: 100.0&#125;0.967 +/- 0.100 &#123;&#x27;decisiontreeclassifier__max_depth&#x27;: 2, &#x27;pipeline-1__clf__C&#x27;: 0.001&#125;0.967 +/- 0.100 &#123;&#x27;decisiontreeclassifier__max_depth&#x27;: 2, &#x27;pipeline-1__clf__C&#x27;: 0.1&#125;1.000 +/- 0.000 &#123;&#x27;decisiontreeclassifier__max_depth&#x27;: 2, &#x27;pipeline-1__clf__C&#x27;: 100.0&#125; 最优的参数和准确度如下： 123print(&#x27;Best params: %s\\nAcc: %.3f&#x27; % (grid.best_score_, grid.best_score_))&gt;&gt; Best params: 1.0&gt;&gt; Acc: 1.000 可以发现，当选择正则化强度较小时，我们能得到最佳的交叉验证结果，而决策树的深度似乎没有什么影响。注意在模型评估时，不止一次使用测试集并非一个好的做法。接下来将学习另外一种集成方法：bagging。 在本节中我们实现的多数投票方法有时也成为堆叠（stocking）。 bagging–通过bootstrap样本构建集成分类器bagging是一种与上节实现的MajorityVoteClassifier关系紧密的集成学习技术，但是不同的是这个算法没有使用相同的训练数据集拟合集成分类器中的单个成员分类器。由于原始数据集使用了bootstrap抽样（有放回的随机抽样），这也是bagging被称为bootstrap aggregating的原因。 接下来为了检验bagging的实际效果，我们用葡萄酒数据集构建一个更复杂的分类问题，在此我们只考虑葡萄酒中的类别2和类别3，且只选择Alcohol和Hue这两个特征： 123456789101112import pandas as pddf_wine = pd.read_csv(&#x27;./wine.data&#x27;, header=None)df_wine.columns = [&#x27;Class label&#x27;, &#x27;Alcohol&#x27;, &#x27;Malic acid&#x27;, &#x27;Ash&#x27;, &#x27;Alcalinity&#x27;, &#x27;Magnesium&#x27;, &#x27;Total phenols&#x27;, &#x27;Flavanoids&#x27;, &#x27;Nonflavanoids&#x27;, &#x27;Proanthocyanins&#x27;, &#x27;Color intensity&#x27;, &#x27;Hue&#x27;, &#x27;Diluted&#x27;, &#x27;Proline&#x27;]df_wine = df_wine[df_wine[&#x27;Class label&#x27;] != 1]y = df_wine[&#x27;Class label&#x27;].valuesX = df_wine[[&#x27;Alcohol&#x27;, &#x27;Hue&#x27;]].values 接下来，对类标进行编码，同时将数据集划分为测试集和训练集： 123456from sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import train_test_splitle = LabelEncoder()y = le.fit_transform(y)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1) scikit-learn中已经实现了Bagging Classifier相关算法，我们可以从ensemble子模块中导入使用： 1234567891011from sklearn.ensemble import BaggingClassifiertree = DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=None)bag = BaggingClassifier(base_estimator=tree, n_estimators=500, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, n_jobs=-1, random_state=1) 接下来我们将计算训练数据集和测试数据集上的预测准确率： 123456789from sklearn.metrics import accuracy_scoretree = tree.fit(X_train, y_train)y_train_pred = tree.predict(X_train)y_test_pred = tree.predict(X_test)tree_train = accuracy_score(y_train, y_train_pred)tree_test = accuracy_score(y_test, y_test_pred)print(tree_train, tree_test)&gt;&gt; 1.0 0.8333333333333334 基于上述代码执行的结果可见，未经剪枝的决策树显现出过拟合的现象。接下来看一下bag的拟合效果： 1234567bag = bag.fit(X_train, y_train)y_train_pred = bag.predict(X_train)y_test_pred = bag.predict(X_test)bag_train = accuracy_score(y_train, y_train_pred)bag_test = accuracy_score(y_test, y_test_pred)print(tree_train, tree_test)&gt;&gt; 1.0 0.8958333333333334 从上图可见bagging分类器在测试数据上的泛化性能稍有胜出。接下来看一下它们的决策区域： 12345678910111213141516x_min = X_train[:, 0].min() - 1x_max = X_train[:, 0].max() + 1y_min = X_train[:, 1].min() - 1y_max = X_train[:, 1].max() + 1xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))f, axarr = plt.subplots(nrows=1, ncols=2, sharex=&#x27;col&#x27;, sharey=&#x27;row&#x27;, figsize=(8, 3))for idx, clf, tt in zip([0, 1], [tree, bag], [&#x27;DT&#x27;, &#x27;Bagging&#x27;]): clf.fit(X_train, y_train) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) axarr[idx].contourf(xx, yy, Z, alpha=0.3) axarr[idx].scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c=&#x27;b&#x27;, marker=&#x27;^&#x27;) axarr[idx].scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c=&#x27;r&#x27;, marker=&#x27;o&#x27;) axarr[idx].set_title(tt)axarr[0].set_ylabel(&#x27;Alcohol&#x27;)plt.show() 可以得到图像如下： 从结果可见，和深度为3的决策树相比，bagging集成分类器的决策边界显得更加平滑。 bagging算法是降低模型方差的一种有效方法，但是它在降低模型偏差方面的作用不大。 通过自适应boosting提高弱学习机的性能在本节中，重点讨论boosting算法的一个常用例子：Adaboost（Adaptive Boosting）。 在boosting中，集成分类器包含多个非常简单的成原分类器，这些成原分类器的性能仅仅好于随即猜测，常被称为弱学习机。原始的boosting过程如下： 从训练集D中以无放回抽样方式随机抽取一个训练子集$ d_1 $，用于弱学习机$ C_1 $的训练 从D中无放回抽样抽取一个训练子集$ d_2 $，并且将$ C_1 $中误分类样本的50%加入到训练集中，训练得到弱学习机$ C_2 $ 从训练集中抽取$ C_1 $和$ C_2 $分类结果不一致的样本生成训练样本$ d_3 $，以此训练第三个弱学习机$ C_3 $ 通过多数投票组合三个弱学习机$ C_1,C_2,C_3 $ 和bagging模型相比，boosting可以同时降低偏差和方差。在实践中，boosting算法对训练数据有过拟合的倾向。 Adaboost和原始的boosting算法不同，它使用整个训练集来训练弱学习机，其中训练样本在每次迭代中都会重新被赋予一个权重，在上一弱学习机错误的基础上进行学习从而构建一个更加强大的分类器。 如上图，从图1开始，所有的样本都被赋予相同的权重，基于次训练集，我们得到了一个分类器（决策曲线是图中虚线）；在下一轮中，我们为前面误分类的样本赋予更高的权重，此外我们降低被正确分类的样本的权重，如子图2所示，弱学习机错误划分了圆形类的三个样本，它们将在子图3中被赋予更大的权重…重复以上过程，最终组合三个学习机得到新的决策区域如图4。 下面是AdaBoost算法的基本步骤： 以等值的方式为权重向量$ w $赋值，其中$ \\sum_iw_i&#x3D;1 $ 在m轮boosting操作中，对第j轮做如下操作 训练一个加权的弱学习机：$ C_j &#x3D; train(X, y, w) $ 预测样本类标：$ \\hat{y} &#x3D; &#x3D;predict(C_j, X) $ 计算权重错误率：$ \\epsilon &#x3D; w \\cdot (\\hat{y} &#x3D;&#x3D; y)$ 计算相关系数：$ a_j &#x3D; 0.5log\\frac{1-\\epsilon} {\\epsilon}$ 更新权重：$ w &#x3D; w \\times exp(-a_j\\times \\hat{y} \\times y) $ 归一化权重：$ w:&#x3D;w&#x2F;\\sum_i{w_i} $ 完成最终预测：$ \\hat{y} &#x3D; (\\sum_{j&#x3D;1}^{m}(a_j\\times predict(C_j, X)) &gt; 0) $ 下面通过scikit-learn来训练一个AdaBoost集成分类器，我们仍然使用上一节中的葡萄酒数据集： 12345678910from sklearn.ensemble import AdaBoostClassifiertree = DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=1)ada = AdaBoostClassifier(base_estimator=tree, n_estimators=500, learning_rate=0.1, random_state=0)tree = tree.fit(X_train, y_train)y_train_pred = tree.predict(X_train)y_test_pred = tree.predict(X_test)tree_train = accuracy_score(y_train, y_train_pred)tree_test = accuracy_score(y_test, y_test_pred)print(tree_train, tree_test)&gt;&gt; 0.8450704225352113 0.8541666666666666 和上一节中未剪枝决策树相比，单层决策树对于训练数据过拟合的成都更加严重一点，接下来看一下AdaBoost分类器的性能： 1234567ada = ada.fit(X_train, y_train)y_train_pred = ada.predict(X_train)y_test_pred = ada.predict(X_test)ada_train = accuracy_score(y_train, y_train_pred)ada_test = accuracy_score(y_test, y_test_pred)print(ada_train, ada_test)&gt;&gt; 1.0 0.875 可以发现，A大Boost模型准确预测了所有的训练集类标，与单层决策树相比，它在测试集上的表现良好，不过，在代码中也可以看到，我们在降低模型偏差的同时使得方差额外的增加。 最后看一下决策区域的形状： 12345678910111213141516x_min = X_train[:, 0].min() - 1x_max = X_train[:, 0].max() + 1y_min = X_train[:, 1].min() - 1y_max = X_train[:, 1].max() + 1xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))f, axarr = plt.subplots(nrows=1, ncols=2, sharex=&#x27;col&#x27;, sharey=&#x27;row&#x27;, figsize=(8, 3))for idx, clf, tt in zip([0, 1], [tree, ada], [&#x27;DT&#x27;, &#x27;AdaBoost&#x27;]): clf.fit(X_train, y_train) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) axarr[idx].contourf(xx, yy, Z, alpha=0.5) axarr[idx].scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c=&#x27;b&#x27;, marker=&#x27;^&#x27;) axarr[idx].scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c=&#x27;r&#x27;, marker=&#x27;o&#x27;) axarr[idx].set_title(tt)axarr[0].set_ylabel(&#x27;Alcohol&#x27;)plt.show() 得到的图像如下： 从上图可知，AdaBoost的决策区域比单层决策树的决策区域复杂得多。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"模型评估与参数调优实战","slug":"模型评估与参数调优实战","date":"2020-02-05T06:28:00.000Z","updated":"2022-05-16T07:41:46.313Z","comments":true,"path":"2020/02/05/模型评估与参数调优实战/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/05/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/","excerpt":"本章中，我们将使用代码进行实践，通过对算法进行调优来构建性能良好的机器学习模型，并对模型的性能进行评估。","text":"本章中，我们将使用代码进行实践，通过对算法进行调优来构建性能良好的机器学习模型，并对模型的性能进行评估。 基于流水线的工作流本节学习scikit-learn中的Pipeline类，它使得我们可以拟合出包含任意多个处理步骤的模型，并将模型用于新数据的预测。 加载威斯康辛乳腺癌数据集首先获取乳腺癌数据集： 12import pandas as pddf = pd.read_csv(&#x27;http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data&#x27;, header=None) 该数据集划分为32列，前两列是样本唯一ID和对样本的诊断结果（M代表恶性，B代表良性），后面的几列是包含了30个从细胞核照片中提取的特征。接下来，将数据集的30个特征赋值给数组对象X，同时转换诊断结果为数字： 1234567from sklearn.preprocessing import LabelEncoderX = df.loc[:, 2:].valuesy = df.loc[:, 1].valuesle = LabelEncoder()y = le.fit_transform(y)le.transform([&#x27;M&#x27;, &#x27;B&#x27;])&gt;&gt; array([1, 0], dtype=int64) 此时良性肿瘤和恶性肿瘤分别被标记为类0和类1。接下来将数据集划分为训练数据集和测试数据集： 12from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) 在流水线中集成数据转换及评估操作接下来可以直接使用Pipeline将上述步骤串联起来： 12345678910from sklearn.preprocessing import StandardScalerfrom sklearn.decomposition import PCAfrom sklearn.linear_model import LogisticRegressionfrom sklearn.pipeline import Pipelinepipe_lr = Pipeline([(&#x27;scl&#x27;, StandardScaler()), (&#x27;pca&#x27;, PCA(n_components=2)), (&#x27;clf&#x27;, LogisticRegression(random_state=1))])pipe_lr.fit(X_train, y_train)print(pipe_lr.score(X_test, y_test))&gt;&gt; 0.9473684210526315 Pipeline对象使用元组的序列作为输入，其中每个元组的第一个值为字符串，它可以是任意的标识符，我们通过它来访问流水线中的元素，而元组的第二个值为scikit-learn中的以恶转换器或者是评估器。 使用k折交叉验证评估模型性能本节中，我们学习两种有用的交叉验证技术：holdout交叉验证和k折交叉验证。借助于这两种方法，我们可以得到模型泛化误差的可靠估计，即模型在新数据上的性能表现。 holdout方法使用holdout进行模型选择更好的方法是将数据分为三个部分：训练数据集，验证数据集和测试数据集。训练数据集用于不同模型的拟合，模型在验证数据集上的性能表现作为模型选择的标准。使用模型训练和模型选择阶段不曾使用的数据作为测试数据集的优势在于：评估模型应用于新数据上能够获得较小偏差。 holdout方法的一个缺点是：模型性能的评估对训练数据集划分为训练及验证子集的方法是敏感的，评价的结果会随着样本的不同而发生变化。下一节介绍鲁棒性更好的性能评价技术：k折交叉验证，我们将在k个训练数据子集上重复holdout方法k次。 k折交叉验证在k折交叉验证中，我们不重复地随机将训练数据集划分为k个，其中$ k-1 $个用于模型的训练，剩余的1个用于测试。重复此过程k次，我们就得到了k个模型及对模型性能的评价。 由于k折交叉验证使用了无重复抽样技术，该方法的优势在于每个样本点只有一次被划入训练数据集或者测试数据集的机会，与holdout方法相比，这将会使得模型性能的评估具有较小的方差。 留一（LOO）交叉验证：在LOO中，我们将数据子集划分的数量等同于样本数（k &#x3D; n），这样每次只有一个样本用于测试。 分层k折交叉验证相对于k折交叉验证做了稍许改进，它可以得到更低的偏差或方差。接下来通过scikit-learn中的StratifiedKFold迭代器来演示： 1234567891011import numpy as npfrom sklearn.model_selection import StratifiedKFoldkfold = StratifiedKFold(n_splits=10, random_state=1)scores = []k = 0for train, test in kfold.split(X_train, y_train): pipe_lr.fit(X_train[train], y_train[train]) score = pipe_lr.score(X_train[test], y_train[test]) scores.append(score) k += 1 print(&#x27;Fold: %s, Class dist.: %s, Acc: %.3f&#x27; % (k, np.bincount(y_train[train]), score)) 得到的结果如下： 12345678910Fold: 1, Class dist.: [256 153], Acc: 0.891Fold: 2, Class dist.: [256 153], Acc: 0.978Fold: 3, Class dist.: [256 153], Acc: 0.978Fold: 4, Class dist.: [256 153], Acc: 0.913Fold: 5, Class dist.: [256 153], Acc: 0.935Fold: 6, Class dist.: [257 153], Acc: 0.978Fold: 7, Class dist.: [257 153], Acc: 0.933Fold: 8, Class dist.: [257 153], Acc: 0.956Fold: 9, Class dist.: [257 153], Acc: 0.978Fold: 10, Class dist.: [257 153], Acc: 0.956 尽管之前的代码清楚介绍了k折交叉验证的工作方式，scikit-learn同样实现了k折交叉验证评分的计算，这时的我们可以更加高效地使用分层k折交叉验证对模型进行评估： 123456from sklearn.model_selection import cross_val_scorescores = cross_val_score(estimator=pipe_lr, X=X_train, y=y_train, cv=10, n_jobs=1)print(scores)&gt;&gt; [0.89130435 0.97826087 0.97826087 0.91304348 0.93478261 0.97777778 0.93333333 0.95555556 0.97777778 0.95555556]print(&#x27;CV Acc: %.3f +/- %.3f&#x27; % (np.mean(scores), np.std(scores)))&gt;&gt; CV Acc: 0.950 +/- 0.029 通过学习及验证曲线来调试算法在本节中，我们将会学习两个有助于提高学习算法性能的简单但功能强大的判定工具：学习曲线（learning curve）与验证曲线（validation curve）。 使用学习曲线判定偏差和方差问题通过将模型的训练及准确性验证看作是训练数据集大小的函数，并且绘制其图像，可以很容易地看出来模型面临高方差还是高偏差。 高偏差模型的训练准确率和交叉验证准确率都很低，这表明此模型未能很好地拟合数据。而高方差模型训练准确率和交叉验证准确率之间相差很大。 接下来，使用scikit-learn中的学习曲线函数评估模型： 123456789101112131415161718192021222324import matplotlib.pyplot as pltfrom sklearn.model_selection import learning_curvepipe_lr = Pipeline([(&#x27;scl&#x27;, StandardScaler()), (&#x27;clf&#x27;, LogisticRegression(penalty=&#x27;l2&#x27;, random_state=0))])train_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=10, n_jobs=1)train_mean = np.mean(train_scores, axis=1)train_std = np.std(train_scores, axis=1)test_mean = np.mean(test_scores, axis=1)test_std = np.std(test_scores, axis=1)plt.plot(train_sizes, train_mean, color=&#x27;b&#x27;, marker=&#x27;o&#x27;, label=&#x27;training acc.&#x27;)plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color=&#x27;b&#x27;)plt.plot(train_sizes, test_mean, color=&#x27;r&#x27;, marker=&#x27;s&#x27;, label=&#x27;validation acc&#x27;)plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color=&#x27;r&#x27;)plt.grid()plt.xlabel(&#x27;Number of training samples&#x27;)plt.ylabel(&#x27;Accuracy&#x27;)plt.legend()plt.ylim([0.8, 1.0])plt.show() 可以得到如下图像： 从图像可知，模型在测试数据集上表现良好。 通过验证曲线来判定过拟合和欠拟合验证曲线和学习曲线类似，不过绘制的不是样本大小和训练准确率，测试准确率之间的关系，而是准确率与模型参数之间的关系，例如逻辑斯蒂回归模型中的正则化参数倒数C。下面使用scikit-learn来绘制验证曲线： 1234567891011121314151617181920212223from sklearn.model_selection import validation_curveparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]train_scores, test_scores = validation_curve(estimator=pipe_lr, X=X_train, y=y_train, param_name=&#x27;clf__C&#x27;, param_range=param_range, cv=10)train_mean = np.mean(train_scores, axis=1)train_std = np.std(train_scores, axis=1)test_mean = np.mean(test_scores, axis=1)test_std = np.std(test_scores, axis=1)plt.plot(param_range, train_mean, color=&#x27;b&#x27;, marker=&#x27;o&#x27;, label=&#x27;training acc.&#x27;)plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.5, color=&#x27;b&#x27;)plt.plot(param_range, test_mean, color=&#x27;r&#x27;, marker=&#x27;s&#x27;, label=&#x27;validation acc.&#x27;)plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha=0.5, color=&#x27;b&#x27;)plt.grid()plt.xscale(&#x27;log&#x27;)plt.legend()plt.xlabel(&#x27;Parameter C&#x27;)plt.ylabel(&#x27;Acc.&#x27;)plt.ylim([0.8, 1.0])plt.show() 得到的图像如下： 在本例中，需要验证的是参数C，即定义在scikit-learn流水线中的逻辑斯蒂回归分类器的正则化参数，我们将其记为clf__C，并且通过param_range参数来设定其值的范围。 从上图可以看到，如果加大正则化强度（较小的C值），会导致模型的欠拟合；如果增加C的值，模型又会趋向于过拟合，在本例中，最优点在$ C&#x3D;0.1 $附近。 使用网格搜索调优机器学习模型机器学习中，有两类参数：通过训练数据学习得到的参数，如逻辑斯蒂回归中的回归系数；以及学习算法中需要单独进行优化的参数。后者是调优参数，也称为超参，对模型来说，就如逻辑斯蒂回归中的正则化系数，或者决策树中的深度参数。 接下来，我们学习一种更加强大的超参数优化技巧：网格搜索（grid search），它通过寻找最优的超参数值的组合以进一步提高模型的性能。 使用网络搜索调优参数网格搜索法很简单，它通过对我们指定的不同超参列表进行暴力穷举法，来计算评估每个组合对模型性能的影响，以获得参数的最优组合： 1234567891011from sklearn.model_selection import GridSearchCVfrom sklearn.svm import SVCpipe_svc = Pipeline([(&#x27;scl&#x27;, StandardScaler()), (&#x27;clf&#x27;, SVC(random_state=1))])param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]param_grid = [&#123;&#x27;clf__C&#x27;: param_range, &#x27;clf__kernel&#x27;: [&#x27;linear&#x27;]&#125;, &#123;&#x27;clf__C&#x27;: param_range, &#x27;clf__gamma&#x27;: param_range, &#x27;clf__kernel&#x27;: [&#x27;rbf&#x27;]&#125;]gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring=&#x27;accuracy&#x27;, cv=10, n_jobs=-1)gs.fit(X_train, y_train)print(gs.best_score_, gs.best_params_)&gt;&gt; 0.978021978021978 &#123;&#x27;clf__C&#x27;: 0.1, &#x27;clf__kernel&#x27;: &#x27;linear&#x27;&#125; 在本例中，线性SVM模型可得到的最优k折交叉验证准确率为$ 97.8% $。 最后，我们使用独立的测试数据集，通过GridSearchCV对象的best_estimator_属性对最优模型进行评估： 1234clf = gs.best_estimator_clf.fit(X_train, y_train)print(clf.score(X_test, y_test))&gt;&gt; 0.9649122807017544 虽然网格搜索时寻找最优参数集合的一种功能强大的方法，但是他的计算成本时很高的，此时可以尝试使用另外一种方法：随即搜索（randomized search）。该方法在scikit-learn中的RandomizedSearchCV类已经实现。 通过嵌套交叉验证选择算法上一节的方法由于在同一个算法中找到最优超参，而本节中介绍的方法用于在不同的机器学习算法中找到最优的机器算法，它就是嵌套交叉验证。 在嵌套交叉验证中，我们将数据划分为训练块和测试块；而在用于模型选择的内部循环中，我们则基于这些训练块使用k折交叉验证。在完成模型的选择后，测试块用于模型性能的评估。 借助于scikit-learn，我们可以通过如下方法使用嵌套交叉验证： 1234gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring=&#x27;accuracy&#x27;, cv=10, n_jobs=-1)scores = cross_val_score(gs, X, y, scoring=&#x27;accuracy&#x27;, cv=5)print(&#x27;CV acc: %.3f +/- %.3f&#x27; % (np.mean(scores), np.std(scores)))&gt;&gt; CV acc: 0.972 +/- 0.012 代码返回的交叉验证准确率平均值对模型超参调优的预期值给出了很好的估计，且使用该值优化过的模型呢能够预测未知数据。例如，我们可以使用嵌套交叉验证方法比较SVM模型与决策树分类器；为了简单起见，我们只调优数的深度参数： 12345678from sklearn.tree import DecisionTreeClassifiergs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0), param_grid=[&#123;&#x27;max_depth&#x27;: [1, 2, 3, 4, 5, 6, 7, None]&#125;], scoring=&#x27;accuracy&#x27;, cv=5)scores = cross_val_score(gs, X_train, y_train, scoring=&#x27;accuracy&#x27;, cv=5)print(&#x27;CV acc: %.3f +/- %.3f&#x27; % (np.mean(scores), np.std(scores)))&gt;&gt; CV acc: 0.908 +/- 0.045 从两个算法的输出看，嵌套交叉验证对SVM的评价高于决策树。由此可见，SVM是用于对测数据集未知数据进行分类的一个更好的选择。 了解不同的性能评价指标前面的几个章节中，我们使用的都是模型准确性来对模型进行评估，接下来学习其他几个性能指标：准确率（precision），召回率（recall），F1分数（F1-score）。 读取混淆矩阵首先，了解一下混淆矩阵： 虽然这些指标可以人工比较的到结果，但是scikit-learn提供了一个confusion_matrix函数： 1234567from sklearn.metrics import confusion_matrixpipe_svc.fit(X_train, y_train)y_pred = pipe_svc.predict(X_test)conmat = confusion_matrix(y_true=y_test, y_pred=y_pred)print(conmat)&gt;&gt; [[71 1]&gt;&gt; [ 2 40]] 在执行上述代码后，我们可以得到混淆矩阵。在本例中，假定类别1是正类，模型正确预测了71个属于类别0的样本（真负），以及40个属于类别1的样本（真正）。 优化分类模型的准确率和召回率预测准确率（ACC）和预测误差率（ERR）都提供了样本分类的相关信息。他们的计算方法如下：$$ERR &#x3D; \\frac{FP + FN}{FP + FN + TP + TN}\\ACC &#x3D; \\frac{TP + TN}{FP + FN + TP + TN}$$对于 类别数量不均衡的分类问题爱来说，真正率（TPR）和假正率（FPR）是非常有用的指标：$$FPR &#x3D; \\frac{FP}{N} &#x3D; {FP}{FP + TN}\\TPR &#x3D; \\frac{TP}{P} &#x3D; {TP}{TP + FN}$$准确率（PRE）和召回率（REC）是和真正率，真负率相关的性能指标，实际上，召回率和真正率含义相同：$$PRE &#x3D; \\frac{TP}{TP+FP}\\REC &#x3D; TPR &#x3D; \\frac{TP}{FN + TP}$$在实践中，常常用准确率和召回率的结合，称为F1分数：$$F1 &#x3D; 2\\frac{PRE \\times REC}{PRE + REC}$$所有的这些评分指标在scikit-learn中已经实现，他们使用方法如下： 12345678from sklearn.metrics import precision_scorefrom sklearn.metrics import recall_score, f1_scoreprint(&#x27;Pre.: %.3f&#x27; % precision_score(y_true=y_test, y_pred=y_pred))print(&#x27;Rec.: %.3f&#x27; % recall_score(y_true=y_test, y_pred=y_pred))print(&#x27;F1.: %.3f&#x27; % f1_score(y_true=y_test, y_pred=y_pred))&gt;&gt; Pre.: 0.976&gt;&gt; Rec.: 0.952&gt;&gt; F1.: 0.964 请记住scikit-learn中将正类类标标识为1。如果我们想要指定一个不同的类标，可以通过make_scorer来构建我们自己的评分，这样我们可以将其应用于GridSearchCV： 123from sklearn.metrics import make_scorer, f1_scorescorer = make_scorer(f1_score, pos_label=0)gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring=scorer, cv=10) ROC曲线受试者工作特征曲线（receiver operator characteristic，ROC）是基于假正率和真正率等性能指标进行分类模型选择的有用工具，假正率和真正率通过移动分类器的阈值实现。基于ROC曲线，我们可以计算所谓的ROC线下区域（AUC），用来刻画分类模型的性能。 在scikit-learn中ROC AUC得分可以通过roc_auc_score函数计算得到： 12345from sklearn.metrics import roc_auc_score, accuracy_scoreprint(&quot;ACC: %.3f&quot; % (accuracy_score(y_true=y_test, y_pred=y_pred)))print(&#x27;ROC AUC: %.3f&#x27; % (roc_auc_score(y_true=y_test, y_score=y_pred)))&gt;&gt; ACC: 0.974&gt;&gt; ROC AUC: 0.969 通过ROC AUC得到的分类器性能可以让我们进一步洞悉分类器在类别不均衡样本集合上的性能。 多类别分类的评价标准本节中讨论的评分标准都是基于二分类系统的。不同，scikit-learn实现了macro（宏）和micro（微）均值方法，计算公式如下：$$PRE_{micro} &#x3D; \\frac{\\sum_i^kTP_i}{\\sum_i^kTP_i+FP_i}\\PRE_{macro} &#x3D; \\frac{\\sum_i^kPRE_i}{k}$$","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"通过降维压缩数据","slug":"通过降维压缩数据","date":"2020-02-04T09:28:09.000Z","updated":"2022-05-16T07:41:46.383Z","comments":true,"path":"2020/02/04/通过降维压缩数据/","link":"","permalink":"http://blog.zsstrike.tech/2020/02/04/%E9%80%9A%E8%BF%87%E9%99%8D%E7%BB%B4%E5%8E%8B%E7%BC%A9%E6%95%B0%E6%8D%AE/","excerpt":"在本章中，我们将会学习到三种特征提取的方法，它们都可以将原始数据集变换到一个维度更低的新的特征子空间。","text":"在本章中，我们将会学习到三种特征提取的方法，它们都可以将原始数据集变换到一个维度更低的新的特征子空间。 无监督数据降维技术之主成分分析主成分分析（PCA）是一种广泛应用于不同领域的无监督线性数据转换技术，突出作用是降维。PCA的目标是在高维数据中找到最大方差的方向，并且将数据映射到一个维度不大于原始数据的新的子空间上。 如果使用PCA技术，我们需要构建一个$ d * k $维的转换矩阵$ W $，从而将原来的d维特征向量转换为k维特征向量（k&lt;d）。PCA算法的步骤如下： 对原始d维数据做标准化处理 构造样本的协方差矩阵 计算协方差矩阵的特征值和相应的特征向量 选择前k个最大特征对应的特征向量（k为新的特征空间维度） 通过前k个特征向量构建映射矩阵$ W $ 将原始的d维特征$ x $通过$ W $转换为新的k维特征$ x’ $ 总体方差和贡献方差这一小节完成PCA的前四个步骤。 首先，使用前面用到的葡萄酒数据集： 12import pandas as pddf_wine = pd.read_csv(&#x27;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&#x27;, header=None) 接着，将数据集划分为训练集和测试集，同时使用StandardScaler来将其标准化： 12345678from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].valuesX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)sc = StandardScaler()X_train_std = sc.fit_transform(X_train)X_test_std = sc.fit_transform(X_test) 接下来构造协方差矩阵，同时求解协方差矩阵的特征值和特征向量： 1234567import numpy as npcov_mat = np.cov(X_train_std.T)eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)print(eigen_vals)&gt;&gt; [4.8923083 2.46635032 1.42809973 1.01233462 0.84906459 0.60181514&gt;&gt; 0.52251546 0.08414846 0.33051429 0.29595018 0.16831254 0.21432212&gt;&gt; 0.2399553 ] 通过使用np.linalg.elg函数，可以得到一个包含有13个特征值的向量（eigen_vals）和一个13 * 13的特征矩阵（eigen_vecs），其中，特征向量以列的方式存在于特征矩阵中。 由于我们需要将数据压缩到一个新的特征子空间上实现降维，我们只需要选择那些包含最多信息的特征向量组成的子集。在此衡量函数是特征值$ \\lambda_j $的方差贡献率：$$\\frac{\\lambda_j}{\\sum_{i&#x3D;1}^{d}j}$$接下来看一下不同特征值对应的方差贡献率： 1234tot = sum(eigen_vals)var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]print(var_exp)&gt;&gt; [0.3732964772349068, 0.18818926106599568, 0.10896790724757796, 0.07724389477124863, 0.0647859460182618, 0.045920138114781475, 0.03986935597634714, 0.025219142607261574, 0.022581806817679666, 0.01830924471952691, 0.016353362655051454, 0.01284270583749274, 0.006420756933868311] 可以知道，第一主成分占方差总和的$ 40% $左右。 特征转换接下来继续执行PCA方法的最后三个步骤。 首先，按照特征值的降序排列特征对： 12eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]eigen_pairs.sort(reverse=True) 接下来，我们只选择两个对应的最大的特征向量： 12345678910111213141516w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis]))print(w)&gt;&gt; [[ 0.14669811 0.50417079]&gt;&gt; [-0.24224554 0.24216889]&gt;&gt; [-0.02993442 0.28698484]&gt;&gt; [-0.25519002 -0.06468718]&gt;&gt; [ 0.12079772 0.22995385]&gt;&gt; [ 0.38934455 0.09363991]&gt;&gt; [ 0.42326486 0.01088622]&gt;&gt; [-0.30634956 0.01870216]&gt;&gt; [ 0.30572219 0.03040352]&gt;&gt; [-0.09869191 0.54527081]&gt;&gt; [ 0.30032535 -0.27924322]&gt;&gt; [ 0.36821154 -0.174365 ]&gt;&gt; [ 0.29259713 0.36315461]] 从而我们现在得到了一个13*2的映射矩阵$ W $。接下来转换原始的数据集： 1X_train_pca = X_train_std.dot(w) 最后，新的数据集被保存在124*2的矩阵中，接下来对其进行可视化： 123456789import matplotlib.pyplot as pltcolors = [&#x27;r&#x27;, &#x27;b&#x27;, &#x27;g&#x27;]markers = [&#x27;s&#x27;, &#x27;x&#x27;, &#x27;o&#x27;]for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(X_train_pca[y_train==l, 0], X_train_pca[y_train==l, 1], c=c, label=l, marker=m)plt.xlabel(&#x27;PC 1&#x27;)plt.ylabel(&#x27;PC 2&#x27;)plt.legend()plt.show() 得到的图像如下： 从上图可以很直观的看到，线性分类器能够对其有很好的划分。 使用scikit-learn进行主成分分析我们先使用PCA对葡萄酒数据做预处理，然后再使用逻辑斯蒂回归模型对转换后的数据进行分类，最后绘制出散点图： 123456789101112131415from sklearn.decomposition import PCApca = PCA(n_components=2)X_train_pca = pca.fit_transform(X_train_std)X_test_pca = pca.transform(X_test_std)import matplotlib.pyplot as pltcolors = [&#x27;r&#x27;, &#x27;b&#x27;, &#x27;g&#x27;]markers = [&#x27;s&#x27;, &#x27;x&#x27;, &#x27;o&#x27;]for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(X_train_pca[y_train==l, 0], X_train_pca[y_train==l, 1], c=c, label=l, marker=m)plt.xlabel(&#x27;PC 1&#x27;)plt.ylabel(&#x27;PC 2&#x27;)plt.legend()plt.show() 得到的图像如下： 比较该图和上一节中的图像，可以发现上图实际上就是我们自己完成的PCA图沿着PC1轴翻转的结果。出现此差异的原因在于特征分析方法：特征向量为正或者为负。 接下来使用逻辑斯蒂回归模型进行训练，并且得到训练结果： 12345678from sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import accuracy_scorelr = LogisticRegression()lr.fit(X_train_pca, y_train)y_pred = lr.predict(X_test_pca)accuracy_score(y_test, y_pred)&gt;&gt; 0.9814814814814815 可以发现的逻辑斯蒂回归模型的拟合率很优良。 通过线性判别分析压缩无监督数据线性判别分析（LDA）是一种可作为特征抽取的技术，它可以提高数据分析过程中的计算效率，同时，对于不适用于正则化的模型，它可以降低因维度灾难带来的过拟合。 LDA方法的步骤如下： 对d为数据集进行标准化处理 对于每一类别，计算d维的均值向量 构造类间的散布矩阵$ S_{B} $以及类内的散布举证$ S_{W} $ 计算矩阵$ s_{W}^{-1}S_{B} $的特征值及对应的特征向量 选取前k个特征值对应的特征向量，构造一个d*k维的转换矩阵$ W $ 使用转换矩阵$ W $将样本映射到新的特征子空间中 计算散布矩阵葡萄酒数据我们已经经过标准化处理，接下来求解均值向量$ m_i $： 1234567891011np.set_printoptions(precision=4)mean_vecs = []for label in range(1, 4): mean_vecs.append(np.mean(X_train_std[y_train==label], axis=0))print(mean_vecs)&gt;&gt; [array([ 0.9259, -0.3091, 0.2592, -0.7989, 0.3039, 0.9608, 1.0515,&gt;&gt; -0.6306, 0.5354, 0.2209, 0.4855, 0.798 , 1.2017]),&gt;&gt; array([-0.8727, -0.3854, -0.4437, 0.2481, -0.2409, -0.1059, 0.0187,&gt;&gt; -0.0164, 0.1095, -0.8796, 0.4392, 0.2776, -0.7016]),&gt;&gt; array([ 0.1637, 0.8929, 0.3249, 0.5658, -0.01 , -0.9499, -1.228 ,&gt;&gt; 0.7436, -0.7652, 0.979 , -1.1698, -1.3007, -0.3912])] 通过均值向量，我们计算一下类内散布矩阵$ S_W $:$$S_W &#x3D; \\sum_{i&#x3D;1}^cS_i$$这可以通过累加各类别i的散步矩阵$ S_i $来计算：$$S_i &#x3D; \\sum_{x \\in D_i}^{c}(x-m_i)(x-m_i)^T$$ 12345678910d = 13S_W = np.zeros((d, d))for label, mv in zip(range(1, 4), mean_vecs): class_scater = np.zeros((d, d)) for row in X[y==label]: row, mv = row.reshape(d, 1), mv.reshape(d, 1) class_scater += (row - mv).dot((row - mv).T) S_W += class_scaterS_W.shape&gt;&gt; (13, 13) 此前，我们假定对散步矩阵计算时，曾假设训练集的类标是均匀分布的，但是，通过以下程序，我们发现其不遵守这个假设： 12np.bincount(y_train)&gt;&gt; array([ 0, 40, 49, 35], dtype=int64) 因此，在我们通过累加方式计算散布矩阵$ S_{W} $前，需要对各类别的散步矩阵$ S_i $做缩放处理。但采用此种方式时，此时散布矩阵和协方差矩阵计算方式相同。协方差矩阵可以看作是归一化的散布矩阵：$$\\frac{1}{N_i}S_{W} &#x3D; \\frac{1}{N_i}\\sum_{x \\in D_i}^c(x-m_i)(x-m_i)^T$$ 1234567d = 13S_W = np.zeros((d, d))for label, mv in zip(range(1, 4), mean_vecs): class_scater = np.cov(X_train_std[y_train==label].T) S_W += class_scaterS_W.shape&gt;&gt; (13, 13) 接下来计算类间散布矩阵$ S_B $:$$S_B &#x3D; \\sum_{i&#x3D;1}^cN_i(m_i-m)(m_i-m)^T$$其中，m是全局均值，他在计算时用到了所有类别中的全部样本： 12345678910mean_overall = np.mean(X_train_std, axis=0)d = 13S_B = np.zeros((d, d))for i, mean_vec in enumerate(mean_vecs): n = X[y==i+1, :].shape[0] mean_vec = mean_vec.reshape(d, 1) mean_overall = mean_overall.reshape(d, 1) S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)S_B.shape&gt;&gt; (13, 13) 在新特征子空间上选取线性判别算法LDA余下的步骤和PCA的步骤相似： 12345eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]eigen_pairs = sorted(eigen_pairs, key=lambda k: k[0], reverse=True)for eigen_pair in eigen_pairs: print(eigen_pair[0]) 得到的结果如下： 12345678910111213643.0153843460517225.086981854162568.002675183788468e-145.757534614184537e-143.5105079604736804e-143.4638958368304884e-142.587811510007498e-142.587811510007498e-142.4449817310582036e-141.6532199129716054e-148.331225171347768e-152.3238388797036527e-156.522430076120113e-16 从上述输出来看，我们只得到了两个非零特征值（实际得到的3-13个特征值并未严格为0，这是由numpy的浮点数运算导致的），说明只有前面两个特征值对应的特征几乎包含了葡萄酒训练数据集中的全部有用信息。 接下来构造转换矩阵： 123456789101112131415w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real, eigen_pairs[1][1][:, np.newaxis].real))w&gt;&gt; array([[-0.0707, 0.3778],&gt;&gt; [ 0.0359, 0.2223],&gt;&gt; [-0.0263, 0.3813],&gt;&gt; [ 0.1875, -0.2955],&gt;&gt; [-0.0033, -0.0143],&gt;&gt; [ 0.2328, -0.0151],&gt;&gt; [-0.7719, -0.2149],&gt;&gt; [-0.0803, -0.0726],&gt;&gt; [ 0.0896, -0.1767],&gt;&gt; [ 0.1815, 0.2909],&gt;&gt; [-0.0631, -0.2376],&gt;&gt; [-0.3794, -0.0867],&gt;&gt; [-0.3355, 0.586 ]]) 将样本映射到新的特征空间通过上一节中构建的转换矩阵$ W $，我们来对原始数据进行转换： 123456789X_train_lda = X_train_std.dot(w)colors = [&#x27;r&#x27;, &#x27;b&#x27;, &#x27;g&#x27;]markers = [&#x27;s&#x27;, &#x27;x&#x27;, &#x27;o&#x27;]for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(X_train_lda[y_train==l, 0], X_train_lda[y_train==l, 1], c=c, label=l, marker=m)plt.xlabel(&#x27;LD 1&#x27;)plt.ylabel(&#x27;LD 2&#x27;)plt.legend()plt.show() 得到的图像如下： 通过图像可知，三个葡萄酒类在新的特征子空间上是线性可分的。 使用scikit-laern进行LDA分析接下来，看一下scikit-laern中对LDA类的实现： 1234567891011from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDAlda = LDA(n_components=2)X_train_lda = lda.fit_transform(X_train_std, y_train)colors = [&#x27;r&#x27;, &#x27;b&#x27;, &#x27;g&#x27;]markers = [&#x27;s&#x27;, &#x27;x&#x27;, &#x27;o&#x27;]for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(X_train_lda[y_train==l, 0], X_train_lda[y_train==l, 1], c=c, label=l, marker=m)plt.xlabel(&#x27;LD 1&#x27;)plt.ylabel(&#x27;LD 2&#x27;)plt.legend()plt.show() 得到的图像如下： 此时看一下逻辑斯蒂回归模型的预测准确度： 123456lr = LogisticRegression()lr = lr.fit(X_train_lda, y_train)X_test_lda = lda.fit_transform(X_test_std, y_test)y_pred = lr.predict(X_test_lda)accuracy_score(y_pred, y_test)&gt;&gt; 1.0 可以看到，逻辑斯蒂回归模型在测试数据集上对样本分类可谓完美。 使用核主成分分析进行非线性映射许多机器学习算法都假定输入数据是线性可分的，但是在现实世界中，大多数的数据是线性不可分的，针对此类问题，使用PCA或者LDA等降维技术，将其转化为线性问题并不是最好的方法。在本节中，我们将了解一下利用核技巧的PCA，或者称其为核PCA，这和第三章中我们介绍的核支持向量机的概念有一定的联系。使用核PCA，我们将学习如何将非线性可分的数据转换到一个适合对其进行线性分类的新的低维子空间中。 核函数通过核PCA，我们能够得到已经映射到各成分的样本，而不像标准PCA那样去构建一个转换矩阵。简单地说，可以将核函数理解为：通过两个向量点积来度量向量间相似度的函数。最常用的核函数有： 多项式核：$$k(x^i, x^j) &#x3D; (x^{iT}x^j + \\theta)^p$$其中，阈值$ \\theta $和幂的值$ p $需要自行定义。 双曲正切（sigmoid）核：$$k(x^i, x^j) &#x3D; thah(\\eta x^{iT}x^j+\\theta)$$ 径向基核函数（RBF）或者称为高斯核函数：$$k(x^i, x^j) &#x3D; exp\\left(-\\frac{||x^i-x^j||^2}{2\\sigma^2}\\right)&#x3D; exp(-\\gamma||x^i - x^j||^2)$$ 基于RBF核的PCA可以通过如下三个步骤实现： 为了计算核矩阵$ k $，我们需要做如下计算：$$k(x^i,x^j) &#x3D; &#x3D; exp(-\\gamma||x^i - x^j||^2)$$我们需要计算任意两个样本对之间的值：$$K &#x3D; \\begin{bmatrix}k(x^1,x^1) &amp; k(x^1, x^2) &amp; \\cdots &amp; k(x^1, x^n)\\k(x^2,x^1) &amp; k(x^2, x^2) &amp; \\cdots &amp; k(x^2, x^n)\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\k(x^n,x^1) &amp; k(x^n, x^2) &amp; \\cdots &amp; k(x^n, x^n)\\\\end{bmatrix}$$ 通过如下公式，使得核矩阵$ k $更为聚集：$$K’ &#x3D; K-l_nK-Kl_n+l_nKl_n$$其中， $ l_n $是一个n*n的矩阵，其所有的值都是$ \\frac{1}{n} $。 将聚集后的核矩阵的特征值按照降序排列，选择前k个特征值对应的特征向量。和标准PCA不同，这里的特征向量不是主成分轴，而是将样本映射到这些轴上。 使用Python实现主成分分析接下来，借助SciPy和NumPy的函数，我们手动实现一个核PCA： 123456789101112131415from scipy.spatial.distance import pdist, squareformfrom scipy import expfrom scipy.linalg import eighimport numpy as npdef rbf_kernel_pca(X, gamma, n_components): sq_dists = pdist(X, &#x27;sqeuclidean&#x27;) mat_sq_dists = squareform(sq_dists) K = exp(-gamma * mat_sq_dists) N = K.shape[0] one_n = np.ones((N, N)) / N K = K - one_n.dot(K) - k.dot(one_n) + one_n.dot(K).dot(one_n) eigvals, eigvecs = eigh(K) X_pc = np.column_stack((eigvecs[:, -i] for i in range(1, n_components + 1))) return X_pc 接下来查看几个实例。 实例一：分离半月形数据 首先创建一个包含100个样本点的二维数据集，以两个半月形状表示： 12345from sklearn.datasets import make_moonsX, y = make_moons(n_samples=100, random_state=123)plt.scatter(X[y==0, 0], X[y==0, 1], color=&#x27;r&#x27;, marker=&#x27;^&#x27;, alpha=0.5)plt.scatter(X[y==1, 0], X[y==1, 1], color=&#x27;b&#x27;, marker=&#x27;o&#x27;, alpha=0.5)plt.show() 得到的图像如下： 显然，这两个半月形不是线性可分的，我们的目标是通过核PCA将这两个半月形数据展开，使得数据集成为适用于某一线性分类器的输入数据。 首先，我们看一下经过标准PCA处理的数据集的图像： 123456from sklearn.decomposition import PCAscikit_pca = PCA(n_components=2)X_spca = scikit_pca.fit_transform(X)plt.scatter(X_spca[y==0, 0], X_spca[y==0, 1], color=&#x27;r&#x27;, marker=&#x27;^&#x27;, alpha=0.5)plt.scatter(X_spca[y==1, 0], X_spca[y==1, 1], color=&#x27;b&#x27;, marker=&#x27;o&#x27;, alpha=0.5)plt.show() 得到的图像如下： 可以发现，经过标准化PCA处理后，线性分类器未必能很好地发挥作用。 接下来尝试一下核PCA函数： 1234X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color=&#x27;r&#x27;, marker=&#x27;^&#x27;, alpha=0.5)plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color=&#x27;b&#x27;, marker=&#x27;o&#x27;, alpha=0.5)plt.show() 得到的图像如下： 可以看到，此时两个类别是线性可分的。 示例二：分离同心圆 接下俩看一下非线性相关的另外一个例子：同心圆： 12345from sklearn.datasets import make_circlesX, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)plt.scatter(X[y==0, 0], X[y==0, 1], color=&#x27;r&#x27;, marker=&#x27;^&#x27;, alpha=0.5)plt.scatter(X[y==1, 0], X[y==1, 1], color=&#x27;b&#x27;, marker=&#x27;o&#x27;, alpha=0.5)plt.show() 得到的图像如下： 接下来使用核PCA，观察数据集分布： 1234X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color=&#x27;r&#x27;, marker=&#x27;^&#x27;, alpha=0.5)plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color=&#x27;b&#x27;, marker=&#x27;o&#x27;, alpha=0.5)plt.show() 得到的图像如下： 可以发现，此时两个类别的数据是线性可分的。 映射新的数据点在标准PCA方法中，我们通过转换矩阵和输入样本之间的点积来对数据进行映射。但是在核PCA中，该如何转换型的数据点呢？实际上，如果我们希望将新的样本$ x’ $映射到此主成分轴，需要进行如下计算：$$\\phi(x’)^Tv$$幸运的是，我们可以使用核技巧，这样就无需精确计算映射$ \\phi(x’)^Tv $。通过以下公式计算：$$\\phi(x’)^Tv &#x3D; \\sum_ia^ik(x’, x^i)^T$$其中，核矩阵K的特征向量$ a $和特征值$ \\lambda $关系如下：$$Ka &#x3D; \\lambda a$$通过如下程序实现映射： 1234def project_x(x_new, X, gamma, alphas, lambdas): pair_dist = np.array([np.sum(x_new - row)**2 for row in X]) k = np.exp(-gamma * pair_dist) return k.dot(alphas / lambdas) 其中，alphas是前k个特征向量，lambdas是前k个对应的特征值： 12alphas = np.column_stack((eigvecs[:, i] for i in range(1, n_components+1)))lambdas = [eigvals[-i] for i in range(1, n_components+1)] 将上述两条语句加到rbf_kernel_pca函数末端并且返回他们的值即可。 scikit-learn中的核主成分分析使用scikit-learn中的API实现核PCA如下： 1234567from sklearn.decomposition import KernelPCAX, y = make_moons(n_samples=100, random_state=123)scikit_kpca = KernelPCA(n_components=2, kernel=&#x27;rbf&#x27;, gamma=15)X_skpca = scikit_kpca.fit_transform(X)plt.scatter(X_skpca[y==0, 0], X_skpca[y==0, 1], color=&#x27;r&#x27;, marker=&#x27;^&#x27;, alpha=0.5)plt.scatter(X_skpca[y==1, 0], X_skpca[y==1, 1], color=&#x27;b&#x27;, marker=&#x27;o&#x27;, alpha=0.5)plt.show() 得到的图像如下： 从上图来看，scikit-learn中KernelPCA得到的结果核我们手动实现的结果相一致。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"数据预处理","slug":"数据预处理","date":"2020-01-31T07:54:15.000Z","updated":"2022-05-16T07:41:46.307Z","comments":true,"path":"2020/01/31/数据预处理/","link":"","permalink":"http://blog.zsstrike.tech/2020/01/31/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/","excerpt":"在本节中，我们将会学习主要的数据预处理技术，使用这些技术可以高效地构建好的机器学习模型。","text":"在本节中，我们将会学习主要的数据预处理技术，使用这些技术可以高效地构建好的机器学习模型。 缺失数据的处理在采集数据的时候，可能有的数据会有缺失的情况。通常我们见到的缺失值是数据表中的空值，或者是类似于NaN的占位符。 首先构造一个含有缺失值的CSV文件： 1234567891011121314import pandas as pdfrom io import StringIOcsv_data = &quot;&quot;&quot;A, B, C, D1.0, 2.0, 3.0, 4.05.0, 6.0, , 8.010.0, 11.0, 12.0, &quot;&quot;&quot;df = pd.read_csv(StringIO(csv_data))print(df) &gt;&gt; A B C D&gt;&gt; 0 1.0 2.0 3.0 4.0&gt;&gt; 1 5.0 6.0 NaN 8.0&gt;&gt; 2 10.0 11.0 12.0 NaN 上述代码中，我们通过read_csv将CSV格式的数据读取到pandas库的DataFrame中，可以看到有两个缺失值。 对于大的DataFrame来说，我们可以使用内置的isnull方法来判断某单元中是否含有缺失值： 123456df.isnull().sum()&gt;&gt; A 0&gt;&gt; B 0&gt;&gt; C 1&gt;&gt; D 1&gt;&gt; dtype: int64 通过这个方式我们可以得到每列中缺失值的数量。 将存在缺失值的特征或样本删除这是最简单的数据处理方式：将含有缺失值的特征（列）或者样本（行）从数据中删除。 可通过 dropna方法来删除包含缺失值的行： 123df.dropna()&gt;&gt; A B C D&gt;&gt; 0 1.0 2.0 3.0 4.0 类似地，我们可以通过将axis参数设置为1，以删除包含缺失值的列： 12345df.dropna(axis=1)&gt;&gt; A B &gt;&gt; 0 1.0 2.0 &gt;&gt; 1 5.0 6.0 &gt;&gt; 2 10.0 11.0 同样地， dropna方法还有其他的参数，以应对各种缺失值的情况： 12345678# only drop rows where all columns are NaNdf.dropna(how=&#x27;any&#x27;)# drop rows that hava not at least 4 non-NaN valuedf.dropna(thresh=4)# only drop rows where NaN in specific columns(here is &#x27;C&#x27;)df.dropna(subset=[&#x27;C&#x27;]) 删除数据是一种简单的方法，但是如果删除过多的样本，会导致分析结果可靠性不高。接下来学习另外一种最常用的处理缺失数据的方法：插值技术。 缺失数据填充所谓插值技术是指通过数据集中的其他训练样本的数据来估计缺失值，最常用的插值技术是均值插值（meaneinputation），即使用相应的特征均值来替换缺失值。我们可以使用scikit-learn中的Impute类来实现此方法： 12345678910from sklearn.preprocessing import Imputerimr = Imputer(missing_values=&#x27;NaN&#x27;, strategy=&#x27;mean&#x27;, axis=0)imr = imr.fit(df)imputed_data = imr.transform(df.values)imputed_data&gt;&gt; array([[ 1. , 2. , 3. , 4. ], [ 5. , 6. , 7.5, 8. ], [10. , 11. , 12. , 6. ]]) 首先计算各个特征列的均值，然后将均值插入到NaN处。参数axis用来控制按列计算均值还是按行计算均值，参数strategy还有median和most_frequent可选值。 理解scikit-learn预估器的API上一节中，我们使用的Imputer类来填充我们数据集中的缺失值，这个类属于scikit-learn中的转换器类，主要用于数据的转换。这些类中常用的两个方法是fit和transform。其中，fit方法用于对数据集中的参数进行识别并且构建相应的数据补齐模型，而transform方法则使用刚创建的数据补齐模型对数据集中的缺失值进行补齐。 在前面的章节中，我们用到了分类器，它们在scikit-learn中属于预估器类别，其API的设计与转换器非常相似。预估器包含一个predict方法，同时也包含一个transform方法。 处理类别数据目前我们只学习了处理数值型数据的方法，但是在真实的数据集中，常常会出现类别数据。类别数据可以进一步划分为标称特征和有序特征。有序特征可以理解为类别的值是可以排序的，如T恤的尺寸；相反，标称数据不具备排序的特征，如T恤的颜色。 首先构造一个数据集： 12345678910111213import pandas as pddf = pd.DataFrame([ [&#x27;green&#x27;, &#x27;M&#x27;, 10.1, &#x27;class1&#x27;], [&#x27;red&#x27;, &#x27;L&#x27;, 13.5, &#x27;class2&#x27;], [&#x27;blue&#x27;, &#x27;XL&#x27;, 15.3, &#x27;class1&#x27;]])df.columns = [&#x27;color&#x27;, &#x27;size&#x27;, &#x27;price&#x27;, &#x27;classlabel&#x27;]df&gt;&gt; color size price classlabel&gt;&gt; 0 green M 10.1 class1&gt;&gt; 1 red L 13.5 class2&gt;&gt; 2 blue XL 15.3 class1 我们构造的数据包括一个标称特征（颜色），一个有序特征（大小）以及一个数据特征（价格）。类标存储在最后一类。 有序特征的映射对于有序特征，scikit-learn中没有实现相应的自动转换方法，因此，我们需要手动构造相应的映射。假设尺寸之间的关系是：XL &#x3D; L + 1 &#x3D; M + 2. 1234567size_mapping = &#123;&#x27;XL&#x27;: 3, &#x27;L&#x27;: 2, &#x27;M&#x27;: 1&#125;df[&#x27;size&#x27;] = df[&#x27;size&#x27;].map(size_mapping)df&gt;&gt; color size price classlabel&gt;&gt; 0 green 1 10.1 class1&gt;&gt; 1 red 2 13.5 class2&gt;&gt; 2 blue 3 15.3 class1 如果在后续的过程中需要将整数值还原为有序字符串，可以简单定义一个逆映射字典inv_size_mapping = &#123;v : k for k, v in size_mapping.items()&#125;，然后再使用pandas提供的map方法即可。 类标的编码许多机器学习库中要求类标以整数值的方式进行编码。需要注意的一点是，类标不是有序的，因此，我们只需要简单的以枚举的方式从0开始设定类标： 1234import numpy as npclass_mapping = &#123;label: idx for idx, label in enumerate(np.unique(df[&#x27;classlabel&#x27;]))&#125;class_mapping&gt;&gt; &#123;&#x27;class1&#x27;: 0, &#x27;class2&#x27;: 1&#125; 接下来映射一下就行： 123456df[&#x27;classlabel&#x27;] = df[&#x27;classlabel&#x27;].map(class_mapping)df&gt;&gt; color size price classlabel&gt;&gt; 0 green 1 10.1 0&gt;&gt; 1 red 2 13.5 1&gt;&gt; 2 blue 3 15.3 0 同样可以构造一个逆映射来将类表还原为字符串。 此外，可以使用scikit-learn中的LabelEncoder类可以更加方便完成对类标的编码工作： 123456from sklearn.preprocessing import LabelEncoderclass_le = LabelEncoder()y = class_le.fit_transform(df[&#x27;classlabel&#x27;].values)y&gt;&gt; array([0, 1, 0], dtype=int64) 同样可以使用inverse_transform方法将类标转换为原始的字符串。 标称特征的独热编码常见的思路如下，使用LabelEncoder类将字符串转换为整数： 1234567X = df[[&#x27;color&#x27;, &#x27;size&#x27;, &#x27;price&#x27;]].valuescolor_le = LabelEncoder()X[:, 0] = color_le.fit_transform(X[:, 0])X&gt;&gt; array([[1, 1, 10.1], [2, 2, 13.5], [0, 3, 15.3]], dtype=object) 这样的数据处理是常见的错误处理方式，因为学习算法将会假定green大于blue，red大于green，这显然是不合理的。 标称特征不能像有序特征一样简单赋予一个整数值，最常用的转换方法是独热编码技术。这个方法的思想就是创建一个新的虚拟特征，虚拟特征的每一列各代表标称数据的一个值。在此，我们将color特征转换为三个新的特征：blue，green和red。此时可以通过二进制值来标识样本的颜色。 1234567from sklearn.preprocessing import OneHotEncoderohe = OneHotEncoder(categorical_features=[0])ohe.fit_transform(X).toarray()&gt;&gt; array([[ 0. , 1. , 0. , 1. , 10.1], [ 0. , 0. , 1. , 2. , 13.5], [ 1. , 0. , 0. , 3. , 15.3]]) 另外，我们可以通过pandas中的get_dummies方法，更加方便地实现虚拟特征。 12345pd.get_dummies(df[[&#x27;price&#x27;, &#x27;color&#x27;, &#x27;size&#x27;]])&gt;&gt; price size color_blue color_green color_red&gt;&gt;0 10.1 1 0 1 0 &gt;&gt;1 13.5 2 0 0 1&gt;&gt;2 15.3 3 1 0 0 将数据集划分为训练数据集和测试数据集接下来，我们将会使用葡萄酒数据集，可以通过UCI机器学习样本数据库来获得。通过pandas库，我们可以在线获取数据集： 1234df_wine = pd.read_csv(&#x27;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&#x27;, header=None)df_wine.columns = [&#x27;Class label&#x27;, &#x27;Alcohol&#x27;, &#x27;Malic acid&#x27;, &#x27;Ash&#x27;, &#x27;Alcalinity of ash&#x27;, &#x27;Magnesium&#x27;, &#x27;Total phenols&#x27;, &#x27;Flavanoids&#x27;, &#x27;Nonflavanoid phenols&#x27;, &#x27;Proanthocyanins&#x27;, &#x27;Color intensity&#x27;, &#x27;Hue&#x27;, &#x27;diluted wines&#x27;, &#x27;Proline&#x27;]df_wine.head() 得到数据集如下： 葡萄酒样本库通过13个不同的特征，对178个葡萄酒样本划分为类标为1，2，3的三个不同的类别，想要将这些样本划分为训练数据集和测试数据集，可以使用scikit-learn中的train_test_split函数： 1234from sklearn.model_selection import train_test_splitX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].valuesX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) 这样，我们就得到了$ 30% $的测试样本和$ 70% $的训练样本。 将特征的值缩放到相同的区间 特征缩放(peature scaling)是数据预处理中至关重要的一步，除了决策树和随机森林不需要特征缩放，其他的机器学习算法几乎都需要这个处理使得算法准确度提高。 特征缩放有两个常用的方法：归一化和标准化。归一化指的是将特征的值缩放到区间$ [0,1] $上，可以使用min-max缩放来实现：$$x_{norm}^i &#x3D; \\frac{x^i - x_{min}}{x^i - x_{max}}$$在scikit-learn中，已经实现了min-max缩放： 12345from sklearn.preprocessing import MinMaxScalermms = MinMaxScaler()X_train_norm = mms.fit_transform(X_train)X_test_norm = mms.fit_transform(X_test) 而标准化的过程可以使用如下方程：$$x_{std}^i &#x3D; \\frac{x^i - \\mu_x}{\\sigma_x}$$其中，$ \\mu_x $和$ \\sigma_x $分别表示某个特征列的均值和样本。同样地，可以使用scikit-learn中的方法实现标准化： 12345from sklearn.preprocessing import StandardScalerstdsc = StandardScaler()X_train_std = stdsc.fit_transform(X_train)X_test_std = stdsc.fit_transform(X_test) 选择有意义的特征如果一个模型在训练数据集上面的表现比在测试数据集上面好很多，那么很可能产生了过拟合。在本节中，我们将会学习使用正则化和特征选择降维这两种常用的减少过拟合问题的方法。 使用L1正则化满足数据稀疏化在第三章节中，权重向量的L2范数如下：$$L2：||w||2^2&#x3D;\\sum{j&#x3D;1}^{m}w_j^2$$而降低模型复杂度的L1正则化公式：$$L1：||w||1 &#x3D; \\sum{j&#x3D;1}^m|w_j|$$对于scikit-learn来说，已经支持了 L1的正则化模型，可以将penalty参数设置为’l1’来进行简单的数据稀处理： 123from sklearn.linear_model import LogisticRegressionLogisticRegression(penalty=&#x27;l1&#x27;) 我们将L1正则化用于标准化处理的葡萄酒数据，经过L1正则化的逻辑斯蒂回归模型可以产生如下稀疏化结果： 123456lr = LogisticRegression(penalty=&#x27;l1&#x27;, C=0.1)lr.fit(X_train_std, y_train)print(&#x27;Training accuracy: &#x27;, lr.score(X_train_std, y_train))print(&#x27;Test accuracy: &#x27;, lr.score(X_test_std, y_test))&gt;&gt; Training accuracy: 0.9838709677419355&gt;&gt; Test accuracy: 0.9814814814814815 训练和测试的精确度显示此模型未出现过拟合，通过如下代码可以获得截距项： 12lr.intercept_&gt;&gt; array([-0.38381104, -0.1580416 , -0.70043119]) 由于我们lr对象默认使用了一对多(One vs Rest, OvR)的方法，因此，第一项截距是类别1相对于类别2和类别3的匹配结果。同样，我们可以查看系数矩阵： 12345678910lr.coef_&gt;&gt; array([[ 0.2801916 , 0. , 0. , -0.02793952, 0. ,&gt;&gt; 0. , 0.71018709, 0. , 0. , 0. ,&gt;&gt; 0. , 0. , 1.2362193 ],&gt;&gt; [-0.64408995, -0.06876656, -0.05722202, 0. , 0. ,&gt;&gt; 0. , 0. , 0. , 0. , -0.92643033,&gt;&gt; 0.06037655, 0. , -0.37111071],&gt;&gt; [ 0. , 0.06151885, 0. , 0. , 0. ,&gt;&gt; 0. , -0.6360538 , 0. , 0. , 0.49810762,&gt;&gt; -0.35817768, -0.57128442, 0. ]]) 可以发现，权重向量是稀疏的，这意味着只有少数几个特征被考虑进来，符合L1的作用效果。 最后，对L1正则化来说，在强的正则化参数(C&lt;0.1)的作用下，罚项使得所有的特征权重趋于0。 在前面已经介绍过，$ \\lambda $是正则化参数，而C是正则化参数的倒数。 序列特征选择算法另外一种降低模型复杂度从而解决过拟合问题的方法是通过特征选择进行降维，该方法对未经正则化处理的模型特别有效。降维技术主要分为两个大类：特征选择和特征提取。通过特征选择，可以选择原始特征的一个子集；而在特征提取中，通过对现有的特征信息进行推演，构造出一个新的特征子空间。 在本节中，我们着眼于一些经 序列特征选择算法是一种贪婪搜索方法，用于将原始的d维特征空间压缩到一个k维空间中，其中$ k &lt; d $。一个经典的序列特征选择算法是序列后向选择算法（SBS），其目的是在分类行性能衰弱最小的约束小，降低原始数据的维度，提高计算效率。 SBS算法的理念很简单：SBS依次从特征集合中删除某些特征，直到新的特征子空间包含指定数量的特征。为了确定每一步需要删除的特征，为此我们需要定义一个最小化的标准衡量函数J。该函数的计算准则是：比较判定分类器在删除某个特征前后的差异，每次删除的特征，就是那些能够使得标准衡量函数值尽可能大的特征，或者说，每一步特征被删除后，所引起的模型性能损失最小。 基于上述对SBS的定义，总结出以下四个步骤： 设$ k &#x3D; d $进行算法初始化，其中 d 是特征空间$ X_d $的维度。 定义$ x^- $为满足标准$ x^- &#x3D; argmaxJ(X_k - x) $最大化的特征，其中$ x \\in X_k $。 将特征$ x^- $从特征集中删除：$ X_{k-1} &#x3D; X_k - x^- , k &#x3D; k -1$。 如果k的值等于目标特征数量，算法终止，否则跳转到第2步。 遗憾的是，scikit-learn并没有实现SBS算法，我们可以手动实现它： 123456789101112131415161718192021222324252627282930313233343536373839404142434445from sklearn.base import clonefrom itertools import combinationsimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scoreclass SBS(): def __init__(self, estimator, k_features, scoring=accuracy_score, test_size=0.25, random_state=1): self.scoring = scoring self.estimator = clone(estimator) self.k_features = k_features self.test_size = test_size slef.random_state = random_state def fit(self, X, y): X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state) dim = X_train.shape[1] self.indices_ = tuple(range(dim)) self.subsets_ = [self.indices_] score = self._calc_score(X_train, y_train, X_test, y_test, self.indices_) self.scores_ = [score] while dim &gt; self.k_features: scores = [] subsets = [] for p in combinations(self.indices_, r=dim-1): score = self._calc_score(X_train, y_train, X_test, y_test, p) scores.append(score) subsets.append(p) best = np.argmax(scores) self.indices_ = subsets[best] self.subsets_.append(self.indices_) dim -= 1 self.scores_.append(scores[best]) self.k_score_ = self.scores_[-1] return self def transform(self, X): return X[:, self.indices_] def _calc_score(self, X_train, y_train, X_test, y_test, indices): self.estimator.fit(X_train[:, indices], y_train) y_pred = self.estimator.predict(X_test[:, indices]) score = self.scoring(y_test, y_pred) return score 我们使用k_features来指定需要返回的特征数量，并且最终特征子集的列标被赋值给self.indices_。注意，在fit方法中，我们没有在fit方法中明确地计算评价标准，只是简单的删除了那些没有包含在最优特征子集中的特征。 接下来我们看一下SBS应用于KNN分类器的效果： 12345678910111213from sklearn.neighbors import KNeighborsClassifierimport matplotlib.pyplot as pltknn = KNeighborsClassifier(n_neighbors=2)sbs = SBS(knn, k_features=1)sbs.fit(X_train_std, y_train)k_feat = [len(k) for k in sbs.subsets_]plt.plot(k_feat, sbs.scores_, marker=&#x27;o&#x27;)plt.ylim([0.7, 1.1])plt.ylabel(&#x27;Accuracy&#x27;)plt.xlabel(&#x27;Number of features&#x27;)plt.grid()plt.show() 得到的图像如下： 可以发现，当k &#x3D; {5, 6, 7, 8, 9, 10}时，算法可以达到百分百的准确率。 接下来看一下是哪五个特征在验证数据集上有如此良好的表现： 123k5 = list(sbs.subsets_[8])print(df_wine.columns[1:][k5])&gt;&gt; Index([&#x27;Alcohol&#x27;, &#x27;Malic acid&#x27;, &#x27;Alcalinity of ash&#x27;, &#x27;Hue&#x27;, &#x27;Proline&#x27;], dtype=&#x27;object&#x27;) 通过随机森林判定特征的重要性接下来使用随机森林来从数据集中选择相关特征，下面的代码根据葡萄酒数据集特征重要程度对这13个特征给出重要性等级。但是注意：无需对基于树的模型做标准化或者归一化处理。代码如下： 12345678from sklearn.ensemble import RandomForestClassifierfeat_labels = df_wine.columns[1:]forest = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)forest.fit(X_train, y_train)importances = forest.feature_importances_indices = np.argsort(importances)[::-1]for f in range(X_train.shape[1]): print(&quot;%2d) %-*s %f&quot; % (f + 1, 30, feat_labels[f], importances[indices[f]])) 得到的输出数据如下： 12345678910111213 1) Alcohol 0.182483 2) Malic acid 0.158610 3) Ash 0.150948 4) Alcalinity of ash 0.131987 5) Magnesium 0.106589 6) Total phenols 0.078243 7) Flavanoids 0.060718 8) Nonflavanoid phenols 0.032033 9) Proanthocyanins 0.02540010) Color intensity 0.02235111) Hue 0.02207812) diluted wines 0.01464513) Proline 0.013916 从上述输出我们可以得到最具有判别效果的特征是‘Alcohol’。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"使用scikit-learn实现分类算法","slug":"使用scikit-learn实现分类算法","date":"2020-01-30T05:17:10.000Z","updated":"2022-05-16T07:41:46.271Z","comments":true,"path":"2020/01/30/使用scikit-learn实现分类算法/","link":"","permalink":"http://blog.zsstrike.tech/2020/01/30/%E4%BD%BF%E7%94%A8scikit-learn%E5%AE%9E%E7%8E%B0%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/","excerpt":"在本节中，我们将会介绍常用分类算法的概念，以及如何使用 scikit-learn 机器学习库和选择机器学习算法时需要注意的问题。","text":"在本节中，我们将会介绍常用分类算法的概念，以及如何使用 scikit-learn 机器学习库和选择机器学习算法时需要注意的问题。 分类算法的选择机器学习算法涉及到的五个步骤可以描述如下： 特征的选择 确定性能评价标准 选择分类器及其优化算法 对模型性能的评估 算法的调优 在本节中集中学习不同分类算法的概念，并再次回顾特征选择，预处理及性能评价指标等内容。 初涉 scikit-learn 的使用首先，使用 scikit-learn 来实现一个感知器模型，这个模型和前面讲的感知器模型类似。仍旧使用鸢尾花数据集中的两个特征。 提取150朵鸢尾花的花瓣长度和宽度两个特征的值，并且由此构建矩阵$ X $，同时将对应的类标赋值给$ y $： 123456from sklearn import datasetsimport numpy as npiris = datasets.load_iris()X = iris.data[:, [2, 3]]y = iris.target 为了评估训练得到的模型在位置数据上的表现，我们进一步将数据集划分为训练数据集和测试数据集： 1234from sklearn.model_selection import train_test_split# sklearn.cross_validation 已经废弃，改用sklearn.model_selectionX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) 由此，我们得到45个测试样本和105个训练样本。为了优化性能，还需要对数据进行特征缩放： 123456from sklearn.preprocessing import StandardScalersc = StandardScaler()sc.fit(X_train)X_train_std = sc.transform(X_train)X_test_std = sc.transform(X_test) 通过调用 sc.fit可以计算出X_train的每个特征的样本均值$ \\mu $和标准差$ \\sigma $。通过调用transform方法，可以使用已经计算出来的$ \\mu $和$ \\sigma $来对训练集数据做标准化处理。在特征缩放后，我们就可以训练感知器模型了： 1234from sklearn.linear_model import Perceptronppn = Perceptron(max_iter=40, eta0=0.1, random_state=0)ppn.fit(X_train_std, y_train) 训练好后，就可以进行预测了： 123y_pred = ppn.predict(X_test_std)print(&#x27;Misclassified samples: %d&#x27; % (y_test != y_pred).sum())&gt;&gt; Misclassified samples: 5 最终可以看到有5个预测错误，从而准确率是$ 89% $。同样地，scikit-learn还实现了许多不同 的性能矩阵，可以通过如下代码计算准确率： 1234from sklearn.metrics import accuracy_scoreprint(&#x27;Accuracy: %.2f&#x27; % accuracy_score(y_test, y_pred))&gt;&gt; Accuracy: 0.89 最后，我们可以在第二章中实现的plot_decision_regions函数来绘制刚刚训练过的决策区域，并且观察不同分类的效果，代码如下： 12345678910111213141516171819202122232425262728from matplotlib.colors import ListedColormapimport matplotlib.pyplot as pltdef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # setup marker generator and color map markers = (&#x27;s&#x27;, &#x27;x&#x27;, &#x27;o&#x27;, &#x27;^&#x27;, &#x27;v&#x27;) colors = (&#x27;red&#x27;, &#x27;blue&#x27;, &#x27;lightgreen&#x27;, &#x27;gray&#x27;, &#x27;cyan&#x27;) cmap = ListedColormap(colors[:len(np.unique(y))]) # plot the decision surface x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot all samples X_test, y_test = X[test_idx, :], y[test_idx] for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl) if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], c=&#x27;orange&#x27;, alpha=1, linewidth=1, marker=&#x27;+&#x27;, s=55, label=&#x27;test set&#x27;) 接下来，就可以回值决策区域图了： 1234567X_combined_std = np.vstack((X_train_std, X_test_std))y_combined = np.hstack((y_train, y_test))plot_decision_regions(X=X_combined_std, y=y_combined, classifier=ppn, test_idx=range(105, 150))plt.xlabel(&#x27;petal length&#x27;)plt.ylabel(&#x27;petal width&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show() 绘图如下： 从图中我们发现无法通过一个线性的决策边界完美划分三类样本。对于无法完美线性可分的数据集，感知器算法将会永远无法收敛，这也是实践中一般不使用感知器算法的原因。 逻辑斯蒂回归中的类别概率初识逻辑斯蒂回归模型逻辑斯蒂回归模型和Adaline模型类似，不同的是在Adaline中，我们使用$ \\phi(z)&#x3D;z $作为激励函数，而在逻辑斯蒂回归中使用的是sigmoid函数作为激励模型：$$sigmoid(z) &#x3D; \\phi(z) &#x3D; \\frac{1}{1+e^{-z}}$$它的函数图像如下： 在给定特征$ x $和权重$ w $的情况下，sigmoid函数的输出值给出了特定样本$ x $属于类别1的概率$ \\phi(z) &#x3D; P(y&#x3D;1|x;w) $。预测到的概率可以通过一个量化器进行二元输出：$$\\hat{y}&#x3D;\\begin{cases}1 &amp; \\phi(z) \\ge 0.5\\0 &amp; others\\end{cases}$$对应的，逻辑斯蒂回归模型图如下： 逻辑斯蒂回归模型的代价函数在构建逻辑斯蒂回归模型时，需要先定义一个最大似然函数，公式如下:$$L(w) &#x3D; \\prod_{i&#x3D;1}^{n}P(y^i|x^i;w)&#x3D;(\\phi(z^i))^{y^i}(1-\\phi(z))^{1-y^i}$$然后取对数并且改写一下，得到如下：$$J(w) &#x3D; \\sum_i^n-y^ilog(\\phi(z^i)) - (1-y^i)log(1-\\phi(z^i))$$我们可以对单个样本实例进行成本分析：$$J(\\phi(z),y;w) &#x3D; \\begin{cases}-log(\\phi(z)) &amp; y&#x3D;1\\-log(1-\\phi(z)) &amp; y&#x3D;0\\end{cases}$$ 可以看到，如果正确将样本划分到类别1和0中，代价都将会趋于0，但是如果错误分类，代价将会区域无穷，这也就意味着错误预测带来的代价将会越来越大。 使用scikit-learn训练逻辑斯蒂回归模型接下来，我们使用逻辑斯蒂回归模型来训练鸢尾花数据集： 123456789from sklearn.linear_model import LogisticRegressionlr = LogisticRegression(C=1000.0, random_state=0)lr.fit(X_train_std, y_train)plot_decision_regions(X_combined_std, y_combined, classifier=lr, test_idx=range(105, 150))plt.xlabel(&#x27;length&#x27;)plt.ylabel(&#x27;width&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show() 得到的决策区域图如下： 此外，可以通过predict_proba来预测样本属于某个类别的概率： 12lr.predict_proba(X_test_std[0:1, :])&gt;&gt; array([[1.78177322e-11, 6.12453348e-02, 9.38754665e-01]]) 此结果表示模型预测此样本属于类标1的概率是$ 6.1% $，属于类标2的概率是$ 93.9% $。 通过正则化解决过拟合问题过拟合是机器学习中常见的问题，过拟合具有高方差，这可能是使用了较多的参数，使得模型过于复杂。同样地，模型也会面临着欠拟合问题，欠拟合具有高偏差，这意味着模型过于简单，使得我们在预测时性能不佳。 偏差-方差权衡（bias-variance tradeoff）就是通过正则化来调整模型的复杂度。正则化时解决共线性（特征间高度相关）的一个很有用的方法，最常用的正则化形式是L2正则化，可以写作：$$\\frac{\\lambda}{2}||w||^2&#x3D;\\frac{\\lambda}{2}\\sum_{j&#x3D;1}^m w_j^2$$其中，$ \\lambda $是正则化系数。 特征缩放之所以很重要，其中一个原因是正则化。为了使得正则化起作用，需要确保所有特征的衡量标准保持统一。 使用正则化方法时，我们只需要在逻辑斯蒂回归的代价函数中加入正则化项，以降低系数带来的副作用：$$J(w) &#x3D; \\left(\\sum_i^n-y^ilog(\\phi(z^i)) - (1-y^i)log(1-\\phi(z^i))\\right)+\\frac{\\lambda}{2}||w||^2$$前面用到的scikit-learn中的LogisticRegression类，其中的参数C时正则化系数的倒数：$$C &#x3D; \\frac{1}{\\lambda}$$ 使用支持向量机最大化分类间隔另外一种性能强大且广泛应用的学习算法时支持向量机（SVM），它可以看作是对感知器的扩展。在SVM中，我们的目标是最大化分类间隔。在此处间隔指的是两个分离的决策边界间的距离，而最靠近决策边界的训练样本称作是支持向量： 对分类间隔最大化的直观认识我们将平面分为正平面和负平面，对于正平面来说：$$w_0+w^TX_{pos}&#x3D;1$$对于负平面：$$w_0+w^TX_{neg}&#x3D;-1$$对以上两式，相减得$$w^T(X_{pos}-X_{neg})&#x3D;2$$定义$ ||w|| &#x3D; \\sqrt{\\sum_{j&#x3D;1}^{m}w_j^2} $，于是可得到如下等式：$$\\frac{w^T(X_{pos}-X_{neg})}{||w||}&#x3D;\\frac{2}{||w||}$$上述等式的左侧可以解释为正负平面间的距离，也就是我们要最大化的距离。在样本正确分类的前提下，最大化分类间隔就是$ \\frac{2}{||w||} $最大化，这也是SVM的目标函数，记作：$$w_0+w^Tx^i \\ge 1, if\\ y^i&#x3D;1\\w_0+w^Tx^i \\lt -1, if\\ y^i&#x3D;-1$$这两个方程可以解释为：所有的负样本都落在负超平面一侧，所有的正样本都落在正超平面一侧划分的区域中。实践中，使用二次规划方法很容易求出$ \\frac{||w||}{2} $的最小值。 使用松弛变量解决非线性可分问题引入松弛变量$ \\xi $的目的是：放松线性约束条件，以保证在适当的惩罚项样本下，对错误分类的情况进行优化时能够收敛。 取值为正的松弛变量可以简单的加到线性约束条件中：$$w^Tx^i \\ge 1, if\\ y^i&#x3D;1 - \\xi^i\\w^Tx^i \\lt -1, if\\ y^i&#x3D;-1 + \\xi^i$$由此，新的优化目标为$$\\frac{||w||}{2}+C(\\sum_i\\xi^i)$$通过变量C，我们可以控制对错误分类的惩罚程度，进而在偏差和方差之间取得平衡。 使用scikit-learn实现SVM接下来，我们使用SVM模型来对鸢尾花数据集中的样本进行分类： 123456789from sklearn.svm import SVCsvm = SVC(kernel=&#x27;linear&#x27;, C=1.0, random_state=0)svm.fit(X_train_std, y_train)plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105, 150))plt.xlabel(&#x27;length&#x27;)plt.ylabel(&#x27;width&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show() 得到图像如下： 在实际的分类任务中，线性逻辑斯蒂回归和支持向量机往往得到相似的结果。但是逻辑斯蒂回归比SVM更容易处理离群点，而SVM更关注接近决策边界的点。 在有些数据集很大的时候，可以使用scikit-learn提供的SGDClassifier类供用户选择，这个流泪还提供了partial-fit方法支持在线学习。SGDClassifier类的概念类似于随机梯度算法。 我们可以使用以下方式分别构建基于随机梯度下降的感知器，逻辑斯蒂回归以及支持向量机模型。 12345from sklearn.linear_model import SGDClassifierppn = SGDClassifier(loss=&#x27;perceptron&#x27;)lr = SGDClassifier(loss=&#x27;log&#x27;)svm = SGDClassifier(loss=&#x27;hinge&#x27;) 使用核SVM解决非线性问题SVM受欢迎的一个原因是：通过“核技巧”可以很容易解决非线性可分问题。 首先来了解非线性可分问题到底是什么。通过NumPy的logicol_xor来创建数据集，其中100个样本属于类别1，另外的100个样本属于类别-1： 123456789np.random.seed(0)X_xor = np.random.randn(200, 2)y_xor = np.logical_xor(X_xor[:, 0] &gt; 0, X_xor[:, 1] &gt; 0)y_xor = np.where(y_xor, 1, -1)plt.scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c=&#x27;b&#x27;, marker=&#x27;x&#x27;, label=&#x27;1&#x27;)plt.scatter(X_xor[y_xor==-1, 0], X_xor[y_xor==-1, 1], c=&#x27;r&#x27;, marker=&#x27;s&#x27;, label=&#x27;-1&#x27;)plt.ylim(-3.0)plt.legend()plt.show() 执行以上代码后，我们得到了一个“异或”数据集，二维分布如下： 显然，使用前面提到的线性逻辑斯蒂回归或者是线性SVM模型，都无法将样本正确划分为正类别和负类别。 核方法的基本思想是：通过映射函数$ \\phi(\\cdot) $将样本的原始特征映射到一个使样本线性可分的更高维的空间中，然后找到分界面后作反变换$ \\phi^{-1}(\\cdot) $可得到最初的划分平面。 但是这种映射会带来非常大的计算成本，这个时候我们就可以使用核技巧的方法。在实践中，我们所需要做的就是将点积$ x^{iT}x^j $映射为$ \\phi((x^i)^T \\phi(x^j) $，为此定义$$k(x^i, x^j) &#x3D; \\phi((x^i)^T \\phi(x^j)$$一个最常使用的核函数就是径向基核函数（RBF kernel）：$$k(x^i, x^j) &#x3D; exp(-\\frac{||x^i-x^j||^2}{2\\sigma^2}) &#x3D; exp(-\\gamma ||x^i-x^j||^2)$$其中，$ \\gamma &#x3D; \\frac{1}{2\\sigma^2} $是待优化的自由参数。接下来，就使用scikit-learn来训练一个核SVM使之可以对“异或”数据集进行分类： 12345svm = SVC(kernel=&#x27;rbf&#x27;, random_state=0, gamma=0.10, C=10.0)svm.fit(X_xor, y_xor)plot_decision_regions(X_xor, y_xor, classifier=svm)plt.legend()plt.show() 得到的决策边界如下： 正如图像所示，核SVM较好地完成了对“异或”数据的划分。 在此，我们将$ \\gamma $参数设置为了0.1，可以将其理解为高斯球面的截止参数（cut-off parameter）。为了更好的理解$ \\gamma $参数，我们将基于RBF的SVM应用于鸢尾花数据集： 1234567svm = SVC(kernel=&#x27;rbf&#x27;, random_state=0, gamma=0.20, C=10.0)svm.fit(X_train_std, y_train)plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105, 150))plt.xlabel(&#x27;length&#x27;)plt.ylabel(&#x27;width&#x27;)plt.legend()plt.show() 得到的图像如下： 现在改变$ \\gamma &#x3D; 5.0 $，得到的图像如下： 通过图像可以看出，使用一个较大的$ \\gamma $值，会使得类别0核类别1的决策边界变得紧凑了许多。 虽然模型对训练数据的你和很好，但是类似的分类器对未知数据会有较大的泛化误差。 决策树基于训练集的特征，决策树模型通过一系列的问题来推断样本的类标。 从树根开始，基于可获得最大信息增益（Information Gain, IG）的特征来对数据进行划分，通过迭代处理，在每个节点重复此过程，直到叶子节点。 最大化信息增益—-获知尽可能准确的结果就目前来说，大多数的库中实现的树算法都是二叉决策树。二叉决策树中常用的三个不纯度衡量标准或者划分标准分别是：基尼系数（Gini index, $ I_G $），熵（entropy， $ I_H $）以及误分类率（classification error， $ I_E $）。 非空类别熵的定义是$$I_H(t) &#x3D; -\\sum_{i&#x3D;1}^c p(i|t)\\log_2p(i|t)$$其中，$ p(i|t) $为在特定节点t中，属于类别i的样本占特定样本t中样本总数的比例。如果某一个节点中所有样本都属于同一个类别，那么它的熵是0，当样本以相同的比例分属于不同的类时，熵的值最大。 直观地说，基尼系数可以理解为降低误分类可能性的标准：$$I_G(t) &#x3D; \\sum_{i&#x3D;1}^{c}p(i|t)(1-p(i|t)) &#x3D; 1 - \\sum_{i&#x3D;1}^c p(i|t)^2$$和熵类似，当所有样本时等比例分布时，基尼系数的值最大。 误分类率的定义如下：$$I_E &#x3D; 1 - max{p(i|t)}$$这是对于剪枝方法很有用的准则，但不建议用于决策树的构建过程，因为它对节点中各类别样本数量的变动不敏感。 构建决策树通过使用scikit-learn来构建一颗二叉决策树，需要注意的是，决策树的深度不是越大越好，深度过大的决策树，很容易产生过拟合的现象。在此，构建一棵深度是3的决策树： 1234567891011from sklearn.tree import DecisionTreeClassifiertree = DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=3, random_state=0)tree.fit(X_train, y_train)X_combined = np.vstack((X_train, X_test))y_combined = np.hstack((y_train, y_test))plot_decision_regions(X_combined, y_combined, classifier=tree, test_idx=range(105, 150))plt.xlabel(&#x27;length&#x27;)plt.ylabel(&#x27;width&#x27;)plt.legend()plt.show() 得到的决策边界如下： 通过随机森林将弱分类器集成为强分类器直观上，随机森林可以看作是多颗决策树的集成。随机森林算法可以概括为一下几个步骤： 使用bootstrap抽样方法随机选择 n 个样本用于训练（从训练集中随机可重复选择n个样本） 使用第 1 步选定的样本构建一棵决策树，节点划分如下： 不重复选择d个特征 根据目标函数的要求，使用选定的特征对节点进行划分 重复上述过程1~2000次 汇总每棵树的类标进行多数投票 使用scikit-learn来实现随机森林： 123456789from sklearn.ensemble import RandomForestClassifierforest = RandomForestClassifier(criterion=&#x27;entropy&#x27;, n_estimators=10, random_state=1, n_jobs=2)forest.fit(X_train, y_train)plot_decision_regions(X_combined, y_combined, classifier=forest, test_idx=range(105, 150))plt.xlabel(&#x27;length&#x27;)plt.ylabel(&#x27;width&#x27;)plt.legend()plt.show() 得到的决策区域如下： 上述代码中，我们以熵作为不纯度衡量标准，且使用了10棵决策树进行随机森林的训练，同时我们还规定算法中使用的处理器内核数量为2。 惰性学习算法之k-近临算法k-近临算法（k-nearest neighbor classifier，KNN）是惰性学习算法的典型例子。KNN算法本身是简单的，可以归纳为以下几步： 选择近临数量k和距离衡量方法 找到待分类样本的k个最近邻居 根据最近临的类标进行多数投票 下图说明了当k&#x3D;3时，范围内红色三角形多，这个待分类点属于红色三角形；当K &#x3D; 5 时，范围内蓝色正方形多，这个待分类点属于蓝色正方形。 KNN算法可以快速适应新的训练数据，不过它的缺点也是显而易见的，在最坏情况下，计算复杂度随着样本的增多而线性增长。 接下来使用scikit-learn实现KNN模型，在此，我们选择欧几里得距离作为度量标准： 123456789from sklearn.neighbors import KNeighborsClassifierknn = KNeighborsClassifier(n_neighbors=5, p=2, metric=&#x27;minkowski&#x27;)knn.fit(X_train_std, y_train)plot_decision_regions(X_combined_std, y_combined, classifier=knn, test_idx=range(105, 150))plt.xlabel(&#x27;length&#x27;)plt.ylabel(&#x27;width&#x27;)plt.legend()plt.show() 得到的决策区域如下： 在代码中用到的“闵可夫斯基（minkowski）”距离是对欧几里得距离和曼哈顿距离的一种泛化，可写作：$$d(x^i, x^j) &#x3D; \\sqrt[p]{\\sum_k|x^i_kx^j_k|^p}$$如果将参数p设置为2，那么就是欧几里得距离，设置为1，则为曼哈顿距离。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"机器学习分类算法","slug":"机器学习分类算法","date":"2020-01-26T08:53:55.000Z","updated":"2022-05-16T07:41:46.308Z","comments":true,"path":"2020/01/26/机器学习分类算法/","link":"","permalink":"http://blog.zsstrike.tech/2020/01/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/","excerpt":"介绍最早以算法方式描述的分类机器学习算法：感知器和自适应线性神经元。同时我们也会使用Python来实现一个感知器。","text":"介绍最早以算法方式描述的分类机器学习算法：感知器和自适应线性神经元。同时我们也会使用Python来实现一个感知器。 早期机器学习概述罗森布拉特基于MCP模型提出了第一个感知器学习法则。在这个感知器规则中，他提出了一个自学习算法，此算法通过优化得到权重系数，此系数和输入值的乘机决定了神经元是否被激活。在监督学习中，类似算法可以用于预测样本所属的类别。 我们将类别问题看作是二值分类问题，为了简单起见，分为1(正类别)和**-1（负类别）。同时定义一个激励函数$ \\phi(z) $，这个函数将会以特定的输入值x和相应的权值向量w**的线性组合作为输入，也就是说：$ z&#x3D;w_1x_1 + \\cdots + w_nx_n $。此时定义法治函数为阶跃函数：$$\\begin{equation}\\phi(z)&#x3D;\\begin{cases} 1, &amp; z \\ge \\theta \\ -1, &amp; z \\lt \\theta\\end{cases}\\end{equation}$$其中，我们称$\\theta$是阈值。 为了简单起见，可以将阈值移动到等式的左边，同时初始权重是$w_0&#x3D;-\\theta$同时$x_0&#x3D;1$，这样激励函数就变为$$\\begin{equation}\\phi(z)&#x3D;\\begin{cases} 1, &amp; z \\ge 0 \\ -1, &amp; z \\lt 0\\end{cases}\\end{equation}$$同时感知器最初的规则很简单，可以归纳为如下几步： 将权重初始化为零或者一个极小的随机数。 迭代所有训练样本$x^i$,执行如下操作： 计算输出值$\\hat{y}$。 更新权值。 每次对权重向量$w$的更新方式为：$$w_j:&#x3D;w_j+\\Delta{w_j}$$而对于$\\Delta{w_j}$,可以通过感知器的学习规划计算获得：$$\\Delta{w_j}&#x3D;\\eta(y^i-\\hat{y^i})x_j^i$$其中，$\\eta$是学习速率，一个在0到1之间的常数，$y^i$是第i个样本的真实样标，$\\hat{y^i}$是相应的预测值。对于一个二维数据，可以得到如下更新公式：$$\\Delta{w_0} &#x3D; \\eta(y^i-\\hat{output^i})\\\\Delta{w_1} &#x3D; \\eta(y^i-\\hat{output^i})x_1^i\\\\cdots$$需要注意的是，感知器收敛的前提是两个类别必须是线性可分的，并且学习速率足够小。 感应器的模型如下： 使用Python实现感知器学习算法使用面向对象的方法实现感知器的接口，同时按照Python开发惯例，对于那些并非在初始化对象时创建但是又被对象中其他方法调用的属性，可以在后面加上一个下划线，如self.w_。 Python算法如下： 12345678910111213141516171819202122232425import numpy as npclass Perceptron(object): def __init__(self, eta=0.01, n_iter=10): self.eta = eta self.n_iter = n_iter def fit(self, X, y): self.w_ = np.zeros(1 + X.shape[1]) self.errors_ = [] for _ in range(self.n_iter): errors = 0 for xi, target in zip(X, y): update = self.eta * (target - self.predict(xi)) self.w_[1:] += update * xi self.w_[0] += update errors += int(update != 0.0) self.errors_.append(errors) return self def net_input(self, X): return np.dot(X, self.w[1:]) + self.w_[0] def predict(self, X): return np.where(self.net_input(X) &gt;= 0.0, 1, -1) 接下来就可以使用相关的数据集来训练我们的感知器模型。 首先我们从pandas库直接从UCI机器学习库中将鸢尾花数据集转化为DataFrame对象并且加载到内存中，并且使用tail方法显示数据确保数据加载正确。 1234import pandas as pddf = pd.read_csv(&#x27;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&#x27;, header=None)df.tail() 0 1 2 3 4 145 6.7 3.0 5.2 2.3 Iris-virginica 146 6.3 2.5 5.0 1.9 Iris-virginica 147 6.5 3.0 5.2 2.0 Iris-virginica 148 6.2 3.4 5.4 2.3 Iris-virginica 149 5.9 3.0 5.1 1.8 Iris-virginica 鸢尾花数据集时机器学习中一个经典的实例，它包含了Setosa，Versicolor和Virginica三个品种总共150个鸢尾花的测量数据，每个品种的数量是50个。每个数据项包括序号，萼片长度，萼片宽度，花瓣长度，花瓣宽度，类标。 接下来提取前100个类标，其中分别包含50个山鸢尾类标和50个变色鸢尾类标，并且将变色鸢尾表示为1，山鸢尾表示为-1。同时提取萼片长度和花瓣长度作为输入变量$X$。 首先可视化$X$： 123456789101112import matplotlib.pyplot as pltimport numpy as npy = df.iloc[0:100, 4].valuesy = np.where(y == &#x27;Iris-setosa&#x27;, -1, 1)X = df.iloc[0:100, [0, 2]].valuesplt.scatter(X[:50, 0], X[:50, 1], color=&#x27;red&#x27;, marker=&#x27;o&#x27;, label=&#x27;setosa&#x27;)plt.scatter(X[50:100, 0], X[50:100, 1], color=&#x27;blue&#x27;, marker=&#x27;x&#x27;, label=&#x27;versicolor&#x27;)plt.xlabel(&#x27;petal length&#x27;)plt.ylabel(&#x27;sepal length&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show() 可视化的图形如下： 现在，我们可以使用提取出的数据来训练我们的感知器了。同时我们还会绘制每次迭代的错误分类数量的折线图，以检验算法是否收敛并且找到决策边界。 123456ppn = Perceptron(eta=0.1, n_iter=10)ppn.fit(X, y)plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=&#x27;o&#x27;)plt.xlabel(&#x27;Epochs&#x27;)plt.ylabel(&#x27;Number of misclassifications&#x27;)plt.show() 得到的每次迭代的错误数量折线图如下： 如上图所示，我们的迭代器在第六次的时候就已经收敛，下面通过一个简单的函数实现二维数据决策边界的可视化。 1234567891011121314151617181920212223from matplotlib.colors import ListedColormapdef plot_decision_regions(X, y, classifier, resolution=0.02): # setup marker generator and color map markers = (&#x27;s&#x27;, &#x27;x&#x27;, &#x27;o&#x27;, &#x27;^&#x27;, &#x27;v&#x27;) colors = (&#x27;red&#x27;, &#x27;blue&#x27;, &#x27;lightgreen&#x27;, &#x27;gray&#x27;, &#x27;cyan&#x27;) cmap = ListedColormap(colors[:len(np.unique(y))]) # plt the decision surface x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot class sample for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl) 接着调用该函数就可以画出图像： 12345plot_decision_regions(X, y, classifier=ppn)plt.xlabel(&#x27;sepal length&#x27;)plt.ylabel(&#x27;petal length&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show() 通过图中可看出来，感知器能够对训练集中的所有数据进行正确的分类。 自适应线性神经元和学习的收敛性在感知器算法出现之后，又有人提出了Adaline算法，这个算法可以看作是对之前算法的改进。 基于Adaline规则的权重更新是通过一个连续的线性激励函数来完成的，而不像感知器中使用单位阶跃函数，这是二者主要的区别。在Adaline是算法中，激励函数是简单的恒等函数，即$ \\phi(w^Tx)&#x3D;w^Tx $。线性激励函数在更新权重的同时，我们使用量化器对类标进行预测，量化器和前面提到的单位阶跃函数类似，如下图所示： 对比前面的感知器算法模型而可以得到差别：使用线性激励函数的连续型输出值，而不是二类别分类类标来计算模型的误差以及更新权重。 通过梯度下降最小化代价函数在Adaline中，我们可以定义代价函数$J$为通过模型得到的输出值和实际值之间的误差平方和：$$J(w) &#x3D; \\frac{1}{2}\\sum_i(y^i-\\phi(z^i))^2$$这里，系数1&#x2F;2是为了方便的角度，是我们容易求梯度。这个代价函数是一个凸函数，这样我们可以通过简单高效的梯度下降优化算法得到权重，并且保证对训练数据进行分类时代价最小。 如下图所示，我们将梯度学习的原理形象地描述为下山： 这样，权重更新公式如下：$$w:&#x3D;w+\\Delta{w}$$对应的权重增量$ \\Delta{w} $定义为下：$$\\Delta{w}&#x3D;-\\eta\\Delta{J(w)}$$化简得到：$$\\Delta{w_j} &#x3D; \\eta\\sum_i(y^i-\\phi(z^i))x^i_j$$这样权重的更行是根据训练集中所有数据完成的，而不是每次一个样本渐进更新权重，这也是该方法被称为批量下降的原因。 使用Python实现自适应线性神经元我们将会根据感知器模型的代码来复写我们的Adaline模型，如下： 1234567891011121314151617181920212223242526class AdalineGD(object): def __init__(self, eta=0.01, n_iter=50): self.eta = eta self.n_iter = n_iter def fit(self, X, y): self.w_ = np.zeros(1 + X.shape[1]) self.cost_ = [] for i in range(self.n_iter): output = self.net_input(X) errors = (y - output) self.w_[1:] += self.eta * X.T.dot(errors) self.w_[0] += self.eta * errors.sum() cost = (errors**2).sum()/2.0 self.cost_.append(cost) return self def net_input(self, X): return np.dot(X, self.w_[1:]) + self.w_[0] def activation(self, X): return self.net_input(X) def predict(self, X): return np.where(self.activation(X) &gt;= 0.0, 1, -1) 感知器通过self.eta * errors.sum()来更新第0个位置的权重，通过self.eta * X.T.dot(errors)来更新第1到m个位置的权重，同时，我们设置一个列表self.cost_用于追踪本轮训练的误差值。 实践中，我们常常需要进行调参的工作，我们分别让$\\eta &#x3D; 0.1$和$\\eta &#x3D; 0.0001$来训练，同时绘制迭代次数和代价函数的图像，观察Adaline通过数据训练进行学习的效果。 1234567891011121314fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8,4))ada1 = AdalineGD(n_iter=10, eta=0.1).fit(X, y)ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker=&#x27;o&#x27;)ax[0].set_xlabel(&#x27;Epochs&#x27;)ax[0].set_ylabel(&#x27;log(SSE)&#x27;)ax[0].set_title(&#x27;Adaline - 0.1&#x27;)ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker=&#x27;o&#x27;)ax[1].set_xlabel(&#x27;Epochs&#x27;)ax[1].set_ylabel(&#x27;log(SSE)&#x27;)ax[1].set_title(&#x27;Adaline - 0.0001&#x27;)plt.show() 图像如下： 从图中可以看出，我们面临着这两种问题：左边图像显示了学习速率过大可能会导致并没有使得代价函数尽可能低，反而因为算法跳过了全局最优解，导致误差越来越大；右边图像虽然代价函数逐渐减少，但是学习速率太小，使得到算法收敛的目标需要更多次数的迭代。 为了提高算法优化的性能，我们将使用特征缩放的方法，也就是：$$x_j^{‘}&#x3D;\\frac{x_j - \\mu_j}{\\sigma_j}$$标准化可以通过Numpy的mean和std方法实现： 123X_std = np.copy(X)X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std() 接下来，按照$\\eta &#x3D; 0.01$的学习速率来对Adaline进行训练： 123456789101112ada = AdalineGD(n_iter=15, eta=0.01)ada.fit(X_std, y)plot_decision_regions(X_std, y, classifier=ada)plt.title(&#x27;Adaline - 0.01&#x27;)plt.xlabel(&#x27;sepal length&#x27;)plt.ylabel(&#x27;petal length&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show()plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker=&#x27;o&#x27;)plt.xlabel(&#x27;Epochs&#x27;)plt.ylabel(&#x27;SSE&#x27;)plt.show() 得到的图像如下： 如上图，虽然所有样本都被正确分类，但是误差平方和（SSE）的值仍然不为零。 大规模机器学习和随机梯度下降上一节中，我们对所有的数据进行计算，利用计算出来的结果来实现权重的更新。但是，机器学习面临的数据往往是包含着几百万数据的巨大数据集，这种情况下使用批量梯度下降的计算成本很高。 为了解决这个问题，我们引入随机梯度下降，和基于所有样本的累计误差更新权重的策略不同，我们每次使用一个训练样本渐进地更新权重：$$\\eta(y^i-\\phi(z^i))x^i$$ 当实现梯度随即下降时，通常规定学习速率如下：$$\\eta &#x3D; \\frac{c_1}{c_2 + [迭代次数]}$$ 为了让随机梯度下降得到更加准确的结果，让数据以随机的方式提供给算法时很重要的，因此，我们需要在每次迭代的时候打乱训练集。 随机梯度下降的另外一个优势是我们可以将其用于在线学习。通过在线学习，当有新的数据输入时模型会被实时训练。 小批次学习：介于梯度下降和随机梯度下降之间的一种技术。将数据分成一组组的训练数据，在对每组数进行训练。 随机梯度下降的Adaline算法如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859from numpy.random import seedclass AdalineSGD(object): def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None): self.eta = eta self.n_iter = n_iter self.w_initialized = False self.shuffle = shuffle if random_state: seed(random_state) def fit(self, X, y): self._initialize_weights(X.shape[1]) self.cost_ = [] for i in range(self.n_iter): if self.shuffle: X, y = self._shuffle(X, y) cost = [] for xi, target in zip(X, y): cost.append(self._update_weights(xi, target)) avg_cost = sum(cost) / len(y) self.cost_.append(avg_cost) return self def partial_fit(self, X, y): if not self.w_initialized: self._initialize_weights(X.shape[1]) if y.ravel().shape[0] &gt; 1: for xi, target in zip(X, y): self._update_weights(xi, target) else: self._update_weights(X, y) return self def _shuffle(self, X, y): r = np.random.permutation(len(y)) return X[r], y[r] def _initialize_weights(self, m): self.w_ = np.zeros(1 + m) self.w_initialized = True def _update_weights(self, xi, target): output = self.net_input(xi) errors = (target - output) self.w_[1:] += self.eta * xi.dot(errors) self.w_[0] += self.eta * errors cost = 0.5 * errors**2 return cost def net_input(self, X): return np.dot(X, self.w_[1:]) + self.w_[0] def activation(self, X): return self.net_input(X) def predict(self, X): return np.where(self.activation(X) &gt;= 0.0, 1, -1) 绘图程序如下： 123456789101112ada = AdalineSGD(n_iter=15, eta=0.01, random_state=1)ada.fit(X_std, y)plot_decision_regions(X_std, y, classifier=ada)plt.title(&#x27;Adaline - 0.01&#x27;)plt.xlabel(&#x27;sepal length&#x27;)plt.ylabel(&#x27;petal length&#x27;)plt.legend(loc=&#x27;upper left&#x27;)plt.show()plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker=&#x27;o&#x27;)plt.xlabel(&#x27;Epochs&#x27;)plt.ylabel(&#x27;Average Cost&#x27;)plt.show() 图像如下： 可以发现，代价函数的均值下降得很快，经过15次迭代后，基本趋势和梯度下降得到的图像类似。 如果改进模型，如用于数据流的在线学习，可以对单个样本简单调用partial_fit方法，如：ada.partial_fit(X_std[0, :], y[0])。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"赋予计算机学习数据的能力","slug":"赋予计算机学习数据的能力","date":"2020-01-24T04:07:05.000Z","updated":"2022-05-16T07:41:46.383Z","comments":true,"path":"2020/01/24/赋予计算机学习数据的能力/","link":"","permalink":"http://blog.zsstrike.tech/2020/01/24/%E8%B5%8B%E4%BA%88%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE%E7%9A%84%E8%83%BD%E5%8A%9B/","excerpt":"","text":"主要了解机器学习的主要概念和几种不同类型的学习算法。 机器学习的三种方法监督学习使用有类标的数据构建模型，使用经训练得到的模型对未来的数据进行预测。预测方法主要有两种： 使用分类进行预测 分类是监督学习的一个子类，目的是对过往已知示例的观察与学习，实现对新样本类标的预测。检测垃圾邮件的例子就是一种二类标的方法，当然还有多类别分类的例子，比如字母表中每个字母的识别。 使用回归预测连续输出值 这种方法用于对连续型输出变量进行预测，比如学习成绩分数和自习时间多少之间进行预测。 强化（半监督）学习强化学习的目标在于构建一个系统，在于环境的交互过程中逐步提高系统的性能。环境的当前状态中通常包含者一个反馈信号。常用的例子是象棋对弈的例子，在这个例子中，系统根据棋盘上的局势（环境）来决定落子的位置，而游戏结束的输赢可以当做是反馈信号。 无监督学习在这种学习方法下，我们将会处理无类标数据或者是总体趋势不明朗的数据，来提取出有效信息探索数据的整体结构。 通过聚类发现数据的子群 聚类是一种探索性的数据分析技术，在没有任何相关先验信息的情况下，它可以帮我们将数据分成有意义的小组别。 数据压缩中的降维 讲高维数据压缩，是之转变为相对容易处理的维度数据。 机器学习系统的蓝图数据预处理为了尽可能发回机器学习算法的性能，往往需要对原始数据进行处理使得它能达到算法要求的标准，同时选择较高关联的属性作为训练数据。 选择预测模型类型并进行训练和校正选择合适的机器学习算法来对训练数据集进行学习得到模型，同时利用反馈信号来对模型进行校正。 使用未知数据进行预测对未来的数据进行预测。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"Git 使用总结","slug":"Git-使用总结","date":"2019-10-27T12:32:26.000Z","updated":"2022-05-16T07:41:45.992Z","comments":true,"path":"2019/10/27/Git-使用总结/","link":"","permalink":"http://blog.zsstrike.tech/2019/10/27/Git-%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/","excerpt":"整理在使用 Git 过程中的一些总结。","text":"整理在使用 Git 过程中的一些总结。 merge &#x2F; rebase两者都是将其他分支上修改应用到当前工作分支上，不同的是 merge 会产生一个与 merge 相关的提交记录，而 rebase 则不会产生，除此之外，两者之间的 git flow 如下： cherry-pick用于将一些修改应用到当前工作的分支上： 1git cherry-pick &lt;commitHash&gt; | &lt;HashA&gt;..&lt;HashB&gt; 上面的命令分别表示应用 &lt;commitHash&gt; 以及 (HashA, HashB] 到当前工作的分支上。 shallow update not allowed这个问题的产生原因是在克隆远程仓库的时候采用了以下命令： 1git clone --depth=&lt;num&gt; &lt;remote-url&gt; 这将会导致shallow clone(浅复制)。这将会使得这个仓库不能向远程仓库进行push。通过以下命令可修复： 1git fetch --unshallow &lt;remote-repo&gt; 镜像源使用镜像源可以加速下载速度，常见的 Github 镜像源有： https://github.com.cnpmjs.org/ https://hub.fastgit.org/ https://github.wuyanzheshui.workers.dev/ 可以在克隆的时候替换 https://github.com ，也可以通过全局设置，让 git 命令的 url 指向上述镜像源： 1git config --global url.&quot;https://hub.fastgit.org&quot;.insteadOf https://github.com","categories":[],"tags":[{"name":"Git","slug":"Git","permalink":"http://blog.zsstrike.tech/tags/Git/"}]}],"categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.zsstrike.tech/tags/Redis/"},{"name":"操作系统","slug":"操作系统","permalink":"http://blog.zsstrike.tech/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Operating System","slug":"Operating-System","permalink":"http://blog.zsstrike.tech/tags/Operating-System/"},{"name":"计算机系统","slug":"计算机系统","permalink":"http://blog.zsstrike.tech/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"},{"name":"Computer Systems","slug":"Computer-Systems","permalink":"http://blog.zsstrike.tech/tags/Computer-Systems/"},{"name":"数据库","slug":"数据库","permalink":"http://blog.zsstrike.tech/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Database","slug":"Database","permalink":"http://blog.zsstrike.tech/tags/Database/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://blog.zsstrike.tech/tags/Algorithm/"},{"name":"Linux","slug":"Linux","permalink":"http://blog.zsstrike.tech/tags/Linux/"},{"name":"Java","slug":"Java","permalink":"http://blog.zsstrike.tech/tags/Java/"},{"name":"MySQL","slug":"MySQL","permalink":"http://blog.zsstrike.tech/tags/MySQL/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://blog.zsstrike.tech/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"分布式","slug":"分布式","permalink":"http://blog.zsstrike.tech/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.zsstrike.tech/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Hexo","slug":"Hexo","permalink":"http://blog.zsstrike.tech/tags/Hexo/"},{"name":"Typora","slug":"Typora","permalink":"http://blog.zsstrike.tech/tags/Typora/"},{"name":"CI/CD","slug":"CI-CD","permalink":"http://blog.zsstrike.tech/tags/CI-CD/"},{"name":"Nodejs","slug":"Nodejs","permalink":"http://blog.zsstrike.tech/tags/Nodejs/"},{"name":"Vim","slug":"Vim","permalink":"http://blog.zsstrike.tech/tags/Vim/"},{"name":"Python","slug":"Python","permalink":"http://blog.zsstrike.tech/tags/Python/"},{"name":"Nginx","slug":"Nginx","permalink":"http://blog.zsstrike.tech/tags/Nginx/"},{"name":"机器学习","slug":"机器学习","permalink":"http://blog.zsstrike.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://blog.zsstrike.tech/tags/TensorFlow/"},{"name":"SSR","slug":"SSR","permalink":"http://blog.zsstrike.tech/tags/SSR/"},{"name":"Git","slug":"Git","permalink":"http://blog.zsstrike.tech/tags/Git/"}]}