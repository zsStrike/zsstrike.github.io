<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;zh-HK&quot;,&quot;zh-TW&quot;,&quot;default&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>数据预处理 | zsStrike</title><meta name="keywords" content="机器学习"><meta name="author" content="zsStrike"><meta name="copyright" content="zsStrike"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="在本节中，我们将会学习主要的数据预处理技术，使用这些技术可以高效地构建好的机器学习模型。">
<meta property="og:type" content="article">
<meta property="og:title" content="数据预处理">
<meta property="og:url" content="http://zsstrike.github.io/2020/01/31/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/index.html">
<meta property="og:site_name" content="zsStrike">
<meta property="og:description" content="在本节中，我们将会学习主要的数据预处理技术，使用这些技术可以高效地构建好的机器学习模型。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://zsstrike.github.io/assets/default_cover.png">
<meta property="article:published_time" content="2020-01-31T07:54:15.000Z">
<meta property="article:modified_time" content="2022-05-16T07:41:46.307Z">
<meta property="article:author" content="zsStrike">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://zsstrike.github.io/assets/default_cover.png"><link rel="shortcut icon" href="/assets/avatar.jpg"><link rel="canonical" href="http://zsstrike.github.io/2020/01/31/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":5000,"languages":{"author":"作者: zsStrike","link":"链接: ","source":"来源: zsStrike","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '数据预处理',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-05-16 15:41:46'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/assets/avatar.jpg" onerror="onerror=null;src='/assets/default_cover.png'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">43</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/index"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">zsStrike</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/index"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">数据预处理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-01-31T07:54:15.000Z" title="发表于 2020-01-31 15:54:15">2020-01-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-05-16T07:41:46.307Z" title="更新于 2022-05-16 15:41:46">2022-05-16</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>16分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="数据预处理"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><p>在本节中，我们将会学习主要的数据预处理技术，使用这些技术可以高效地构建好的机器学习模型。</p>
<span id="more"></span>

<h2 id="缺失数据的处理"><a href="#缺失数据的处理" class="headerlink" title="缺失数据的处理"></a>缺失数据的处理</h2><p>在采集数据的时候，可能有的数据会有缺失的情况。通常我们见到的缺失值是数据表中的空值，或者是类似于NaN的占位符。</p>
<p>首先构造一个含有缺失值的CSV文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> StringIO</span><br><span class="line"></span><br><span class="line">csv_data = <span class="string">&quot;&quot;&quot;A, B, C, D</span></span><br><span class="line"><span class="string">1.0, 2.0, 3.0, 4.0</span></span><br><span class="line"><span class="string">5.0, 6.0, , 8.0</span></span><br><span class="line"><span class="string">10.0, 11.0, 12.0, </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">df = pd.read_csv(StringIO(csv_data))</span><br><span class="line"><span class="built_in">print</span>(df) </span><br><span class="line">&gt;&gt;       A     B      C     D</span><br><span class="line">&gt;&gt;  <span class="number">0</span>   <span class="number">1.0</span>   <span class="number">2.0</span>    <span class="number">3.0</span>   <span class="number">4.0</span></span><br><span class="line">&gt;&gt;  <span class="number">1</span>   <span class="number">5.0</span>   <span class="number">6.0</span>    NaN   <span class="number">8.0</span></span><br><span class="line">&gt;&gt;  <span class="number">2</span>  <span class="number">10.0</span>  <span class="number">11.0</span>   <span class="number">12.0</span>   NaN </span><br></pre></td></tr></table></figure>

<p>上述代码中，我们通过<code>read_csv</code>将CSV格式的数据读取到pandas库的<code>DataFrame</code>中，可以看到有两个缺失值。</p>
<p>对于大的DataFrame来说，我们可以使用内置的<code>isnull</code>方法来判断某单元中是否含有缺失值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df.isnull().<span class="built_in">sum</span>()</span><br><span class="line">&gt;&gt; A    <span class="number">0</span></span><br><span class="line">&gt;&gt; B    <span class="number">0</span></span><br><span class="line">&gt;&gt; C    <span class="number">1</span></span><br><span class="line">&gt;&gt; D    <span class="number">1</span></span><br><span class="line">&gt;&gt; dtype: int64</span><br></pre></td></tr></table></figure>

<p>通过这个方式我们可以得到每列中缺失值的数量。</p>
<h3 id="将存在缺失值的特征或样本删除"><a href="#将存在缺失值的特征或样本删除" class="headerlink" title="将存在缺失值的特征或样本删除"></a>将存在缺失值的特征或样本删除</h3><p>这是最简单的数据处理方式：将含有缺失值的特征（列）或者样本（行）从数据中删除。</p>
<p>可通过 <code>dropna</code>方法来删除包含缺失值的行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.dropna()</span><br><span class="line">&gt;&gt;       A     B      C     D</span><br><span class="line">&gt;&gt;  <span class="number">0</span>   <span class="number">1.0</span>   <span class="number">2.0</span>    <span class="number">3.0</span>   <span class="number">4.0</span></span><br></pre></td></tr></table></figure>

<p>类似地，我们可以通过将<code>axis</code>参数设置为1，以删除包含缺失值的列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.dropna(axis=<span class="number">1</span>)</span><br><span class="line">&gt;&gt;       A     B     </span><br><span class="line">&gt;&gt;  <span class="number">0</span>   <span class="number">1.0</span>   <span class="number">2.0</span>    </span><br><span class="line">&gt;&gt;  <span class="number">1</span>   <span class="number">5.0</span>   <span class="number">6.0</span>    </span><br><span class="line">&gt;&gt;  <span class="number">2</span>  <span class="number">10.0</span>  <span class="number">11.0</span>   </span><br></pre></td></tr></table></figure>

<p>同样地， <code>dropna</code>方法还有其他的参数，以应对各种缺失值的情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># only drop rows where all columns are NaN</span></span><br><span class="line">df.dropna(how=<span class="string">&#x27;any&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># drop rows that hava not at least 4 non-NaN value</span></span><br><span class="line">df.dropna(thresh=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># only drop rows where NaN in specific columns(here is &#x27;C&#x27;)</span></span><br><span class="line">df.dropna(subset=[<span class="string">&#x27;C&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p>删除数据是一种简单的方法，但是如果删除过多的样本，会导致分析结果可靠性不高。接下来学习另外一种最常用的处理缺失数据的方法：插值技术。</p>
<h3 id="缺失数据填充"><a href="#缺失数据填充" class="headerlink" title="缺失数据填充"></a>缺失数据填充</h3><p>所谓插值技术是指通过数据集中的其他训练样本的数据来估计缺失值，最常用的插值技术是<strong>均值插值</strong>（meaneinputation），即使用相应的特征均值来替换缺失值。我们可以使用scikit-learn中的<code>Impute</code>类来实现此方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line"></span><br><span class="line">imr = Imputer(missing_values=<span class="string">&#x27;NaN&#x27;</span>, strategy=<span class="string">&#x27;mean&#x27;</span>, axis=<span class="number">0</span>)</span><br><span class="line">imr = imr.fit(df)</span><br><span class="line">imputed_data = imr.transform(df.values)</span><br><span class="line">imputed_data</span><br><span class="line">&gt;&gt; array([[ <span class="number">1.</span> ,  <span class="number">2.</span> ,  <span class="number">3.</span> ,  <span class="number">4.</span> ],</span><br><span class="line">          [ <span class="number">5.</span> ,  <span class="number">6.</span> ,  <span class="number">7.5</span>,  <span class="number">8.</span> ],</span><br><span class="line">          [<span class="number">10.</span> , <span class="number">11.</span> , <span class="number">12.</span> ,  <span class="number">6.</span> ]]) </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>首先计算各个特征列的均值，然后将均值插入到NaN处。参数<code>axis</code>用来控制按列计算均值还是按行计算均值，参数<code>strategy</code>还有median和most_frequent可选值。</p>
<h3 id="理解scikit-learn预估器的API"><a href="#理解scikit-learn预估器的API" class="headerlink" title="理解scikit-learn预估器的API"></a>理解scikit-learn预估器的API</h3><p>上一节中，我们使用的<code>Imputer</code>类来填充我们数据集中的缺失值，这个类属于<code>scikit-learn</code>中的转换器类，主要用于数据的转换。这些类中常用的两个方法是<code>fit</code>和<code>transform</code>。其中，fit方法用于对数据集中的参数进行识别并且构建相应的数据补齐模型，而transform方法则使用刚创建的数据补齐模型对数据集中的缺失值进行补齐。</p>
<p>在前面的章节中，我们用到了分类器，它们在scikit-learn中属于预估器类别，其API的设计与转换器非常相似。预估器包含一个predict方法，同时也包含一个transform方法。</p>
<h2 id="处理类别数据"><a href="#处理类别数据" class="headerlink" title="处理类别数据"></a>处理类别数据</h2><p>目前我们只学习了处理数值型数据的方法，但是在真实的数据集中，常常会出现类别数据。类别数据可以进一步划分为<strong>标称特征</strong>和<strong>有序特征</strong>。有序特征可以理解为类别的值是可以排序的，如T恤的尺寸；相反，标称数据不具备排序的特征，如T恤的颜色。</p>
<p>首先构造一个数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame([</span><br><span class="line">    [<span class="string">&#x27;green&#x27;</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">10.1</span>, <span class="string">&#x27;class1&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;L&#x27;</span>, <span class="number">13.5</span>, <span class="string">&#x27;class2&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;XL&#x27;</span>, <span class="number">15.3</span>, <span class="string">&#x27;class1&#x27;</span>]</span><br><span class="line">])</span><br><span class="line">df.columns = [<span class="string">&#x27;color&#x27;</span>, <span class="string">&#x27;size&#x27;</span>, <span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;classlabel&#x27;</span>]</span><br><span class="line">df</span><br><span class="line">&gt;&gt;  	color	size	price	classlabel</span><br><span class="line">&gt;&gt; <span class="number">0</span>	green	M	 	<span class="number">10.1</span>	class1</span><br><span class="line">&gt;&gt; <span class="number">1</span>	red	    L	    <span class="number">13.5</span>	class2</span><br><span class="line">&gt;&gt; <span class="number">2</span>	blue	XL		<span class="number">15.3</span>	class1</span><br></pre></td></tr></table></figure>

<p>我们构造的数据包括一个标称特征（颜色），一个有序特征（大小）以及一个数据特征（价格）。类标存储在最后一类。</p>
<h3 id="有序特征的映射"><a href="#有序特征的映射" class="headerlink" title="有序特征的映射"></a>有序特征的映射</h3><p>对于有序特征，scikit-learn中没有实现相应的自动转换方法，因此，我们需要手动构造相应的映射。假设尺寸之间的关系是：XL &#x3D; L + 1 &#x3D; M + 2.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">size_mapping = &#123;<span class="string">&#x27;XL&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;L&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;M&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">df[<span class="string">&#x27;size&#x27;</span>] = df[<span class="string">&#x27;size&#x27;</span>].<span class="built_in">map</span>(size_mapping)</span><br><span class="line">df</span><br><span class="line">&gt;&gt;  	color	size	price	classlabel</span><br><span class="line">&gt;&gt; <span class="number">0</span>	green	<span class="number">1</span>	 	<span class="number">10.1</span>	class1</span><br><span class="line">&gt;&gt; <span class="number">1</span>	red	    <span class="number">2</span>	    <span class="number">13.5</span>	class2</span><br><span class="line">&gt;&gt; <span class="number">2</span>	blue	<span class="number">3</span>		<span class="number">15.3</span>	class1</span><br></pre></td></tr></table></figure>

<p>如果在后续的过程中需要将整数值还原为有序字符串，可以简单定义一个逆映射字典<code>inv_size_mapping = &#123;v : k for k, v in size_mapping.items()&#125;</code>，然后再使用pandas提供的map方法即可。</p>
<h3 id="类标的编码"><a href="#类标的编码" class="headerlink" title="类标的编码"></a>类标的编码</h3><p>许多机器学习库中要求类标以整数值的方式进行编码。需要注意的一点是，类标不是有序的，因此，我们只需要简单的以枚举的方式从0开始设定类标：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">class_mapping = &#123;label: idx <span class="keyword">for</span> idx, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(np.unique(df[<span class="string">&#x27;classlabel&#x27;</span>]))&#125;</span><br><span class="line">class_mapping</span><br><span class="line">&gt;&gt; &#123;<span class="string">&#x27;class1&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;class2&#x27;</span>: <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>

<p>接下来映射一下就行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;classlabel&#x27;</span>] = df[<span class="string">&#x27;classlabel&#x27;</span>].<span class="built_in">map</span>(class_mapping)</span><br><span class="line">df</span><br><span class="line">&gt;&gt;  	color	size	price	classlabel</span><br><span class="line">&gt;&gt; <span class="number">0</span>	green	<span class="number">1</span>	 	<span class="number">10.1</span>	<span class="number">0</span></span><br><span class="line">&gt;&gt; <span class="number">1</span>	red	    <span class="number">2</span>	    <span class="number">13.5</span>	<span class="number">1</span></span><br><span class="line">&gt;&gt; <span class="number">2</span>	blue	<span class="number">3</span>		<span class="number">15.3</span>	<span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>同样可以构造一个逆映射来将类表还原为字符串。</p>
<p>此外，可以使用scikit-learn中的LabelEncoder类可以更加方便完成对类标的编码工作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line">class_le = LabelEncoder()</span><br><span class="line">y = class_le.fit_transform(df[<span class="string">&#x27;classlabel&#x27;</span>].values)</span><br><span class="line">y</span><br><span class="line">&gt;&gt; array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], dtype=int64)</span><br></pre></td></tr></table></figure>

<p>同样可以使用<code>inverse_transform</code>方法将类标转换为原始的字符串。</p>
<h3 id="标称特征的独热编码"><a href="#标称特征的独热编码" class="headerlink" title="标称特征的独热编码"></a>标称特征的独热编码</h3><p>常见的思路如下，使用LabelEncoder类将字符串转换为整数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = df[[<span class="string">&#x27;color&#x27;</span>, <span class="string">&#x27;size&#x27;</span>, <span class="string">&#x27;price&#x27;</span>]].values</span><br><span class="line">color_le = LabelEncoder()</span><br><span class="line">X[:, <span class="number">0</span>] = color_le.fit_transform(X[:, <span class="number">0</span>])</span><br><span class="line">X</span><br><span class="line">&gt;&gt; array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">10.1</span>],</span><br><span class="line">       	  [<span class="number">2</span>, <span class="number">2</span>, <span class="number">13.5</span>],</span><br><span class="line">          [<span class="number">0</span>, <span class="number">3</span>, <span class="number">15.3</span>]], dtype=<span class="built_in">object</span>)</span><br></pre></td></tr></table></figure>

<p>这样的数据处理是常见的错误处理方式，因为学习算法将会假定green大于blue，red大于green，这显然是不合理的。</p>
<p>标称特征不能像有序特征一样简单赋予一个整数值，最常用的转换方法是<strong>独热编码</strong>技术。这个方法的思想就是创建一个新的虚拟特征，虚拟特征的每一列各代表标称数据的一个值。在此，我们将color特征转换为三个新的特征：blue，green和red。此时可以通过二进制值来标识样本的颜色。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line">ohe = OneHotEncoder(categorical_features=[<span class="number">0</span>])</span><br><span class="line">ohe.fit_transform(X).toarray()</span><br><span class="line">&gt;&gt; array([[ <span class="number">0.</span> ,  <span class="number">1.</span> ,  <span class="number">0.</span> ,  <span class="number">1.</span> , <span class="number">10.1</span>],</span><br><span class="line">          [ <span class="number">0.</span> ,  <span class="number">0.</span> ,  <span class="number">1.</span> ,  <span class="number">2.</span> , <span class="number">13.5</span>],</span><br><span class="line">          [ <span class="number">1.</span> ,  <span class="number">0.</span> ,  <span class="number">0.</span> ,  <span class="number">3.</span> , <span class="number">15.3</span>]])</span><br></pre></td></tr></table></figure>

<p>另外，我们可以通过pandas中的get_dummies方法，更加方便地实现虚拟特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pd.get_dummies(df[[<span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;color&#x27;</span>, <span class="string">&#x27;size&#x27;</span>]])</span><br><span class="line">&gt;&gt; 	price	size	color_blue	color_green	color_red</span><br><span class="line">&gt;&gt;<span class="number">0</span>	<span class="number">10.1</span>	<span class="number">1</span>		<span class="number">0</span>			<span class="number">1</span>			<span class="number">0</span>	</span><br><span class="line">&gt;&gt;<span class="number">1</span>	<span class="number">13.5</span>	<span class="number">2</span>		<span class="number">0</span>			<span class="number">0</span>			<span class="number">1</span></span><br><span class="line">&gt;&gt;<span class="number">2</span>	<span class="number">15.3</span>	<span class="number">3</span>		<span class="number">1</span>			<span class="number">0</span>			<span class="number">0</span></span><br></pre></td></tr></table></figure>

<h2 id="将数据集划分为训练数据集和测试数据集"><a href="#将数据集划分为训练数据集和测试数据集" class="headerlink" title="将数据集划分为训练数据集和测试数据集"></a>将数据集划分为训练数据集和测试数据集</h2><p>接下来，我们将会使用葡萄酒数据集，可以通过UCI机器学习样本数据库来获得。通过pandas库，我们可以在线获取数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df_wine = pd.read_csv(<span class="string">&#x27;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&#x27;</span>, header=<span class="literal">None</span>)</span><br><span class="line">df_wine.columns = [<span class="string">&#x27;Class label&#x27;</span>, <span class="string">&#x27;Alcohol&#x27;</span>, <span class="string">&#x27;Malic acid&#x27;</span>, <span class="string">&#x27;Ash&#x27;</span>, <span class="string">&#x27;Alcalinity of ash&#x27;</span>, <span class="string">&#x27;Magnesium&#x27;</span>, <span class="string">&#x27;Total phenols&#x27;</span>, <span class="string">&#x27;Flavanoids&#x27;</span>,</span><br><span class="line">                  <span class="string">&#x27;Nonflavanoid phenols&#x27;</span>, <span class="string">&#x27;Proanthocyanins&#x27;</span>, <span class="string">&#x27;Color intensity&#x27;</span>, <span class="string">&#x27;Hue&#x27;</span>, <span class="string">&#x27;diluted wines&#x27;</span>, <span class="string">&#x27;Proline&#x27;</span>]</span><br><span class="line">df_wine.head()</span><br></pre></td></tr></table></figure>

<p>得到数据集如下：</p>
<p><img src="/2020/01/31/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/1580709518611.png" alt="1580709518611"></p>
<p>葡萄酒样本库通过13个不同的特征，对178个葡萄酒样本划分为类标为1，2，3的三个不同的类别，想要将这些样本划分为训练数据集和测试数据集，可以使用scikit-learn中的<code>train_test_split</code>函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X, y = df_wine.iloc[:, <span class="number">1</span>:].values, df_wine.iloc[:, <span class="number">0</span>].values</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>这样，我们就得到了$ 30% $的测试样本和$ 70% $的训练样本。</p>
<h2 id="将特征的值缩放到相同的区间"><a href="#将特征的值缩放到相同的区间" class="headerlink" title="将特征的值缩放到相同的区间"></a>将特征的值缩放到相同的区间</h2><p> <strong>特征缩放</strong>(peature scaling)是数据预处理中至关重要的一步，除了决策树和随机森林不需要特征缩放，其他的机器学习算法几乎都需要这个处理使得算法准确度提高。</p>
<p>特征缩放有两个常用的方法：归一化和标准化。归一化指的是将特征的值缩放到区间$ [0,1] $上，可以使用min-max缩放来实现：<br>$$<br>x_{norm}^i &#x3D; \frac{x^i - x_{min}}{x^i - x_{max}}<br>$$<br>在scikit-learn中，已经实现了min-max缩放：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line">mms = MinMaxScaler()</span><br><span class="line">X_train_norm = mms.fit_transform(X_train)</span><br><span class="line">X_test_norm = mms.fit_transform(X_test)</span><br></pre></td></tr></table></figure>

<p>而标准化的过程可以使用如下方程：<br>$$<br>x_{std}^i &#x3D; \frac{x^i - \mu_x}{\sigma_x}<br>$$<br>其中，$ \mu_x $和$ \sigma_x $分别表示某个特征列的均值和样本。同样地，可以使用scikit-learn中的方法实现标准化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">stdsc = StandardScaler()</span><br><span class="line">X_train_std = stdsc.fit_transform(X_train)</span><br><span class="line">X_test_std = stdsc.fit_transform(X_test)</span><br></pre></td></tr></table></figure>

<h2 id="选择有意义的特征"><a href="#选择有意义的特征" class="headerlink" title="选择有意义的特征"></a>选择有意义的特征</h2><p>如果一个模型在训练数据集上面的表现比在测试数据集上面好很多，那么很可能产生了过拟合。在本节中，我们将会学习使用正则化和特征选择降维这两种常用的减少过拟合问题的方法。</p>
<h3 id="使用L1正则化满足数据稀疏化"><a href="#使用L1正则化满足数据稀疏化" class="headerlink" title="使用L1正则化满足数据稀疏化"></a>使用L1正则化满足数据稀疏化</h3><p>在第三章节中，权重向量的L2范数如下：<br>$$<br>L2：||w||<em>2^2&#x3D;\sum</em>{j&#x3D;1}^{m}w_j^2<br>$$<br>而降低模型复杂度的L1正则化公式：<br>$$<br>L1：||w||<em>1 &#x3D; \sum</em>{j&#x3D;1}^m|w_j|<br>$$<br>对于scikit-learn来说，已经支持了 L1的正则化模型，可以将<code>penalty</code>参数设置为’l1’来进行简单的数据稀处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">LogisticRegression(penalty=<span class="string">&#x27;l1&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>我们将L1正则化用于标准化处理的葡萄酒数据，经过L1正则化的逻辑斯蒂回归模型可以产生如下稀疏化结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lr = LogisticRegression(penalty=<span class="string">&#x27;l1&#x27;</span>, C=<span class="number">0.1</span>)</span><br><span class="line">lr.fit(X_train_std, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training accuracy: &#x27;</span>, lr.score(X_train_std, y_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test accuracy: &#x27;</span>, lr.score(X_test_std, y_test))</span><br><span class="line">&gt;&gt; Training accuracy:  <span class="number">0.9838709677419355</span></span><br><span class="line">&gt;&gt; Test accuracy:  <span class="number">0.9814814814814815</span></span><br></pre></td></tr></table></figure>

<p>训练和测试的精确度显示此模型未出现过拟合，通过如下代码可以获得截距项：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr.intercept_</span><br><span class="line">&gt;&gt; array([-<span class="number">0.38381104</span>, -<span class="number">0.1580416</span> , -<span class="number">0.70043119</span>])</span><br></pre></td></tr></table></figure>

<p>由于我们lr对象默认使用了一对多(One vs Rest, OvR)的方法，因此，第一项截距是类别1相对于类别2和类别3的匹配结果。同样，我们可以查看系数矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">lr.coef_</span><br><span class="line">&gt;&gt; array([[ <span class="number">0.2801916</span> ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        , -<span class="number">0.02793952</span>,  <span class="number">0.</span>        ,</span><br><span class="line">&gt;&gt;         <span class="number">0.</span>        ,  <span class="number">0.71018709</span>,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,</span><br><span class="line">&gt;&gt;         <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">1.2362193</span> ],</span><br><span class="line">&gt;&gt;       [-<span class="number">0.64408995</span>, -<span class="number">0.06876656</span>, -<span class="number">0.05722202</span>,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,</span><br><span class="line">&gt;&gt;         <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        , -<span class="number">0.92643033</span>,</span><br><span class="line">&gt;&gt;         <span class="number">0.06037655</span>,  <span class="number">0.</span>        , -<span class="number">0.37111071</span>],</span><br><span class="line">&gt;&gt;       [ <span class="number">0.</span>        ,  <span class="number">0.06151885</span>,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,</span><br><span class="line">&gt;&gt;         <span class="number">0.</span>        , -<span class="number">0.6360538</span> ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.49810762</span>,</span><br><span class="line">&gt;&gt;        -<span class="number">0.35817768</span>, -<span class="number">0.57128442</span>,  <span class="number">0.</span>        ]])</span><br></pre></td></tr></table></figure>

<p>可以发现，权重向量是稀疏的，这意味着只有少数几个特征被考虑进来，符合L1的作用效果。</p>
<p>最后，对L1正则化来说，在强的正则化参数(C&lt;0.1)的作用下，罚项使得所有的特征权重趋于0。</p>
<blockquote>
<p>在前面已经介绍过，$ \lambda $是正则化参数，而C是正则化参数的倒数。</p>
</blockquote>
<h3 id="序列特征选择算法"><a href="#序列特征选择算法" class="headerlink" title="序列特征选择算法"></a>序列特征选择算法</h3><p>另外一种降低模型复杂度从而解决过拟合问题的方法是通过特征选择进行<strong>降维</strong>，该方法对未经正则化处理的模型特别有效。降维技术主要分为两个大类：<strong>特征选择</strong>和<strong>特征提取</strong>。通过特征选择，可以选择原始特征的一个子集；而在特征提取中，通过对现有的特征信息进行推演，构造出一个新的特征子空间。</p>
<p>在本节中，我们着眼于一些经 </p>
<p>序列特征选择算法是一种贪婪搜索方法，用于将原始的d维特征空间压缩到一个k维空间中，其中$ k &lt; d $。一个经典的序列特征选择算法是<strong>序列后向选择算法</strong>（SBS），其目的是在分类行性能衰弱最小的约束小，降低原始数据的维度，提高计算效率。</p>
<p>SBS算法的理念很简单：SBS依次从特征集合中删除某些特征，直到新的特征子空间包含指定数量的特征。为了确定每一步需要删除的特征，为此我们需要定义一个最小化的标准衡量函数J。该函数的计算准则是：比较判定分类器在删除某个特征前后的差异，每次删除的特征，就是那些能够使得标准衡量函数值尽可能大的特征，或者说，每一步特征被删除后，所引起的模型性能损失最小。</p>
<p>基于上述对SBS的定义，总结出以下四个步骤：</p>
<ol>
<li>设$ k &#x3D; d $进行算法初始化，其中 d 是特征空间$ X_d $的维度。</li>
<li>定义$ x^- $为满足标准$ x^- &#x3D; argmaxJ(X_k - x) $最大化的特征，其中$ x \in X_k $。</li>
<li>将特征$ x^- $从特征集中删除：$ X_{k-1}  &#x3D; X_k - x^- , k &#x3D; k -1$。</li>
<li>如果k的值等于目标特征数量，算法终止，否则跳转到第2步。</li>
</ol>
<p>遗憾的是，scikit-learn并没有实现SBS算法，我们可以手动实现它：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> clone</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SBS</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, estimator, k_features, scoring=accuracy_score, test_size=<span class="number">0.25</span>, random_state=<span class="number">1</span></span>):</span><br><span class="line">        self.scoring = scoring</span><br><span class="line">        self.estimator = clone(estimator)</span><br><span class="line">        self.k_features = k_features</span><br><span class="line">        self.test_size = test_size</span><br><span class="line">        slef.random_state = random_state</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state)</span><br><span class="line">        dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line">        self.indices_ = <span class="built_in">tuple</span>(<span class="built_in">range</span>(dim))</span><br><span class="line">        self.subsets_ = [self.indices_]</span><br><span class="line">        score = self._calc_score(X_train, y_train, X_test, y_test, self.indices_)</span><br><span class="line">        self.scores_ = [score]</span><br><span class="line">        <span class="keyword">while</span> dim &gt; self.k_features:</span><br><span class="line">            scores = []</span><br><span class="line">            subsets = []</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> combinations(self.indices_, r=dim-<span class="number">1</span>):</span><br><span class="line">                score = self._calc_score(X_train, y_train, X_test, y_test, p)</span><br><span class="line">                scores.append(score)</span><br><span class="line">                subsets.append(p)</span><br><span class="line">            best = np.argmax(scores)</span><br><span class="line">            self.indices_ = subsets[best]</span><br><span class="line">            self.subsets_.append(self.indices_)</span><br><span class="line">            dim -= <span class="number">1</span></span><br><span class="line">            self.scores_.append(scores[best])</span><br><span class="line">        self.k_score_ = self.scores_[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transform</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X[:, self.indices_]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_calc_score</span>(<span class="params">self, X_train, y_train, X_test, y_test, indices</span>):</span><br><span class="line">        self.estimator.fit(X_train[:, indices], y_train)</span><br><span class="line">        y_pred = self.estimator.predict(X_test[:, indices])</span><br><span class="line">        score = self.scoring(y_test, y_pred)</span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line">            </span><br></pre></td></tr></table></figure>

<p>我们使用k_features来指定需要返回的特征数量，并且最终特征子集的列标被赋值给self.indices_。注意，在fit方法中，我们没有在fit方法中明确地计算评价标准，只是简单的删除了那些没有包含在最优特征子集中的特征。</p>
<p>接下来我们看一下SBS应用于KNN分类器的效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">2</span>)</span><br><span class="line">sbs = SBS(knn, k_features=<span class="number">1</span>)</span><br><span class="line">sbs.fit(X_train_std, y_train)</span><br><span class="line">k_feat = [<span class="built_in">len</span>(k) <span class="keyword">for</span> k <span class="keyword">in</span> sbs.subsets_]</span><br><span class="line">plt.plot(k_feat, sbs.scores_, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.ylim([<span class="number">0.7</span>, <span class="number">1.1</span>])</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of features&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>得到的图像如下：</p>
<p><img src="/Mon,%2003%20Feb%202020%20201354.png" alt="img"></p>
<p>可以发现，当k &#x3D; {5, 6, 7, 8, 9, 10}时，算法可以达到百分百的准确率。</p>
<p>接下来看一下是哪五个特征在验证数据集上有如此良好的表现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">k5 = <span class="built_in">list</span>(sbs.subsets_[<span class="number">8</span>])</span><br><span class="line"><span class="built_in">print</span>(df_wine.columns[<span class="number">1</span>:][k5])</span><br><span class="line">&gt;&gt; Index([<span class="string">&#x27;Alcohol&#x27;</span>, <span class="string">&#x27;Malic acid&#x27;</span>, <span class="string">&#x27;Alcalinity of ash&#x27;</span>, <span class="string">&#x27;Hue&#x27;</span>, <span class="string">&#x27;Proline&#x27;</span>], dtype=<span class="string">&#x27;object&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="通过随机森林判定特征的重要性"><a href="#通过随机森林判定特征的重要性" class="headerlink" title="通过随机森林判定特征的重要性"></a>通过随机森林判定特征的重要性</h2><p>接下来使用随机森林来从数据集中选择相关特征，下面的代码根据葡萄酒数据集特征重要程度对这13个特征给出重要性等级。但是注意：<strong>无需对基于树的模型做标准化或者归一化处理</strong>。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">feat_labels = df_wine.columns[<span class="number">1</span>:]</span><br><span class="line">forest = RandomForestClassifier(n_estimators=<span class="number">10000</span>, random_state=<span class="number">0</span>, n_jobs=-<span class="number">1</span>)</span><br><span class="line">forest.fit(X_train, y_train)</span><br><span class="line">importances = forest.feature_importances_</span><br><span class="line">indices = np.argsort(importances)[::-<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">range</span>(X_train.shape[<span class="number">1</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%2d) %-*s %f&quot;</span> % (f + <span class="number">1</span>, <span class="number">30</span>, feat_labels[f], importances[indices[f]]))</span><br></pre></td></tr></table></figure>

<p>得到的输出数据如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> 1) Alcohol                        0.182483</span><br><span class="line"> 2) Malic acid                     0.158610</span><br><span class="line"> 3) Ash                            0.150948</span><br><span class="line"> 4) Alcalinity of ash              0.131987</span><br><span class="line"> 5) Magnesium                      0.106589</span><br><span class="line"> 6) Total phenols                  0.078243</span><br><span class="line"> 7) Flavanoids                     0.060718</span><br><span class="line"> 8) Nonflavanoid phenols           0.032033</span><br><span class="line"> 9) Proanthocyanins                0.025400</span><br><span class="line">10) Color intensity                0.022351</span><br><span class="line">11) Hue                            0.022078</span><br><span class="line">12) diluted wines                  0.014645</span><br><span class="line">13) Proline                        0.013916</span><br></pre></td></tr></table></figure>

<p>从上述输出我们可以得到最具有判别效果的特征是‘Alcohol’。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://zsstrike.github.io">zsStrike</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://zsstrike.github.io/2020/01/31/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/">http://zsstrike.github.io/2020/01/31/数据预处理/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://zsstrike.github.io" target="_blank">zsStrike</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/assets/default_cover.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/assets/avatar.jpg" onerror="this.onerror=null;this.src='/assets/default_cover.png'" alt="avatar"/></div><div class="author-info__name">zsStrike</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">43</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zsStrike"><i class="fab fa-github"></i><span>Github</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E5%A4%B1%E6%95%B0%E6%8D%AE%E7%9A%84%E5%A4%84%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">缺失数据的处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86%E5%AD%98%E5%9C%A8%E7%BC%BA%E5%A4%B1%E5%80%BC%E7%9A%84%E7%89%B9%E5%BE%81%E6%88%96%E6%A0%B7%E6%9C%AC%E5%88%A0%E9%99%A4"><span class="toc-number">1.1.</span> <span class="toc-text">将存在缺失值的特征或样本删除</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E5%A4%B1%E6%95%B0%E6%8D%AE%E5%A1%AB%E5%85%85"><span class="toc-number">1.2.</span> <span class="toc-text">缺失数据填充</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%90%86%E8%A7%A3scikit-learn%E9%A2%84%E4%BC%B0%E5%99%A8%E7%9A%84API"><span class="toc-number">1.3.</span> <span class="toc-text">理解scikit-learn预估器的API</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E7%B1%BB%E5%88%AB%E6%95%B0%E6%8D%AE"><span class="toc-number">2.</span> <span class="toc-text">处理类别数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%89%E5%BA%8F%E7%89%B9%E5%BE%81%E7%9A%84%E6%98%A0%E5%B0%84"><span class="toc-number">2.1.</span> <span class="toc-text">有序特征的映射</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B1%BB%E6%A0%87%E7%9A%84%E7%BC%96%E7%A0%81"><span class="toc-number">2.2.</span> <span class="toc-text">类标的编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E7%A7%B0%E7%89%B9%E5%BE%81%E7%9A%84%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81"><span class="toc-number">2.3.</span> <span class="toc-text">标称特征的独热编码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86%E4%B8%BA%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.</span> <span class="toc-text">将数据集划分为训练数据集和测试数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86%E7%89%B9%E5%BE%81%E7%9A%84%E5%80%BC%E7%BC%A9%E6%94%BE%E5%88%B0%E7%9B%B8%E5%90%8C%E7%9A%84%E5%8C%BA%E9%97%B4"><span class="toc-number">4.</span> <span class="toc-text">将特征的值缩放到相同的区间</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E6%9C%89%E6%84%8F%E4%B9%89%E7%9A%84%E7%89%B9%E5%BE%81"><span class="toc-number">5.</span> <span class="toc-text">选择有意义的特征</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8L1%E6%AD%A3%E5%88%99%E5%8C%96%E6%BB%A1%E8%B6%B3%E6%95%B0%E6%8D%AE%E7%A8%80%E7%96%8F%E5%8C%96"><span class="toc-number">5.1.</span> <span class="toc-text">使用L1正则化满足数据稀疏化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%AE%97%E6%B3%95"><span class="toc-number">5.2.</span> <span class="toc-text">序列特征选择算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%88%A4%E5%AE%9A%E7%89%B9%E5%BE%81%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-number">6.</span> <span class="toc-text">通过随机森林判定特征的重要性</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2017 - 2022 By zsStrike</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>