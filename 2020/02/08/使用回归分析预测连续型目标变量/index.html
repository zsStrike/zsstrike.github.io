<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;zh-HK&quot;,&quot;zh-TW&quot;,&quot;default&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>使用回归分析预测连续型目标变量 | zsStrike</title><meta name="keywords" content="机器学习"><meta name="author" content="zsStrike"><meta name="copyright" content="zsStrike"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本章将会介绍监督学习的另外一个分支，回归分析（regression analysis）。回归模型可以用于连续型目标变量的预测分析，这使得它在探寻变量间关系，评估趋势，做出预测等领域极具吸引力。具体的例子如预测公司在未来几个月的销售情况等。">
<meta property="og:type" content="article">
<meta property="og:title" content="使用回归分析预测连续型目标变量">
<meta property="og:url" content="http://blog.zsstrike.tech/2020/02/08/%E4%BD%BF%E7%94%A8%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%E8%BF%9E%E7%BB%AD%E5%9E%8B%E7%9B%AE%E6%A0%87%E5%8F%98%E9%87%8F/index.html">
<meta property="og:site_name" content="zsStrike">
<meta property="og:description" content="本章将会介绍监督学习的另外一个分支，回归分析（regression analysis）。回归模型可以用于连续型目标变量的预测分析，这使得它在探寻变量间关系，评估趋势，做出预测等领域极具吸引力。具体的例子如预测公司在未来几个月的销售情况等。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://blog.zsstrike.tech/assets/default_cover.png">
<meta property="article:published_time" content="2020-02-08T04:32:51.000Z">
<meta property="article:modified_time" content="2022-05-16T07:41:46.287Z">
<meta property="article:author" content="zsStrike">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.zsstrike.tech/assets/default_cover.png"><link rel="shortcut icon" href="/assets/avatar.jpg"><link rel="canonical" href="http://blog.zsstrike.tech/2020/02/08/%E4%BD%BF%E7%94%A8%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%E8%BF%9E%E7%BB%AD%E5%9E%8B%E7%9B%AE%E6%A0%87%E5%8F%98%E9%87%8F/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":5000,"languages":{"author":"作者: zsStrike","link":"链接: ","source":"来源: zsStrike","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '使用回归分析预测连续型目标变量',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-05-16 15:41:46'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/assets/avatar.jpg" onerror="onerror=null;src='/assets/default_cover.png'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">50</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/index"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">zsStrike</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/index"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">使用回归分析预测连续型目标变量</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-02-08T04:32:51.000Z" title="发表于 2020-02-08 12:32:51">2020-02-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-05-16T07:41:46.287Z" title="更新于 2022-05-16 15:41:46">2022-05-16</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>18分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="使用回归分析预测连续型目标变量"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><p>本章将会介绍监督学习的另外一个分支，<strong>回归分析</strong>（regression analysis）。回归模型可以用于连续型目标变量的预测分析，这使得它在探寻变量间关系，评估趋势，做出预测等领域极具吸引力。具体的例子如预测公司在未来几个月的销售情况等。</p>
<span id="more"></span>

<h2 id="简单线性回归模型初探"><a href="#简单线性回归模型初探" class="headerlink" title="简单线性回归模型初探"></a>简单线性回归模型初探</h2><p>简单（单变量）线性回归的目标是：通过模型来描述某一特征（解释变量x）与输出变量（目标变量y）之间的关系。当只有一个解释变量时，线性模型函数定义如下：<br>$$<br>y &#x3D; w_0 + w_1x<br>$$<br>其中，$ w_0 $为函数在y轴上的截距，$ w_1 $为解释变量的系数。</p>
<p>基于前面定义的线性方程，线性回归可以看作是求解样本点的最佳拟合直线，如下图：</p>
<p><img src="/2020/02/08/%E4%BD%BF%E7%94%A8%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%E8%BF%9E%E7%BB%AD%E5%9E%8B%E7%9B%AE%E6%A0%87%E5%8F%98%E9%87%8F/1581137289049.png" alt="1581137289049"></p>
<p>这条最佳拟合线称作是<strong>回归线</strong>，回归线和样本点之间的垂直连线就是偏移或<strong>残差</strong>。</p>
<p>多元线性回归函数定义如下：<br>$$<br>y &#x3D; w_0x_0+w_1x_1+\cdots+w_mx_m<br>$$<br>其中，$ w_0 $时$ x_0 &#x3D;1 $时在y轴上的截距。</p>
<h2 id="波士顿房屋数据集"><a href="#波士顿房屋数据集" class="headerlink" title="波士顿房屋数据集"></a>波士顿房屋数据集</h2><p>在本章的后续内容中，我们将会使用房屋价格（MEDV）作为目标变量，使用其他13个变量中的一个或多个值作为解释变量对其进行预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data&#x27;</span>, header=<span class="literal">None</span>, sep=<span class="string">&#x27;\s+&#x27;</span>)</span><br><span class="line">df.columns = [<span class="string">&#x27;CRIM&#x27;</span>, <span class="string">&#x27;ZN&#x27;</span>, <span class="string">&#x27;INDUS&#x27;</span>, <span class="string">&#x27;CHAS&#x27;</span>, <span class="string">&#x27;NOX&#x27;</span>, <span class="string">&#x27;RM&#x27;</span>, <span class="string">&#x27;AGE&#x27;</span>, </span><br><span class="line">              <span class="string">&#x27;DIS&#x27;</span>, <span class="string">&#x27;RAD&#x27;</span>, <span class="string">&#x27;TAX&#x27;</span>, <span class="string">&#x27;PTRATIO&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;LSTAT&#x27;</span>, <span class="string">&#x27;MEDV&#x27;</span>]</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>

<p> 输出如下：</p>
<p><img src="/2020/02/08/%E4%BD%BF%E7%94%A8%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%E8%BF%9E%E7%BB%AD%E5%9E%8B%E7%9B%AE%E6%A0%87%E5%8F%98%E9%87%8F/1581140075569.png" alt="1581140075569"></p>
<p>搜索性数据分析（EDA）是机器学习模型训练前的一个重要步骤。首先，借助散点图矩阵，我们可以以可视化的方法汇总显示各不同特征两两之间的关系：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, context=<span class="string">&#x27;notebook&#x27;</span>)</span><br><span class="line">cols = [<span class="string">&#x27;LSTAT&#x27;</span>, <span class="string">&#x27;INDUS&#x27;</span>, <span class="string">&#x27;NOX&#x27;</span>, <span class="string">&#x27;RM&#x27;</span>, <span class="string">&#x27;MEDV&#x27;</span>]</span><br><span class="line">sns.pairplot(df[cols], size=<span class="number">2.5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>得到的图像如下：</p>
<p><img src="/2020/02/08/%E4%BD%BF%E7%94%A8%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%E8%BF%9E%E7%BB%AD%E5%9E%8B%E7%9B%AE%E6%A0%87%E5%8F%98%E9%87%8F/fig.png" alt="fig"></p>
<p>通过此散点图矩阵，我们可以快速了解数据是如何分布的，以及其中是否包含异常值。从右下角子图可以发现：MEDV看似呈正态分布，但是包含几个异常值。</p>
<p>为了量化特征之间的关系，我们创建一个相关系数矩阵。相关系数矩阵是一个包含<strong>皮尔逊积矩相关系数</strong>，它是用来衡量两两特征间的线性依赖关系。计算公式如下：<br>$$<br>r &#x3D; \frac{\sum_{i&#x3D;1}^{n}[(x^i - \mu_x)(y^i-\mu_y)]}{\sqrt{\sum_{i&#x3D;1}^n(x^i-\mu_x)^2}\sqrt{\sum_{i&#x3D;1}^n(y^i-\mu_y)^2}}&#x3D;<br>\frac{\sigma_{xy}}{\sigma_x\sigma_y}<br>$$<br>其中，$ \mu $为样本特征的均值，$ \sigma_{xy} $为相应的协方差，$ \sigma_x,\sigma_y $分别为两个特征的标准差。</p>
<p>通过以下代码，我们计算前5个特征间的相关系数矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">cm = np.corrcoef(df[cols].values.T)</span><br><span class="line"><span class="comment"># sns.set(font_scale=1.5)</span></span><br><span class="line">hm = sns.heatmap(cm, </span><br><span class="line">                cbar=<span class="literal">True</span>,</span><br><span class="line">                annot=<span class="literal">True</span>,</span><br><span class="line">                square=<span class="literal">True</span>,</span><br><span class="line">                fmt=<span class="string">&#x27;.2f&#x27;</span>,</span><br><span class="line">                annot_kws=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">15</span>&#125;,</span><br><span class="line">                yticklabels=cols,</span><br><span class="line">                xticklabels=cols)</span><br><span class="line">hm.set_ylim([<span class="number">5</span>, <span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>得到的图像如下：</p>
<p><img src="/Sat,%2008%20Feb%202020%20141210.png" alt="img"></p>
<p>为了拟合线性回归模型，我们主要关注那些跟目标变量MEDV高度相关的特征。观察前面的相关系数矩阵，可以发现MEDV与变量LSTAT的相关性最大（-0.74）。另一方面，RM和MEDV间的相关性也较高（0.70）。</p>
<h2 id="基于最小二乘法构建线性回归模型"><a href="#基于最小二乘法构建线性回归模型" class="headerlink" title="基于最小二乘法构建线性回归模型"></a>基于最小二乘法构建线性回归模型</h2><p>接下来我们需要对最优拟合做出判断，在此使用<strong>最小二乘法</strong>（Ordinary Least Square，OLS）估计回归曲线的参数，使得回归曲线到样本点垂直距离的平方和最小。</p>
<h3 id="通过梯度下降计算回归参数"><a href="#通过梯度下降计算回归参数" class="headerlink" title="通过梯度下降计算回归参数"></a>通过梯度下降计算回归参数</h3><p>在第二章中介绍的Adaline中使用了一个线性激励函数，同时定义了一个激励函数，可以通过梯度下降（GD），随机梯度下降（SGD）等优化算法使得代价函数最小，从而得到相应的权重。Adaline中的代价函数就是误差平方和（SSE），他等同于我们定义的OLS代价函数：<br>$$<br>J(w) &#x3D; \frac{1}{2}\sum_{i&#x3D;1}^n(y^i - \hat{y^i})^2<br>$$<br>本质上，OLS线性回归可以理解为无单位阶跃函数的Adaline，这样我们的得到的是连续型的输出值，而不是-1或者1的类标。接下来可以看一下线性回归梯度下降代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressionGD</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, eta=<span class="number">0.001</span>, n_iter=<span class="number">20</span></span>):</span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        self.w_  = np.zeros(<span class="number">1</span> + X.shape[<span class="number">1</span>])</span><br><span class="line">        self.cost_ = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_iter):</span><br><span class="line">            output = self.net_input(X)</span><br><span class="line">            errors = (y - output)</span><br><span class="line">            self.w_[<span class="number">1</span>:] += self.eta * X.T.dot(errors)</span><br><span class="line">            self.w_[<span class="number">0</span>] +=self.eta * errors.<span class="built_in">sum</span>()</span><br><span class="line">            cost = (errors**<span class="number">2</span>).<span class="built_in">sum</span>() / <span class="number">2.0</span></span><br><span class="line">            self.cost_.append(cost)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">net_input</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> np.dot(X, self.w_[<span class="number">1</span>:] + self.w_[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net_input(X)</span><br></pre></td></tr></table></figure>

<p>接下来我们使用房屋数据集中的RM（房间数量）作为解释变量来训练模型以预测MEDV（房屋价格）。此外，为了使得梯度下降算法收敛性更佳，在此对相关变量做了标准化处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X = df[[<span class="string">&#x27;RM&#x27;</span>]].values</span><br><span class="line">y = df[<span class="string">&#x27;MEDV&#x27;</span>].values</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">sc_x = StandardScaler()</span><br><span class="line">sc_y = StandardScaler()</span><br><span class="line">X_std = sc_x.fit_transform(X)</span><br><span class="line">y_std = sc_y.fit_transform(y[:, np.newaxis]).flatten()</span><br><span class="line">lr = LinearRegressionGD()</span><br><span class="line">lr.fit(X_std, y_std)</span><br></pre></td></tr></table></figure>

<p>接下来看一下代价函数和迭代次数的图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, lr.n_iter+<span class="number">1</span>), lr.cost_)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;SSE&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>得到的图像如下：</p>
<p><img src="/Sat,%2008%20Feb%202020%20161004.png" alt="img"></p>
<p>接下来，绘制房间数和房屋价格的关系：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lin_regplot</span>(<span class="params">X, y, model</span>):</span><br><span class="line">    plt.scatter(X, y, c=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    plt.plot(X, model.predict(X), color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">lin_regplot(X_std, y_std, lr)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Average Number&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Price&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>得到的图像如下：</p>
<p><img src="/Sat,%2008%20Feb%202020%20161429.png" alt="img"></p>
<p>从图中可知，随着房间数的增加，房价呈现上涨趋势。但是从图中可以看到，房间数在很多的情况下并不能很好解释房价。对于经过标准化处理的变量，它们的截距必定是0：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Slope: %.3f&#x27;</span> % lr.w_[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Intercept: %.3f&#x27;</span> % lr.w_[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<h3 id="使用scikit-learn估计回归模型的系数"><a href="#使用scikit-learn估计回归模型的系数" class="headerlink" title="使用scikit-learn估计回归模型的系数"></a>使用scikit-learn估计回归模型的系数</h3><p>下面，我们使用scikit-learn中的库实现回归分析：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">slr = LinearRegression()</span><br><span class="line">slr.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Slope: %.3f&#x27;</span> % slr.coef_[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Intercept: %.3f&#x27;</span> % slr.intercept_)</span><br><span class="line">&gt;&gt; Slope: <span class="number">9.102</span></span><br><span class="line">&gt;&gt; Intercept: -<span class="number">34.671</span></span><br></pre></td></tr></table></figure>

<p>执行代码发现得到了不同的模型系数，现在绘制出图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lin_regplot(X, y, slr)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Average Number&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Price&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>得到的图像如下：</p>
<p><img src="/Sat,%2008%20Feb%202020%20162851.png" alt="img"></p>
<p>从图中可以看出，总体结果与GD算法实现的模型是一致的。</p>
<h2 id="使用RANSAC拟合高鲁棒性回归模型"><a href="#使用RANSAC拟合高鲁棒性回归模型" class="headerlink" title="使用RANSAC拟合高鲁棒性回归模型"></a>使用RANSAC拟合高鲁棒性回归模型</h2><p>作为清除异常值的一种高鲁棒性回归方法，在此我们将学习<strong>随机抽样一致性（RANSAC）</strong>算法，使用数据的一个子集来进行回归模型的拟合。该算法流程如下：</p>
<ol>
<li>从数据集中随机抽取样本构建内点集合类拟合模型</li>
<li>使用剩余数据对上一步得到的模型进行测试，并将落在预定公差范围内的样本点增至内带你集合中</li>
<li>使用全部内点集合数据再次进行模型的拟合</li>
<li>使用内点集合来估计模型的误差</li>
<li>如果模型性能达到了特定阈值或者迭代达到了预定次数，则算法中止，否则跳转到第1步</li>
</ol>
<p>首先我们用RANSACRegressor对象来实现我们的线性模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RANSACRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">ransac = RANSACRegressor(LinearRegression(),</span><br><span class="line">                        max_trials=<span class="number">100</span>,</span><br><span class="line">                        min_samples=<span class="number">50</span>,</span><br><span class="line">                        residual_threshold=<span class="number">5.0</span>,</span><br><span class="line">                        random_state=<span class="number">0</span>)</span><br><span class="line">ransac.fit(X, y)</span><br></pre></td></tr></table></figure>

<p>完成拟合后，我们接着来绘制内点和异常值图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">inlier_mask = ransac.inlier_mask_</span><br><span class="line">outlier_mask = np.logical_not(inlier_mask)</span><br><span class="line">line_X = np.arange(<span class="number">3</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">line_y_ransac = ransac.predict(line_X[:, np.newaxis])</span><br><span class="line">plt.scatter(X[inlier_mask], y[inlier_mask], c=<span class="string">&#x27;b&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;Inliers&#x27;</span>)</span><br><span class="line">plt.scatter(X[outlier_mask], y[outlier_mask], c=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;s&#x27;</span>, label=<span class="string">&#x27;Outliers&#x27;</span>)</span><br><span class="line">plt.plot(line_X, line_y_ransac, color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Average Number&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Price&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>图像如下：</p>
<p><img src="/Sat,%2008%20Feb%202020%20162353.png" alt="img"></p>
<p>接下来看一下模型的截距和斜率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Slope: %.3f&#x27;</span> % ransac.estimator_.coef_[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Intercept: %.3f&#x27;</span> % ransac.estimator_.intercept_)</span><br><span class="line">&gt;&gt; Slope: <span class="number">10.735</span></span><br><span class="line">&gt;&gt; Intercept: -<span class="number">44.089</span></span><br></pre></td></tr></table></figure>

<h2 id="线性回归模型性能的评估"><a href="#线性回归模型性能的评估" class="headerlink" title="线性回归模型性能的评估"></a>线性回归模型性能的评估</h2><p>现在我们使用数据集中的所有变量训练多元回归模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">y = df[<span class="string">&#x27;MEDV&#x27;</span>].values</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</span><br><span class="line">slr = LinearRegression()</span><br><span class="line">slr.fit(X_train, y_train)</span><br><span class="line">y_train_pred = slr.predict(X_train)</span><br><span class="line">y_test_pred = slr.predict(X_test)</span><br></pre></td></tr></table></figure>

<p>使用如下代码，我们会绘制出<strong>残差图</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(y_train_pred, y_train_pred - y_train, c=&#x27;b&#x27;, marker=&#x27;o&#x27;, label=&#x27;Training data&#x27;)</span><br><span class="line">plt.scatter(y_test_pred, y_test_pred - y_test, c=&#x27;r&#x27;, marker=&#x27;s&#x27;, label=&#x27;Test data&#x27;)</span><br><span class="line">plt.xlabel(&#x27;Predicted values&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Residuals&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color=&#x27;red&#x27;)</span><br><span class="line">plt.xlim([-10, 50])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>得到的残差图如下：</p>
<p><img src="/Sat,%2008%20Feb%202020%20185508.png" alt="img"></p>
<p>完美的预测结果其残差为0，但是在实际的应用中，这种情况不会出现。不过，对于一个好的回归模型，我们期望误差是随机分布在中心线附近的。</p>
<p>另外一种对模型性能进行定量评估的方法是<strong>均方误差</strong>（Mean Squared Error，MSE），计算公式如下：<br>$$<br>MSE &#x3D; \frac{1}{n}\sum_{i&#x3D;1}^{n}(y^i - \hat{y^i})^2<br>$$<br>执行如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MSE train: %.3f, test: %.3f&#x27;</span> % (mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)))</span><br><span class="line">&gt;&gt; MSE train: <span class="number">19.958</span>, test: <span class="number">27.196</span></span><br></pre></td></tr></table></figure>

<p>从结果而知，训练集上的MSE值为19.96，测试集上的MSE值骤升为27.20，这意味着我们的模型过拟合于训练数据。</p>
<p>某些情况下也可以使用<strong>决定系数</strong>来进行评估，它的计算公式如下：<br>$$<br>R^2 &#x3D; 1 - \frac{MSE}{Var(y)}<br>$$<br>可以使用如下代码来计算$ R^2 $：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;R^2 train: %.3f, test: %.3f&#x27;</span> % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))</span><br><span class="line">&gt;&gt; R^<span class="number">2</span> train: <span class="number">0.765</span>, test: <span class="number">0.673</span></span><br></pre></td></tr></table></figure>

<h2 id="回归中的正则化方法"><a href="#回归中的正则化方法" class="headerlink" title="回归中的正则化方法"></a>回归中的正则化方法</h2><p>正则化是通过在模型中加入额外信息来解决过拟合问题的一种方法，引入罚项增加了模型的复杂度但却降低了模型了模型参数的影响。最常见的正则化线性回归方法就是所谓的<strong>岭回归</strong>（Ridge Regression），<strong>最小绝对收缩及算子选择</strong>（LASSO）以及<strong>弹性网络</strong>（Elastic Net）。</p>
<p>岭回归是基于L2罚项的模型，我们只是在最小二乘代价函数中加入了权重的平方和：<br>$$<br>J(w)<em>{Ridge} &#x3D; \sum</em>{i&#x3D;1}^{n}(y^i-\hat{y^i})^2+\lambda||w||^2_2<br>$$<br>其中：<br>$$<br>L2： \lambda||w||^2_2&#x3D;\lambda\sum_{j&#x3D;1}^{m}w_j^2<br>$$<br>对于稀疏数据训练的模型，还可以使用LASSO：<br>$$<br>J(w)<em>{LASSO}&#x3D; &#x3D; \sum</em>{i&#x3D;1}^{n}(y^i-\hat{y^i})^2+\lambda||w||<em>1<br>$$<br>其中：<br>$$<br>L1： \lambda||w||<em>1 &#x3D; \lambda\sum</em>{j&#x3D;1}^{m}|w_j|<br>$$<br>弹性网络如下：<br>$$<br>J(w)</em>{ElasticNet} &#x3D; \sum_{i&#x3D;1}^{n}(y^i-\hat{y^i})^2+<br>\lambda_1\sum_{j&#x3D;1}^{m}w_j^2+<br>\lambda_2\sum_{j&#x3D;1}^{m}|w_j|<br>$$<br>scikit-learn中岭回归模型的初始化方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line">ridge = Ridge(alpha=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>

<p>正则化强度通过alpha参数来调节，类似于参数$ \lambda $。</p>
<p>LASSO对象的初始化如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line">lasso = Lasso(alpha=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>

<p>最后，scikit-learn下面的ElasticNet允许我们调整L1与L2的比率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet</span><br><span class="line">lasso = ElasticNet(alpha=<span class="number">1.0</span>, l1_ratio=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>

<h2 id="线性回归模型的曲线化—-多项式回归"><a href="#线性回归模型的曲线化—-多项式回归" class="headerlink" title="线性回归模型的曲线化—-多项式回归"></a>线性回归模型的曲线化—-多项式回归</h2><p>对于不符合线性假设的问题，一种常用的解释方法就是：<br>$$<br>y &#x3D; w_0+w_1x+w_2x^2+\cdots+w_dx^d<br>$$<br>接下来我们讨论一下如何使用scikit-learn中的PolynomialFeatures转化类在只含有一个解释变量的简单回归问题中加入二次项。步骤如下：</p>
<ol>
<li><p>增加一个二次多项式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">X = np.array([ <span class="number">258.0</span>, <span class="number">270.0</span>, <span class="number">294.0</span>, <span class="number">320.0</span>, <span class="number">342.0</span>,</span><br><span class="line">            <span class="number">368.0</span>, <span class="number">396.0</span>, <span class="number">446.0</span>, <span class="number">480.0</span>, <span class="number">586.0</span>])[:, np.newaxis]</span><br><span class="line">y = np.array([ <span class="number">236.4</span>, <span class="number">234.4</span>, <span class="number">252.8</span>, <span class="number">298.6</span>, <span class="number">314.2</span>, <span class="number">342.2</span>, <span class="number">360.8</span>, <span class="number">368.0</span>, <span class="number">391.2</span>, <span class="number">390.8</span>])</span><br><span class="line">lr = LinearRegression()</span><br><span class="line">pr = LinearRegression()</span><br><span class="line">quadratic = PolynomialFeatures(degree=<span class="number">2</span>)</span><br><span class="line">X_quad = quadratic.fit_transform(X)</span><br></pre></td></tr></table></figure>
</li>
<li><p>拟合一个用于对比的简单线性回归模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr.fit(X, y)</span><br><span class="line">X_fit = np.arange(<span class="number">250</span>, <span class="number">600</span>, <span class="number">10</span>)[:, np.newaxis]</span><br><span class="line">y_lin_fit = lr.predict(X_fit)</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用经过转换后的特征针对多项式回归拟合一个多元线性回归模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pr.fit(X_quad, y)</span><br><span class="line">y_quad_fit = pr.predict(quadratic.fit_transform(X_fit))</span><br><span class="line">plt.scatter(X, y, label=<span class="string">&#x27;training points&#x27;</span>)</span><br><span class="line">plt.plot(X_fit, y_lin_fit, label=<span class="string">&#x27;linear fit&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">plt.plot(X_fit, y_quad_fit, label=<span class="string">&#x27;quadratic fit&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>得到的图像如下：</p>
<p><img src="/Sat,%2008%20Feb%202020%20194224.png" alt="img"></p>
<p>从图像可以看出，和线性拟合相比，多项式拟合可以更好地捕捉到解释变量和响应变量之间的关系。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">y_lin_pred = lr.predict(X)</span><br><span class="line">y_quad_pred = pr.predict(X_quad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training MSE linear: %.3f, quadratic: %.3f&#x27;</span> % (</span><br><span class="line">        mean_squared_error(y, y_lin_pred),</span><br><span class="line">        mean_squared_error(y, y_quad_pred)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training R^2 linear: %.3f, quadratic: %.3f&#x27;</span> % (</span><br><span class="line">        r2_score(y, y_lin_pred),</span><br><span class="line">        r2_score(y, y_quad_pred)))</span><br><span class="line">&gt;&gt; Training MSE linear: <span class="number">569.780</span>, quadratic: <span class="number">61.330</span></span><br><span class="line">&gt;&gt; Training R^<span class="number">2</span> linear: <span class="number">0.832</span>, quadratic: <span class="number">0.982</span>        </span><br></pre></td></tr></table></figure>

<p>执行上述代码后，MSE的值由线性拟合的570下降到了61。同时和线性拟合结果相比，二次模型的判定系数结果更好，说明二次拟合的效果更好。</p>
<h3 id="房屋数据中的非线性关系建模"><a href="#房屋数据中的非线性关系建模" class="headerlink" title="房屋数据中的非线性关系建模"></a>房屋数据中的非线性关系建模</h3><p>接下来，我们将会使用二次核三次多项式对房屋价格核LSTAT之间的关系进行建模，并且和线性拟合进行对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">X = df[[<span class="string">&#x27;LSTAT&#x27;</span>]].values</span><br><span class="line">y = df[<span class="string">&#x27;MEDV&#x27;</span>].values</span><br><span class="line">regr = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># create polynomial features</span></span><br><span class="line">quadratic = PolynomialFeatures(degree=<span class="number">2</span>)</span><br><span class="line">cubic = PolynomialFeatures(degree=<span class="number">3</span>)</span><br><span class="line">X_quad = quadratic.fit_transform(X)</span><br><span class="line">X_cubic = cubic.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># linear fit</span></span><br><span class="line">X_fit = np.arange(X.<span class="built_in">min</span>(), X.<span class="built_in">max</span>(), <span class="number">1</span>)[:, np.newaxis]</span><br><span class="line">regr = regr.fit(X, y)</span><br><span class="line">y_lin_fit = regr.predict(X_fit)</span><br><span class="line">linear_r2 = r2_score(y, regr.predict(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># quadratic fit</span></span><br><span class="line">regr = regr.fit(X_quad, y)</span><br><span class="line">y_quad_fit = regr.predict(quadratic.fit_transform(X_fit))</span><br><span class="line">quadratic_r2 = r2_score(y, regr.predict(X_quad))</span><br><span class="line"></span><br><span class="line"><span class="comment"># cubic fit</span></span><br><span class="line">regr = regr.fit(X_cubic, y)</span><br><span class="line">y_cubic_fit = regr.predict(cubic.fit_transform(X_fit))</span><br><span class="line">cubic_r2 = r2_score(y, regr.predict(X_cubic))</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot results</span></span><br><span class="line">plt.scatter(X, y, label=<span class="string">&#x27;training points&#x27;</span>, color=<span class="string">&#x27;lightgray&#x27;</span>)</span><br><span class="line">plt.plot(X_fit, y_lin_fit, label=<span class="string">&#x27;linear(d=1), R^2 = %.2f&#x27;</span> % linear_r2, color=<span class="string">&#x27;b&#x27;</span>, lw=<span class="number">2</span>, linestyle=<span class="string">&#x27;:&#x27;</span>)</span><br><span class="line">plt.plot(X_fit, y_quad_fit, label=<span class="string">&#x27;quadratic(d=2), R^2 = %.2f&#x27;</span> % quadratic_r2, color=<span class="string">&#x27;g&#x27;</span>, lw=<span class="number">2</span>, linestyle=<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">plt.plot(X_fit, y_cubic_fit, label=<span class="string">&#x27;cubic(d=3), R^2 = %.2f&#x27;</span> % cubic_r2, color=<span class="string">&#x27;r&#x27;</span>, lw=<span class="number">2</span>, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;LSTAT&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Price&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>图像如下：</p>
<p><img src="/Sat,%2008%20Feb%202020%20200734.png" alt="img"></p>
<p>从图像而知，相较于线性拟合和二次拟合，三次拟合更好地捕获了房屋价格与LSTAT之间的关系。不过，加入越来越多的多项式特征会增加模型复杂度，容易造成过拟合。</p>
<p>此外，多项式特征并非总是非线性关系建模的最佳选择。例如，我们仅就MEDV-LSTAT的散点图来说，我们可以将LSTAT特征变量的对数值以及MEDV的平方根映射到一个特征空间，并用线性回归进行拟合：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># transform features</span></span><br><span class="line">X_log = np.log(X)</span><br><span class="line">y_sqrt = np.sqrt(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit features</span></span><br><span class="line">X_fit = np.arange(X_log.<span class="built_in">min</span>()-<span class="number">1</span>, X_log.<span class="built_in">max</span>()+<span class="number">1</span>, <span class="number">1</span>)[:, np.newaxis]</span><br><span class="line">regr = regr.fit(X_log, y_sqrt)</span><br><span class="line">y_lin_fit = regr.predict(X_fit)</span><br><span class="line">linear_r2 = r2_score(y_sqrt, regr.predict(X_log))</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot results</span></span><br><span class="line">plt.scatter(X_log, y_sqrt, label=<span class="string">&#x27;training points&#x27;</span>,color=<span class="string">&#x27;lightgray&#x27;</span>)</span><br><span class="line">plt.plot(X_fit, y_lin_fit, label=<span class="string">&#x27;linear(d=1), R^2=%.2f&#x27;</span> % linear_r2, color=<span class="string">&#x27;blue&#x27;</span>, lw=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;LSTAT&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Price&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>得到的图像如下：</p>
<p><img src="/Sat,%2008%20Feb%202020%20201959.png" alt="img"></p>
<p>从$ R^2 $的值可以看出，这种拟合形式优于前面的任何一种多项式回归。</p>
<h3 id="使用随机森林处理非线性关系"><a href="#使用随机森林处理非线性关系" class="headerlink" title="使用随机森林处理非线性关系"></a>使用随机森林处理非线性关系</h3><p>本节中，我们将会学习<strong>随机森林</strong>回归，他从概念上异于本章中介绍的其他回归模型。随机森林是多颗<strong>决策树</strong>的集合，它可以被理解成分段线性函数的集成。</p>
<ol>
<li><p>决策树回归</p>
<p>决策树算法的一个优点是我们无需对数据进行特征转换。在scikit-learn中对其进行建模：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line">X = df[[<span class="string">&#x27;LSTAT&#x27;</span>]].values</span><br><span class="line">y = df[<span class="string">&#x27;MEDV&#x27;</span>].values</span><br><span class="line">tree = DecisionTreeRegressor(max_depth=<span class="number">3</span>)</span><br><span class="line">tree.fit(X, y)</span><br><span class="line">sort_idx = X.flatten().argsort()</span><br><span class="line">lin_regplot(X[sort_idx], y[sort_idx], tree)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;LSTAT&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Price&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>图像如下：</p>
<p><img src="/Sat,%2008%20Feb%202020%20203210.png" alt="img"></p>
<p>在此例中，深度为3的树看起来是比较合适的。</p>
</li>
<li><p>随机森林回归</p>
<p>随机森林算法是组合多颗决策树的一种集成技术。随机森林的一个优势是：它对数据集中的异常值不敏感，且无需过多的参数调优。接下来使用scikit-learn中的库来拟合一个随机森林回归模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">y = df[<span class="string">&#x27;MEDV&#x27;</span>].values</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.4</span>, random_state=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line">forest = RandomForestRegressor(n_estimators=<span class="number">1000</span>, criterion=<span class="string">&#x27;mse&#x27;</span>, random_state=<span class="number">1</span>, n_jobs=-<span class="number">1</span>)</span><br><span class="line">forest.fit(X_train, y_train)</span><br><span class="line">y_train_pred = forest.predict(X_train)</span><br><span class="line">y_test_pred = forest.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MSE train: %.3f, test: %.3f&#x27;</span> % (mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;R^2 train: %.3f, test: %.3f&#x27;</span> % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))</span><br><span class="line">&gt;&gt; MSE train: <span class="number">1.641</span>, test: <span class="number">11.056</span></span><br><span class="line">&gt;&gt; R^<span class="number">2</span> train: <span class="number">0.979</span>, test: <span class="number">0.878</span></span><br></pre></td></tr></table></figure>

<p>遗憾的是，我们发现随机森林对于训练数据有些过拟合，接下来看一下预测的残差图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(y_train_pred, y_train_pred - y_train, c=<span class="string">&#x27;black&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, s=<span class="number">35</span>, alpha=<span class="number">0.5</span>, label=<span class="string">&#x27;Training data&#x27;</span>)</span><br><span class="line">plt.scatter(y_test_pred, y_test_pred - y_test, c=<span class="string">&#x27;lightgreen&#x27;</span>, marker=<span class="string">&#x27;s&#x27;</span>, s=<span class="number">35</span>, alpha=<span class="number">0.7</span>, label=<span class="string">&#x27;Test data&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Predicted values&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Residuals&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.hlines(y=<span class="number">0</span>, xmin=-<span class="number">10</span>, xmax=<span class="number">50</span>, lw=<span class="number">2</span>, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.xlim([-<span class="number">10</span>, <span class="number">50</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>图像如下：</p>
<p><img src="/Sat,%2008%20Feb%202020%20204701.png" alt="img"></p>
<p>可以发现，随机森林的残差图相比线性拟合产生的残差图有了很大的改进。</p>
</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://blog.zsstrike.tech">zsStrike</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://blog.zsstrike.tech/2020/02/08/%E4%BD%BF%E7%94%A8%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%E8%BF%9E%E7%BB%AD%E5%9E%8B%E7%9B%AE%E6%A0%87%E5%8F%98%E9%87%8F/">http://blog.zsstrike.tech/2020/02/08/使用回归分析预测连续型目标变量/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://blog.zsstrike.tech" target="_blank">zsStrike</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/assets/default_cover.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/assets/avatar.jpg" onerror="this.onerror=null;this.src='/assets/default_cover.png'" alt="avatar"/></div><div class="author-info__name">zsStrike</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">50</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zsStrike"><i class="fab fa-github"></i><span>Github</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%8E%A2"><span class="toc-number">1.</span> <span class="toc-text">简单线性回归模型初探</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E5%B1%8B%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.</span> <span class="toc-text">波士顿房屋数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%9E%84%E5%BB%BA%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.</span> <span class="toc-text">基于最小二乘法构建线性回归模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%AE%A1%E7%AE%97%E5%9B%9E%E5%BD%92%E5%8F%82%E6%95%B0"><span class="toc-number">3.1.</span> <span class="toc-text">通过梯度下降计算回归参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8scikit-learn%E4%BC%B0%E8%AE%A1%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%B3%BB%E6%95%B0"><span class="toc-number">3.2.</span> <span class="toc-text">使用scikit-learn估计回归模型的系数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8RANSAC%E6%8B%9F%E5%90%88%E9%AB%98%E9%B2%81%E6%A3%92%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.</span> <span class="toc-text">使用RANSAC拟合高鲁棒性回归模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E7%9A%84%E8%AF%84%E4%BC%B0"><span class="toc-number">5.</span> <span class="toc-text">线性回归模型性能的评估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">6.</span> <span class="toc-text">回归中的正则化方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9B%B2%E7%BA%BF%E5%8C%96%E2%80%94-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="toc-number">7.</span> <span class="toc-text">线性回归模型的曲线化—-多项式回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%88%BF%E5%B1%8B%E6%95%B0%E6%8D%AE%E4%B8%AD%E7%9A%84%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%E5%BB%BA%E6%A8%A1"><span class="toc-number">7.1.</span> <span class="toc-text">房屋数据中的非线性关系建模</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%A4%84%E7%90%86%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB"><span class="toc-number">7.2.</span> <span class="toc-text">使用随机森林处理非线性关系</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2017 - 2023 By zsStrike</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>